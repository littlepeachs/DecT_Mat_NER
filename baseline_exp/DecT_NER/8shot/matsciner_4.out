/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:33:01 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:33:01 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:8, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1230.18it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:33:29 - INFO - __main__ - ***** Running training *****
05/30/2023 12:33:29 - INFO - __main__ -   Num examples = 21
05/30/2023 12:33:29 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:33:29 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:33:29 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:33:29 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:33:29 - INFO - __main__ -   Total optimization steps = 35
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.562054634094238
Total epoch: 1. epoch loss: 14.793957710266113
Total epoch: 2. epoch loss: 14.034504890441895
Total epoch: 3. epoch loss: 13.286455154418945
Total epoch: 4. epoch loss: 12.553096771240234
Total epoch: 5. epoch loss: 11.838129043579102
Total epoch: 6. epoch loss: 11.145261764526367
Total epoch: 7. epoch loss: 10.477930068969727
Total epoch: 8. epoch loss: 9.83884048461914
Total epoch: 9. epoch loss: 9.229597091674805
Total epoch: 10. epoch loss: 8.650733947753906
Total epoch: 11. epoch loss: 8.101910591125488
Total epoch: 12. epoch loss: 7.582164287567139
Total epoch: 13. epoch loss: 7.090419769287109
Total epoch: 14. epoch loss: 6.6254801750183105
Total epoch: 15. epoch loss: 6.186376571655273
Total epoch: 16. epoch loss: 5.772156715393066
Total epoch: 17. epoch loss: 5.381966590881348
Total epoch: 18. epoch loss: 5.01494026184082
Total epoch: 19. epoch loss: 4.670219898223877
Total epoch: 20. epoch loss: 4.346938133239746
Total epoch: 21. epoch loss: 4.044208526611328
Total epoch: 22. epoch loss: 3.761033058166504
Total epoch: 23. epoch loss: 3.4964094161987305
Total epoch: 24. epoch loss: 3.24934983253479
Total epoch: 25. epoch loss: 3.0187628269195557
Total epoch: 26. epoch loss: 2.803727865219116
Total epoch: 27. epoch loss: 2.6032874584198
Total epoch: 28. epoch loss: 2.4174537658691406
Total epoch: 29. epoch loss: 2.245952844619751
Total epoch: 30. epoch loss: 2.0882463455200195
Total epoch: 31. epoch loss: 1.943628191947937
Total epoch: 32. epoch loss: 1.8112951517105103
Total epoch: 33. epoch loss: 1.6903696060180664
Total epoch: 34. epoch loss: 1.5799992084503174
Total epoch: 34. DecT loss: 1.5799992084503174
Training time: 0.18384766578674316
APL_precision: 0.23076923076923078, APL_recall: 0.3352941176470588, APL_f1: 0.2733812949640288, APL_number: 170
CMT_precision: 0.04904306220095694, CMT_recall: 0.21025641025641026, CMT_f1: 0.07953443258971873, CMT_number: 195
DSC_precision: 0.42063492063492064, DSC_recall: 0.2425629290617849, DSC_f1: 0.3076923076923077, DSC_number: 437
MAT_precision: 0.4634974533106961, MAT_recall: 0.4002932551319648, MAT_f1: 0.42958300550747447, MAT_number: 682
PRO_precision: 0.3333333333333333, PRO_recall: 0.26329442282749677, PRO_f1: 0.2942028985507246, PRO_number: 771
SMT_precision: 0.12021857923497267, SMT_recall: 0.38596491228070173, SMT_f1: 0.18333333333333332, SMT_number: 171
SPL_precision: 0.3253968253968254, SPL_recall: 0.5466666666666666, SPL_f1: 0.4079601990049751, SPL_number: 75
overall_precision: 0.24532418952618454, overall_recall: 0.31467413034786085, overall_f1: 0.27570502715011386, overall_accuracy: 0.7327567722107069
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:36:54 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:36:55 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:8, proto_dim:128, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1147.87it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:37:01 - INFO - __main__ - ***** Running training *****
05/30/2023 12:37:01 - INFO - __main__ -   Num examples = 21
05/30/2023 12:37:01 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:37:01 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:37:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:37:01 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:37:01 - INFO - __main__ -   Total optimization steps = 100
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.58311653137207
Total epoch: 1. epoch loss: 14.505024909973145
Total epoch: 2. epoch loss: 13.44638729095459
Total epoch: 3. epoch loss: 12.416942596435547
Total epoch: 4. epoch loss: 11.427861213684082
Total epoch: 5. epoch loss: 10.489774703979492
Total epoch: 6. epoch loss: 9.610274314880371
Total epoch: 7. epoch loss: 8.792227745056152
Total epoch: 8. epoch loss: 8.034248352050781
Total epoch: 9. epoch loss: 7.3329668045043945
Total epoch: 10. epoch loss: 6.684723377227783
Total epoch: 11. epoch loss: 6.086580753326416
Total epoch: 12. epoch loss: 5.536016464233398
Total epoch: 13. epoch loss: 5.030719757080078
Total epoch: 14. epoch loss: 4.5682291984558105
Total epoch: 15. epoch loss: 4.145927429199219
Total epoch: 16. epoch loss: 3.761091709136963
Total epoch: 17. epoch loss: 3.4109678268432617
Total epoch: 18. epoch loss: 3.0928287506103516
Total epoch: 19. epoch loss: 2.8040049076080322
Total epoch: 20. epoch loss: 2.541957139968872
Total epoch: 21. epoch loss: 2.3056447505950928
Total epoch: 22. epoch loss: 2.0935769081115723
Total epoch: 23. epoch loss: 1.9039771556854248
Total epoch: 24. epoch loss: 1.734961748123169
Total epoch: 25. epoch loss: 1.5846360921859741
Total epoch: 26. epoch loss: 1.4511488676071167
Total epoch: 27. epoch loss: 1.33279287815094
Total epoch: 28. epoch loss: 1.2279772758483887
Total epoch: 29. epoch loss: 1.1352591514587402
Total epoch: 30. epoch loss: 1.0533158779144287
Total epoch: 31. epoch loss: 0.98091059923172
Total epoch: 32. epoch loss: 0.9169027805328369
Total epoch: 33. epoch loss: 0.8602345585823059
Total epoch: 34. epoch loss: 0.8099293112754822
Total epoch: 35. epoch loss: 0.7651081681251526
Total epoch: 36. epoch loss: 0.7250082492828369
Total epoch: 37. epoch loss: 0.688961386680603
Total epoch: 38. epoch loss: 0.6564096212387085
Total epoch: 39. epoch loss: 0.6268861293792725
Total epoch: 40. epoch loss: 0.600004255771637
Total epoch: 41. epoch loss: 0.5754435062408447
Total epoch: 42. epoch loss: 0.5529362559318542
Total epoch: 43. epoch loss: 0.5322412848472595
Total epoch: 44. epoch loss: 0.5131723284721375
Total epoch: 45. epoch loss: 0.49554282426834106
Total epoch: 46. epoch loss: 0.4792087972164154
Total epoch: 47. epoch loss: 0.4640290141105652
Total epoch: 48. epoch loss: 0.4498915374279022
Total epoch: 49. epoch loss: 0.4367002844810486
Total epoch: 50. epoch loss: 0.4243593215942383
Total epoch: 51. epoch loss: 0.41279515624046326
Total epoch: 52. epoch loss: 0.40193840861320496
Total epoch: 53. epoch loss: 0.3917316198348999
Total epoch: 54. epoch loss: 0.3821136951446533
Total epoch: 55. epoch loss: 0.3730376660823822
Total epoch: 56. epoch loss: 0.36445698142051697
Total epoch: 57. epoch loss: 0.3563295602798462
Total epoch: 58. epoch loss: 0.34861668944358826
Total epoch: 59. epoch loss: 0.34129008650779724
Total epoch: 60. epoch loss: 0.33431702852249146
Total epoch: 61. epoch loss: 0.32766884565353394
Total epoch: 62. epoch loss: 0.32132959365844727
Total epoch: 63. epoch loss: 0.31527137756347656
Total epoch: 64. epoch loss: 0.30947980284690857
Total epoch: 65. epoch loss: 0.3039344549179077
Total epoch: 66. epoch loss: 0.29862308502197266
Total epoch: 67. epoch loss: 0.2935265898704529
Total epoch: 68. epoch loss: 0.2886353135108948
Total epoch: 69. epoch loss: 0.2839319705963135
Total epoch: 70. epoch loss: 0.2794097363948822
Total epoch: 71. epoch loss: 0.2750568091869354
Total epoch: 72. epoch loss: 0.27086353302001953
Total epoch: 73. epoch loss: 0.2668161392211914
Total epoch: 74. epoch loss: 0.2629137635231018
Total epoch: 75. epoch loss: 0.25914281606674194
Total epoch: 76. epoch loss: 0.2554965317249298
Total epoch: 77. epoch loss: 0.25196772813796997
Total epoch: 78. epoch loss: 0.24855025112628937
Total epoch: 79. epoch loss: 0.2452395111322403
Total epoch: 80. epoch loss: 0.24202848970890045
Total epoch: 81. epoch loss: 0.2389129400253296
Total epoch: 82. epoch loss: 0.23588857054710388
Total epoch: 83. epoch loss: 0.23295120894908905
Total epoch: 84. epoch loss: 0.23009395599365234
Total epoch: 85. epoch loss: 0.22731651365756989
Total epoch: 86. epoch loss: 0.2246134728193283
Total epoch: 87. epoch loss: 0.2219826728105545
Total epoch: 88. epoch loss: 0.21942001581192017
Total epoch: 89. epoch loss: 0.2169213593006134
Total epoch: 90. epoch loss: 0.2144865095615387
Total epoch: 91. epoch loss: 0.2121107131242752
Total epoch: 92. epoch loss: 0.20979267358779907
Total epoch: 93. epoch loss: 0.2075301855802536
Total epoch: 94. epoch loss: 0.20532016456127167
Total epoch: 95. epoch loss: 0.20316116511821747
Total epoch: 96. epoch loss: 0.20105119049549103
Total epoch: 97. epoch loss: 0.19898861646652222
Total epoch: 98. epoch loss: 0.19697321951389313
Total epoch: 99. epoch loss: 0.19500046968460083
Total epoch: 99. DecT loss: 0.19500046968460083
Training time: 0.4539661407470703
APL_precision: 0.3333333333333333, APL_recall: 0.3176470588235294, APL_f1: 0.3253012048192771, APL_number: 170
CMT_precision: 0.0846286701208981, CMT_recall: 0.2512820512820513, CMT_f1: 0.12661498708010338, CMT_number: 195
DSC_precision: 0.5333333333333333, DSC_recall: 0.21967963386727687, DSC_f1: 0.3111831442463533, DSC_number: 437
MAT_precision: 0.5631929046563193, MAT_recall: 0.3724340175953079, MAT_f1: 0.44836716681376876, MAT_number: 682
PRO_precision: 0.30120481927710846, PRO_recall: 0.19455252918287938, PRO_f1: 0.2364066193853428, PRO_number: 771
SMT_precision: 0.17829457364341086, SMT_recall: 0.40350877192982454, SMT_f1: 0.24731182795698922, SMT_number: 171
SPL_precision: 0.3644859813084112, SPL_recall: 0.52, SPL_f1: 0.4285714285714286, SPL_number: 75
overall_precision: 0.30076142131979694, overall_recall: 0.2842862854858057, overall_f1: 0.2922918807810894, overall_accuracy: 0.7558430419555429
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:39:03 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:39:04 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1061.45it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:40:31 - INFO - __main__ - ***** Running training *****
05/30/2023 12:40:31 - INFO - __main__ -   Num examples = 21
05/30/2023 12:40:31 - INFO - __main__ -   Num Epochs = 150
05/30/2023 12:40:31 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:40:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:40:31 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:40:31 - INFO - __main__ -   Total optimization steps = 150
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/150 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 15.518220901489258
Total epoch: 1. epoch loss: 14.317334175109863
Total epoch: 2. epoch loss: 13.140705108642578
Total epoch: 3. epoch loss: 12.002508163452148
Total epoch: 4. epoch loss: 10.918734550476074
Total epoch: 5. epoch loss: 9.90354061126709
Total epoch: 6. epoch loss: 8.965085983276367
Total epoch: 7. epoch loss: 8.104133605957031
Total epoch: 8. epoch loss: 7.316677570343018
Total epoch: 9. epoch loss: 6.597278118133545
Total epoch: 10. epoch loss: 5.94115686416626
Total epoch: 11. epoch loss: 5.344417095184326
Total epoch: 12. epoch loss: 4.803481101989746
Total epoch: 13. epoch loss: 4.314695358276367
Total epoch: 14. epoch loss: 3.8742170333862305
Total epoch: 15. epoch loss: 3.4781277179718018
Total epoch: 16. epoch loss: 3.1224398612976074
Total epoch: 17. epoch loss: 2.8032302856445312
Total epoch: 18. epoch loss: 2.5167958736419678
Total epoch: 19. epoch loss: 2.26127028465271
Total epoch: 20. epoch loss: 2.0343997478485107
Total epoch: 21. epoch loss: 1.8337738513946533
Total epoch: 22. epoch loss: 1.656927466392517
Total epoch: 23. epoch loss: 1.501406192779541
Total epoch: 24. epoch loss: 1.3648395538330078
Total epoch: 25. epoch loss: 1.2450742721557617
Total epoch: 26. epoch loss: 1.1401785612106323
Total epoch: 27. epoch loss: 1.0484288930892944
Total epoch: 28. epoch loss: 0.968237042427063
Total epoch: 29. epoch loss: 0.8981595039367676
Total epoch: 30. epoch loss: 0.8368490934371948
Total epoch: 31. epoch loss: 0.7830807566642761
Total epoch: 32. epoch loss: 0.7357332110404968
Total epoch: 33. epoch loss: 0.6938214898109436
Total epoch: 34. epoch loss: 0.6565164923667908
Total epoch: 35. epoch loss: 0.6231037974357605
Total epoch: 36. epoch loss: 0.5930153131484985
Total epoch: 37. epoch loss: 0.5657913684844971
Total epoch: 38. epoch loss: 0.5410515069961548
Total epoch: 39. epoch loss: 0.5185009241104126
Total epoch: 40. epoch loss: 0.49787503480911255
Total epoch: 41. epoch loss: 0.47895610332489014
Total epoch: 42. epoch loss: 0.4615554213523865
Total epoch: 43. epoch loss: 0.4454957842826843
Total epoch: 44. epoch loss: 0.4306349754333496
Total epoch: 45. epoch loss: 0.41684186458587646
Total epoch: 46. epoch loss: 0.4040068984031677
Total epoch: 47. epoch loss: 0.3920302391052246
Total epoch: 48. epoch loss: 0.3808366060256958
Total epoch: 49. epoch loss: 0.37035152316093445
Total epoch: 50. epoch loss: 0.36051490902900696
Total epoch: 51. epoch loss: 0.3512711226940155
Total epoch: 52. epoch loss: 0.34257009625434875
Total epoch: 53. epoch loss: 0.3343650996685028
Total epoch: 54. epoch loss: 0.32661423087120056
Total epoch: 55. epoch loss: 0.31927868723869324
Total epoch: 56. epoch loss: 0.31232717633247375
Total epoch: 57. epoch loss: 0.30572500824928284
Total epoch: 58. epoch loss: 0.29944634437561035
Total epoch: 59. epoch loss: 0.29346299171447754
Total epoch: 60. epoch loss: 0.28776025772094727
Total epoch: 61. epoch loss: 0.2823140621185303
Total epoch: 62. epoch loss: 0.2771065831184387
Total epoch: 63. epoch loss: 0.2721208333969116
Total epoch: 64. epoch loss: 0.26734647154808044
Total epoch: 65. epoch loss: 0.2627672553062439
Total epoch: 66. epoch loss: 0.2583727538585663
Total epoch: 67. epoch loss: 0.2541510760784149
Total epoch: 68. epoch loss: 0.25009432435035706
Total epoch: 69. epoch loss: 0.24618932604789734
Total epoch: 70. epoch loss: 0.24243074655532837
Total epoch: 71. epoch loss: 0.23880694806575775
Total epoch: 72. epoch loss: 0.23531247675418854
Total epoch: 73. epoch loss: 0.23193901777267456
Total epoch: 74. epoch loss: 0.22867722809314728
Total epoch: 75. epoch loss: 0.22552450001239777
Total epoch: 76. epoch loss: 0.22247231006622314
Total epoch: 77. epoch loss: 0.2195170819759369
Total epoch: 78. epoch loss: 0.21665135025978088
Total epoch: 79. epoch loss: 0.2138742357492447
Total epoch: 80. epoch loss: 0.21117794513702393
Total epoch: 81. epoch loss: 0.2085600048303604
Total epoch: 82. epoch loss: 0.20601798593997955
Total epoch: 83. epoch loss: 0.20354489982128143
Total epoch: 84. epoch loss: 0.20114116370677948
Total epoch: 85. epoch loss: 0.19880357384681702
Total epoch: 86. epoch loss: 0.19652660191059113
Total epoch: 87. epoch loss: 0.19430945813655853
Total epoch: 88. epoch loss: 0.19214849174022675
Total epoch: 89. epoch loss: 0.19004131853580475
Total epoch: 90. epoch loss: 0.18798816204071045
Total epoch: 91. epoch loss: 0.1859837770462036
Total epoch: 92. epoch loss: 0.1840277463197708
Total epoch: 93. epoch loss: 0.18211843073368073
Total epoch: 94. epoch loss: 0.18025344610214233
Total epoch: 95. epoch loss: 0.17843195796012878
Total epoch: 96. epoch loss: 0.17665135860443115
Total epoch: 97. epoch loss: 0.17491182684898376
Total epoch: 98. epoch loss: 0.1732095628976822
Total epoch: 99. epoch loss: 0.1715455800294876
Total epoch: 100. epoch loss: 0.1699182242155075
Total epoch: 101. epoch loss: 0.1683247685432434
Total epoch: 102. epoch loss: 0.166765958070755
Total epoch: 103. epoch loss: 0.16523902118206024
Total epoch: 104. epoch loss: 0.1637440323829651
Total epoch: 105. epoch loss: 0.16227874159812927
Total epoch: 106. epoch loss: 0.16084463894367218
Total epoch: 107. epoch loss: 0.15943893790245056
Total epoch: 108. epoch loss: 0.15806137025356293
Total epoch: 109. epoch loss: 0.15671028196811676
Total epoch: 110. epoch loss: 0.1553855985403061
Total epoch: 111. epoch loss: 0.15408679842948914
Total epoch: 112. epoch loss: 0.15281309187412262
Total epoch: 113. epoch loss: 0.15156356990337372
Total epoch: 114. epoch loss: 0.1503371000289917
Total epoch: 115. epoch loss: 0.1491331309080124
Total epoch: 116. epoch loss: 0.14795245230197906
Total epoch: 117. epoch loss: 0.1467921882867813
Total epoch: 118. epoch loss: 0.14565415680408478
Total epoch: 119. epoch loss: 0.1445358246564865
Total epoch: 120. epoch loss: 0.14343677461147308
Total epoch: 121. epoch loss: 0.1423579454421997
Total epoch: 122. epoch loss: 0.14129813015460968
Total epoch: 123. epoch loss: 0.14025577902793884
Total epoch: 124. epoch loss: 0.1392320990562439
Total epoch: 125. epoch loss: 0.1382259875535965
Total epoch: 126. epoch loss: 0.13723637163639069
Total epoch: 127. epoch loss: 0.1362628936767578
Total epoch: 128. epoch loss: 0.13530683517456055
Total epoch: 129. epoch loss: 0.13436630368232727
Total epoch: 130. epoch loss: 0.1334417313337326
Total epoch: 131. epoch loss: 0.13253165781497955
Total epoch: 132. epoch loss: 0.13163572549819946
Total epoch: 133. epoch loss: 0.13075415790081024
Total epoch: 134. epoch loss: 0.12988780438899994
Total epoch: 135. epoch loss: 0.12903402745723724
Total epoch: 136. epoch loss: 0.12819410860538483
Total epoch: 137. epoch loss: 0.12736742198467255
Total epoch: 138. epoch loss: 0.12655416131019592
Total epoch: 139. epoch loss: 0.12575207650661469
Total epoch: 140. epoch loss: 0.12496302276849747
Total epoch: 141. epoch loss: 0.12418513000011444
Total epoch: 142. epoch loss: 0.12341921776533127
Total epoch: 143. epoch loss: 0.12266530096530914
Total epoch: 144. epoch loss: 0.12192273139953613
Total epoch: 145. epoch loss: 0.12119041383266449
Total epoch: 146. epoch loss: 0.1204693466424942
Total epoch: 147. epoch loss: 0.11975830048322678
Total epoch: 148. epoch loss: 0.11905813962221146
Total epoch: 149. epoch loss: 0.11836814135313034
Total epoch: 149. DecT loss: 0.11836814135313034
Training time: 0.6954724788665771
APL_precision: 0.3312883435582822, APL_recall: 0.3176470588235294, APL_f1: 0.3243243243243243, APL_number: 170
CMT_precision: 0.08256880733944955, CMT_recall: 0.23076923076923078, CMT_f1: 0.12162162162162163, CMT_number: 195
DSC_precision: 0.5393939393939394, DSC_recall: 0.2036613272311213, DSC_f1: 0.2956810631229236, DSC_number: 437
MAT_precision: 0.5737704918032787, MAT_recall: 0.3592375366568915, MAT_f1: 0.4418394950405771, MAT_number: 682
PRO_precision: 0.2880324543610548, PRO_recall: 0.18417639429312582, PRO_f1: 0.2246835443037975, PRO_number: 771
SMT_precision: 0.17048346055979643, SMT_recall: 0.391812865497076, SMT_f1: 0.23758865248226949, SMT_number: 171
SPL_precision: 0.3611111111111111, SPL_recall: 0.52, SPL_f1: 0.42622950819672134, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.2968613775065388, overall_recall: 0.2722910835665734, overall_f1: 0.28404588112617307, overall_accuracy: 0.7549853477235365
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/150 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:45:12 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:45:13 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.008, batch_size:32, shot:8, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1070.39it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:45:20 - INFO - __main__ - ***** Running training *****
05/30/2023 12:45:20 - INFO - __main__ -   Num examples = 21
05/30/2023 12:45:20 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:45:20 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:45:20 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:45:20 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:45:20 - INFO - __main__ -   Total optimization steps = 100
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.562054634094238
Total epoch: 1. epoch loss: 14.337308883666992
Total epoch: 2. epoch loss: 13.14035701751709
Total epoch: 3. epoch loss: 11.984329223632812
Total epoch: 4. epoch loss: 10.88456916809082
Total epoch: 5. epoch loss: 9.85484504699707
Total epoch: 6. epoch loss: 8.90363883972168
Total epoch: 7. epoch loss: 8.032170295715332
Total epoch: 8. epoch loss: 7.236688137054443
Total epoch: 9. epoch loss: 6.511652946472168
Total epoch: 10. epoch loss: 5.851787567138672
Total epoch: 11. epoch loss: 5.252617359161377
Total epoch: 12. epoch loss: 4.7100114822387695
Total epoch: 13. epoch loss: 4.220157146453857
Total epoch: 14. epoch loss: 3.779252767562866
Total epoch: 15. epoch loss: 3.3834431171417236
Total epoch: 16. epoch loss: 3.028729200363159
Total epoch: 17. epoch loss: 2.71109676361084
Total epoch: 18. epoch loss: 2.4271292686462402
Total epoch: 19. epoch loss: 2.1748311519622803
Total epoch: 20. epoch loss: 1.951923131942749
Total epoch: 21. epoch loss: 1.755795955657959
Total epoch: 22. epoch loss: 1.5837194919586182
Total epoch: 23. epoch loss: 1.4329454898834229
Total epoch: 24. epoch loss: 1.3009514808654785
Total epoch: 25. epoch loss: 1.1854602098464966
Total epoch: 26. epoch loss: 1.0845239162445068
Total epoch: 27. epoch loss: 0.9964266419410706
Total epoch: 28. epoch loss: 0.9196264147758484
Total epoch: 29. epoch loss: 0.8526805639266968
Total epoch: 30. epoch loss: 0.794237494468689
Total epoch: 31. epoch loss: 0.7430787086486816
Total epoch: 32. epoch loss: 0.6980723738670349
Total epoch: 33. epoch loss: 0.6582469344139099
Total epoch: 34. epoch loss: 0.6227924823760986
Total epoch: 35. epoch loss: 0.5910300016403198
Total epoch: 36. epoch loss: 0.5624209046363831
Total epoch: 37. epoch loss: 0.5365437865257263
Total epoch: 38. epoch loss: 0.5130277872085571
Total epoch: 39. epoch loss: 0.49158766865730286
Total epoch: 40. epoch loss: 0.47197115421295166
Total epoch: 41. epoch loss: 0.45396000146865845
Total epoch: 42. epoch loss: 0.43737876415252686
Total epoch: 43. epoch loss: 0.4220670461654663
Total epoch: 44. epoch loss: 0.40788671374320984
Total epoch: 45. epoch loss: 0.39472267031669617
Total epoch: 46. epoch loss: 0.38247889280319214
Total epoch: 47. epoch loss: 0.37106508016586304
Total epoch: 48. epoch loss: 0.3604089319705963
Total epoch: 49. epoch loss: 0.3504331111907959
Total epoch: 50. epoch loss: 0.34107884764671326
Total epoch: 51. epoch loss: 0.33229780197143555
Total epoch: 52. epoch loss: 0.32402926683425903
Total epoch: 53. epoch loss: 0.3162307143211365
Total epoch: 54. epoch loss: 0.3088683784008026
Total epoch: 55. epoch loss: 0.30189868807792664
Total epoch: 56. epoch loss: 0.29529398679733276
Total epoch: 57. epoch loss: 0.2890219986438751
Total epoch: 58. epoch loss: 0.2830641567707062
Total epoch: 59. epoch loss: 0.2773861587047577
Total epoch: 60. epoch loss: 0.27197667956352234
Total epoch: 61. epoch loss: 0.26681241393089294
Total epoch: 62. epoch loss: 0.2618791460990906
Total epoch: 63. epoch loss: 0.257159024477005
Total epoch: 64. epoch loss: 0.25264033675193787
Total epoch: 65. epoch loss: 0.24831147491931915
Total epoch: 66. epoch loss: 0.24416008591651917
Total epoch: 67. epoch loss: 0.24017851054668427
Total epoch: 68. epoch loss: 0.2363506704568863
Total epoch: 69. epoch loss: 0.23266994953155518
Total epoch: 70. epoch loss: 0.22912849485874176
Total epoch: 71. epoch loss: 0.22571630775928497
Total epoch: 72. epoch loss: 0.22242668271064758
Total epoch: 73. epoch loss: 0.21925123035907745
Total epoch: 74. epoch loss: 0.21618370711803436
Total epoch: 75. epoch loss: 0.21321989595890045
Total epoch: 76. epoch loss: 0.21035191416740417
Total epoch: 77. epoch loss: 0.20757260918617249
Total epoch: 78. epoch loss: 0.2048836052417755
Total epoch: 79. epoch loss: 0.20227499306201935
Total epoch: 80. epoch loss: 0.1997472196817398
Total epoch: 81. epoch loss: 0.19729307293891907
Total epoch: 82. epoch loss: 0.19491046667099
Total epoch: 83. epoch loss: 0.1925949603319168
Total epoch: 84. epoch loss: 0.19034425914287567
Total epoch: 85. epoch loss: 0.18815520405769348
Total epoch: 86. epoch loss: 0.1860259473323822
Total epoch: 87. epoch loss: 0.18394975364208221
Total epoch: 88. epoch loss: 0.18192951381206512
Total epoch: 89. epoch loss: 0.1799602061510086
Total epoch: 90. epoch loss: 0.17803992331027985
Total epoch: 91. epoch loss: 0.17616701126098633
Total epoch: 92. epoch loss: 0.17434152960777283
Total epoch: 93. epoch loss: 0.17255739867687225
Total epoch: 94. epoch loss: 0.17081564664840698
Total epoch: 95. epoch loss: 0.16911448538303375
Total epoch: 96. epoch loss: 0.1674519032239914
Total epoch: 97. epoch loss: 0.16582778096199036
Total epoch: 98. epoch loss: 0.1642381101846695
Total epoch: 99. epoch loss: 0.1626863032579422
Total epoch: 99. DecT loss: 0.1626863032579422
Training time: 0.35543298721313477
APL_precision: 0.33112582781456956, APL_recall: 0.29411764705882354, APL_f1: 0.31152647975077885, APL_number: 170
CMT_precision: 0.07927927927927927, CMT_recall: 0.22564102564102564, CMT_f1: 0.11733333333333333, CMT_number: 195
DSC_precision: 0.5294117647058824, DSC_recall: 0.20594965675057209, DSC_f1: 0.2965403624382208, DSC_number: 437
MAT_precision: 0.5697940503432495, MAT_recall: 0.3651026392961877, MAT_f1: 0.4450402144772118, MAT_number: 682
PRO_precision: 0.2907216494845361, PRO_recall: 0.1828793774319066, PRO_f1: 0.22452229299363058, PRO_number: 771
SMT_precision: 0.16449086161879894, SMT_recall: 0.3684210526315789, SMT_f1: 0.22743682310469313, SMT_number: 171
SPL_precision: 0.3302752293577982, SPL_recall: 0.48, SPL_f1: 0.391304347826087, SPL_number: 75
overall_precision: 0.2938864628820961, overall_recall: 0.26909236305477807, overall_f1: 0.28094343560843243, overall_accuracy: 0.7546279751268673
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:07:42 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:07:43 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:8, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1071.21it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3676.15 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:549: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:07:59 - INFO - __main__ - ***** Running training *****
05/31/2023 13:07:59 - INFO - __main__ -   Num examples = 21
05/31/2023 13:07:59 - INFO - __main__ -   Num Epochs = 35
05/31/2023 13:07:59 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:07:59 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:07:59 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:07:59 - INFO - __main__ -   Total optimization steps = 35
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.621950149536133
Total epoch: 1. epoch loss: 14.855944633483887
Total epoch: 2. epoch loss: 14.098182678222656
Total epoch: 3. epoch loss: 13.351595878601074
Total epoch: 4. epoch loss: 12.619659423828125
Total epoch: 5. epoch loss: 11.906238555908203
Total epoch: 6. epoch loss: 11.215353965759277
Total epoch: 7. epoch loss: 10.550697326660156
Total epoch: 8. epoch loss: 9.914901733398438
Total epoch: 9. epoch loss: 9.309276580810547
Total epoch: 10. epoch loss: 8.733738899230957
Total epoch: 11. epoch loss: 8.187345504760742
Total epoch: 12. epoch loss: 7.668821334838867
Total epoch: 13. epoch loss: 7.176910877227783
Total epoch: 14. epoch loss: 6.710650444030762
Total epoch: 15. epoch loss: 6.269379615783691
Total epoch: 16. epoch loss: 5.852507591247559
Total epoch: 17. epoch loss: 5.459534645080566
Total epoch: 18. epoch loss: 5.089807510375977
Total epoch: 19. epoch loss: 4.742642402648926
Total epoch: 20. epoch loss: 4.417259216308594
Total epoch: 21. epoch loss: 4.112661838531494
Total epoch: 22. epoch loss: 3.8278980255126953
Total epoch: 23. epoch loss: 3.5618960857391357
Total epoch: 24. epoch loss: 3.3136332035064697
Total epoch: 25. epoch loss: 3.0819740295410156
Total epoch: 26. epoch loss: 2.865922212600708
Total epoch: 27. epoch loss: 2.6644656658172607
Total epoch: 28. epoch loss: 2.4772496223449707
Total epoch: 29. epoch loss: 2.304016351699829
Total epoch: 30. epoch loss: 2.1442203521728516
Total epoch: 31. epoch loss: 1.997212529182434
Total epoch: 32. epoch loss: 1.8622865676879883
Total epoch: 33. epoch loss: 1.7386664152145386
Total epoch: 34. epoch loss: 1.6256299018859863
Total epoch: 34. DecT loss: 1.6256299018859863
Training time: 0.23694586753845215
APL_precision: 0.22529644268774704, APL_recall: 0.3352941176470588, APL_f1: 0.2695035460992908, APL_number: 170
CMT_precision: 0.04640371229698376, CMT_recall: 0.20512820512820512, CMT_f1: 0.07568590350047304, CMT_number: 195
DSC_precision: 0.42366412213740456, DSC_recall: 0.2540045766590389, DSC_f1: 0.31759656652360513, DSC_number: 437
MAT_precision: 0.45969125214408235, MAT_recall: 0.39296187683284456, MAT_f1: 0.42371541501976284, MAT_number: 682
PRO_precision: 0.3296875, PRO_recall: 0.27367055771725035, PRO_f1: 0.299078667611623, PRO_number: 771
SMT_precision: 0.12939001848428835, SMT_recall: 0.4093567251461988, SMT_f1: 0.19662921348314608, SMT_number: 171
SPL_precision: 0.3037037037037037, SPL_recall: 0.5466666666666666, SPL_f1: 0.3904761904761904, SPL_number: 75
overall_precision: 0.24358974358974358, overall_recall: 0.3190723710515794, overall_f1: 0.2762679591483469, overall_accuracy: 0.7339718390393825
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 971, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 794, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:14:15 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:14:16 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:150,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:2.0, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1183.49it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 382, in main
    model = AutoModelForMaskedLM.from_pretrained(
  File "/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 471, in from_pretrained
    return model_class.from_pretrained(
  File "/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2498, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1315, in __init__
    self.cls = BertOnlyMLMHead(config)
  File "/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 707, in __init__
    self.predictions = BertLMPredictionHead(config)
  File "/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 691, in __init__
    self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
  File "/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 90, in __init__
    self.reset_parameters()
  File "/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in reset_parameters
    init.kaiming_uniform_(self.weight, a=math.sqrt(5))
  File "/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/init.py", line 410, in kaiming_uniform_
    return tensor.uniform_(-bound, bound)
KeyboardInterrupt
