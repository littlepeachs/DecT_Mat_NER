/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:33:00 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:33:01 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-90b165f8431f5e0a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:8, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1281.49it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/24 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:33:29 - INFO - __main__ - ***** Running training *****
05/30/2023 12:33:29 - INFO - __main__ -   Num examples = 24
05/30/2023 12:33:29 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:33:29 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:33:29 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:33:29 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:33:29 - INFO - __main__ -   Total optimization steps = 35
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.565006256103516
Total epoch: 1. epoch loss: 14.88126277923584
Total epoch: 2. epoch loss: 14.205647468566895
Total epoch: 3. epoch loss: 13.539329528808594
Total epoch: 4. epoch loss: 12.883416175842285
Total epoch: 5. epoch loss: 12.239055633544922
Total epoch: 6. epoch loss: 11.607537269592285
Total epoch: 7. epoch loss: 10.990333557128906
Total epoch: 8. epoch loss: 10.38900375366211
Total epoch: 9. epoch loss: 9.805131912231445
Total epoch: 10. epoch loss: 9.24022388458252
Total epoch: 11. epoch loss: 8.695666313171387
Total epoch: 12. epoch loss: 8.172657012939453
Total epoch: 13. epoch loss: 7.672110557556152
Total epoch: 14. epoch loss: 7.194633483886719
Total epoch: 15. epoch loss: 6.740543365478516
Total epoch: 16. epoch loss: 6.309898376464844
Total epoch: 17. epoch loss: 5.902575969696045
Total epoch: 18. epoch loss: 5.518240451812744
Total epoch: 19. epoch loss: 5.156327247619629
Total epoch: 20. epoch loss: 4.816173553466797
Total epoch: 21. epoch loss: 4.496922969818115
Total epoch: 22. epoch loss: 4.1976165771484375
Total epoch: 23. epoch loss: 3.9171090126037598
Total epoch: 24. epoch loss: 3.654228448867798
Total epoch: 25. epoch loss: 3.407761573791504
Total epoch: 26. epoch loss: 3.176607608795166
Total epoch: 27. epoch loss: 2.9597678184509277
Total epoch: 28. epoch loss: 2.756446599960327
Total epoch: 29. epoch loss: 2.5665714740753174
Total epoch: 30. epoch loss: 2.3900883197784424
Total epoch: 31. epoch loss: 2.2266721725463867
Total epoch: 32. epoch loss: 2.0759198665618896
Total epoch: 33. epoch loss: 1.9373087882995605
Total epoch: 34. epoch loss: 1.8102138042449951
Total epoch: 34. DecT loss: 1.8102138042449951
Training time: 0.18682479858398438
APL_precision: 0.16113744075829384, APL_recall: 0.4, APL_f1: 0.22972972972972971, APL_number: 170
CMT_precision: 0.2882882882882883, CMT_recall: 0.3282051282051282, CMT_f1: 0.30695443645083936, CMT_number: 195
DSC_precision: 0.2479740680713128, DSC_recall: 0.3501144164759725, DSC_f1: 0.2903225806451613, DSC_number: 437
MAT_precision: 0.5250569476082004, MAT_recall: 0.6759530791788856, MAT_f1: 0.5910256410256409, MAT_number: 682
PRO_precision: 0.3140495867768595, PRO_recall: 0.29571984435797666, PRO_f1: 0.3046092184368738, PRO_number: 771
SMT_precision: 0.19714285714285715, SMT_recall: 0.40350877192982454, SMT_f1: 0.26487523992322454, SMT_number: 171
SPL_precision: 0.21774193548387097, SPL_recall: 0.36, SPL_f1: 0.271356783919598, SPL_number: 75
overall_precision: 0.32045522611560345, overall_recall: 0.42782886845261897, overall_f1: 0.3664383561643835, overall_accuracy: 0.783360731899078
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:36:54 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:36:55 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-90b165f8431f5e0a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:8, proto_dim:128, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1125.08it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/24 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:37:02 - INFO - __main__ - ***** Running training *****
05/30/2023 12:37:02 - INFO - __main__ -   Num examples = 24
05/30/2023 12:37:02 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:37:02 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:37:02 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:37:02 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:37:02 - INFO - __main__ -   Total optimization steps = 100
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.542598724365234
Total epoch: 1. epoch loss: 14.582524299621582
Total epoch: 2. epoch loss: 13.639612197875977
Total epoch: 3. epoch loss: 12.716856002807617
Total epoch: 4. epoch loss: 11.817760467529297
Total epoch: 5. epoch loss: 10.94632625579834
Total epoch: 6. epoch loss: 10.106966972351074
Total epoch: 7. epoch loss: 9.304145812988281
Total epoch: 8. epoch loss: 8.54202938079834
Total epoch: 9. epoch loss: 7.824014186859131
Total epoch: 10. epoch loss: 7.15241813659668
Total epoch: 11. epoch loss: 6.528392314910889
Total epoch: 12. epoch loss: 5.951920509338379
Total epoch: 13. epoch loss: 5.4220147132873535
Total epoch: 14. epoch loss: 4.93695068359375
Total epoch: 15. epoch loss: 4.494417190551758
Total epoch: 16. epoch loss: 4.091598987579346
Total epoch: 17. epoch loss: 3.725205421447754
Total epoch: 18. epoch loss: 3.3917078971862793
Total epoch: 19. epoch loss: 3.087569236755371
Total epoch: 20. epoch loss: 2.8096065521240234
Total epoch: 21. epoch loss: 2.5558786392211914
Total epoch: 22. epoch loss: 2.3253560066223145
Total epoch: 23. epoch loss: 2.116999864578247
Total epoch: 24. epoch loss: 1.9296281337738037
Total epoch: 25. epoch loss: 1.7619796991348267
Total epoch: 26. epoch loss: 1.6126494407653809
Total epoch: 27. epoch loss: 1.4801762104034424
Total epoch: 28. epoch loss: 1.3630162477493286
Total epoch: 29. epoch loss: 1.2596558332443237
Total epoch: 30. epoch loss: 1.1686062812805176
Total epoch: 31. epoch loss: 1.088407039642334
Total epoch: 32. epoch loss: 1.0176972150802612
Total epoch: 33. epoch loss: 0.955200731754303
Total epoch: 34. epoch loss: 0.8997400999069214
Total epoch: 35. epoch loss: 0.8503085374832153
Total epoch: 36. epoch loss: 0.806024968624115
Total epoch: 37. epoch loss: 0.7661499977111816
Total epoch: 38. epoch loss: 0.7300843000411987
Total epoch: 39. epoch loss: 0.6973282098770142
Total epoch: 40. epoch loss: 0.6674706339836121
Total epoch: 41. epoch loss: 0.6401672959327698
Total epoch: 42. epoch loss: 0.6151160001754761
Total epoch: 43. epoch loss: 0.5920637845993042
Total epoch: 44. epoch loss: 0.5707869529724121
Total epoch: 45. epoch loss: 0.5511052012443542
Total epoch: 46. epoch loss: 0.5328429341316223
Total epoch: 47. epoch loss: 0.5158649682998657
Total epoch: 48. epoch loss: 0.5000478029251099
Total epoch: 49. epoch loss: 0.48528698086738586
Total epoch: 50. epoch loss: 0.4714883863925934
Total epoch: 51. epoch loss: 0.4585581421852112
Total epoch: 52. epoch loss: 0.44642576575279236
Total epoch: 53. epoch loss: 0.4350176453590393
Total epoch: 54. epoch loss: 0.424266517162323
Total epoch: 55. epoch loss: 0.4141179919242859
Total epoch: 56. epoch loss: 0.4045199155807495
Total epoch: 57. epoch loss: 0.3954290747642517
Total epoch: 58. epoch loss: 0.38680577278137207
Total epoch: 59. epoch loss: 0.3786153197288513
Total epoch: 60. epoch loss: 0.3708219528198242
Total epoch: 61. epoch loss: 0.3634036183357239
Total epoch: 62. epoch loss: 0.3563271760940552
Total epoch: 63. epoch loss: 0.3495739698410034
Total epoch: 64. epoch loss: 0.34311816096305847
Total epoch: 65. epoch loss: 0.3369363844394684
Total epoch: 66. epoch loss: 0.33101457357406616
Total epoch: 67. epoch loss: 0.32533368468284607
Total epoch: 68. epoch loss: 0.31987980008125305
Total epoch: 69. epoch loss: 0.3146374821662903
Total epoch: 70. epoch loss: 0.3095933794975281
Total epoch: 71. epoch loss: 0.3047367036342621
Total epoch: 72. epoch loss: 0.3000544309616089
Total epoch: 73. epoch loss: 0.2955407500267029
Total epoch: 74. epoch loss: 0.2911812365055084
Total epoch: 75. epoch loss: 0.28696882724761963
Total epoch: 76. epoch loss: 0.28289657831192017
Total epoch: 77. epoch loss: 0.2789553701877594
Total epoch: 78. epoch loss: 0.27513715624809265
Total epoch: 79. epoch loss: 0.27144020795822144
Total epoch: 80. epoch loss: 0.267855703830719
Total epoch: 81. epoch loss: 0.26437807083129883
Total epoch: 82. epoch loss: 0.2610008120536804
Total epoch: 83. epoch loss: 0.25772106647491455
Total epoch: 84. epoch loss: 0.25453197956085205
Total epoch: 85. epoch loss: 0.25143328309059143
Total epoch: 86. epoch loss: 0.24841591715812683
Total epoch: 87. epoch loss: 0.2454792708158493
Total epoch: 88. epoch loss: 0.24261850118637085
Total epoch: 89. epoch loss: 0.2398323118686676
Total epoch: 90. epoch loss: 0.2371135950088501
Total epoch: 91. epoch loss: 0.23446336388587952
Total epoch: 92. epoch loss: 0.2318762093782425
Total epoch: 93. epoch loss: 0.22935093939304352
Total epoch: 94. epoch loss: 0.22688482701778412
Total epoch: 95. epoch loss: 0.2244759500026703
Total epoch: 96. epoch loss: 0.2221207320690155
Total epoch: 97. epoch loss: 0.21981801092624664
Total epoch: 98. epoch loss: 0.21756616234779358
Total epoch: 99. epoch loss: 0.21536244451999664
Total epoch: 99. DecT loss: 0.21536244451999664
Training time: 0.4758322238922119
APL_precision: 0.20819112627986347, APL_recall: 0.3588235294117647, APL_f1: 0.2634989200863931, APL_number: 170
CMT_precision: 0.2931937172774869, CMT_recall: 0.28717948717948716, CMT_f1: 0.29015544041450775, CMT_number: 195
DSC_precision: 0.3893333333333333, DSC_recall: 0.3340961098398169, DSC_f1: 0.35960591133004927, DSC_number: 437
MAT_precision: 0.5919055649241147, MAT_recall: 0.5146627565982405, MAT_f1: 0.5505882352941176, MAT_number: 682
PRO_precision: 0.351409978308026, PRO_recall: 0.21011673151750973, PRO_f1: 0.262987012987013, PRO_number: 771
SMT_precision: 0.20212765957446807, SMT_recall: 0.3333333333333333, SMT_f1: 0.2516556291390728, SMT_number: 171
SPL_precision: 0.3372093023255814, SPL_recall: 0.38666666666666666, SPL_f1: 0.36024844720496896, SPL_number: 75
overall_precision: 0.37790442788250767, overall_recall: 0.34466213514594163, overall_f1: 0.36051861145964037, overall_accuracy: 0.7845043242084196
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:39:03 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:39:04 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-90b165f8431f5e0a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1102.89it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/24 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:40:30 - INFO - __main__ - ***** Running training *****
05/30/2023 12:40:30 - INFO - __main__ -   Num examples = 24
05/30/2023 12:40:30 - INFO - __main__ -   Num Epochs = 150
05/30/2023 12:40:30 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:40:30 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:40:30 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:40:30 - INFO - __main__ -   Total optimization steps = 150
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/150 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 15.5235595703125
Total epoch: 1. epoch loss: 14.453498840332031
Total epoch: 2. epoch loss: 13.406646728515625
Total epoch: 3. epoch loss: 12.386613845825195
Total epoch: 4. epoch loss: 11.39810562133789
Total epoch: 5. epoch loss: 10.446918487548828
Total epoch: 6. epoch loss: 9.539267539978027
Total epoch: 7. epoch loss: 8.68097972869873
Total epoch: 8. epoch loss: 7.876752853393555
Total epoch: 9. epoch loss: 7.129662990570068
Total epoch: 10. epoch loss: 6.441072463989258
Total epoch: 11. epoch loss: 5.810704231262207
Total epoch: 12. epoch loss: 5.237061500549316
Total epoch: 13. epoch loss: 4.717728614807129
Total epoch: 14. epoch loss: 4.24948787689209
Total epoch: 15. epoch loss: 3.8284337520599365
Total epoch: 16. epoch loss: 3.4500339031219482
Total epoch: 17. epoch loss: 3.1094563007354736
Total epoch: 18. epoch loss: 2.8020846843719482
Total epoch: 19. epoch loss: 2.524778366088867
Total epoch: 20. epoch loss: 2.2756001949310303
Total epoch: 21. epoch loss: 2.052785634994507
Total epoch: 22. epoch loss: 1.8545913696289062
Total epoch: 23. epoch loss: 1.6791735887527466
Total epoch: 24. epoch loss: 1.5246447324752808
Total epoch: 25. epoch loss: 1.3890751600265503
Total epoch: 26. epoch loss: 1.2705409526824951
Total epoch: 27. epoch loss: 1.1671602725982666
Total epoch: 28. epoch loss: 1.0771174430847168
Total epoch: 29. epoch loss: 0.9986817240715027
Total epoch: 30. epoch loss: 0.9302397966384888
Total epoch: 31. epoch loss: 0.8703038096427917
Total epoch: 32. epoch loss: 0.817552387714386
Total epoch: 33. epoch loss: 0.7708582878112793
Total epoch: 34. epoch loss: 0.7292641997337341
Total epoch: 35. epoch loss: 0.6919972896575928
Total epoch: 36. epoch loss: 0.658443033695221
Total epoch: 37. epoch loss: 0.6280819773674011
Total epoch: 38. epoch loss: 0.6005049347877502
Total epoch: 39. epoch loss: 0.5753625631332397
Total epoch: 40. epoch loss: 0.5523501634597778
Total epoch: 41. epoch loss: 0.5312250852584839
Total epoch: 42. epoch loss: 0.5117639899253845
Total epoch: 43. epoch loss: 0.4937867224216461
Total epoch: 44. epoch loss: 0.47714102268218994
Total epoch: 45. epoch loss: 0.461687833070755
Total epoch: 46. epoch loss: 0.44731780886650085
Total epoch: 47. epoch loss: 0.4339236319065094
Total epoch: 48. epoch loss: 0.4214170277118683
Total epoch: 49. epoch loss: 0.40971675515174866
Total epoch: 50. epoch loss: 0.3987482786178589
Total epoch: 51. epoch loss: 0.3884442150592804
Total epoch: 52. epoch loss: 0.37874552607536316
Total epoch: 53. epoch loss: 0.3696012496948242
Total epoch: 54. epoch loss: 0.3609688878059387
Total epoch: 55. epoch loss: 0.35280412435531616
Total epoch: 56. epoch loss: 0.3450692296028137
Total epoch: 57. epoch loss: 0.3377329409122467
Total epoch: 58. epoch loss: 0.33076727390289307
Total epoch: 59. epoch loss: 0.3241400718688965
Total epoch: 60. epoch loss: 0.31782662868499756
Total epoch: 61. epoch loss: 0.3118021786212921
Total epoch: 62. epoch loss: 0.3060489594936371
Total epoch: 63. epoch loss: 0.30054572224617004
Total epoch: 64. epoch loss: 0.29527726769447327
Total epoch: 65. epoch loss: 0.290226012468338
Total epoch: 66. epoch loss: 0.2853800654411316
Total epoch: 67. epoch loss: 0.28072577714920044
Total epoch: 68. epoch loss: 0.2762485444545746
Total epoch: 69. epoch loss: 0.2719380855560303
Total epoch: 70. epoch loss: 0.2677878737449646
Total epoch: 71. epoch loss: 0.2637862265110016
Total epoch: 72. epoch loss: 0.2599214017391205
Total epoch: 73. epoch loss: 0.25618916749954224
Total epoch: 74. epoch loss: 0.25258177518844604
Total epoch: 75. epoch loss: 0.2490939199924469
Total epoch: 76. epoch loss: 0.24571484327316284
Total epoch: 77. epoch loss: 0.24244424700737
Total epoch: 78. epoch loss: 0.2392699271440506
Total epoch: 79. epoch loss: 0.23619520664215088
Total epoch: 80. epoch loss: 0.23320958018302917
Total epoch: 81. epoch loss: 0.23030923306941986
Total epoch: 82. epoch loss: 0.22749045491218567
Total epoch: 83. epoch loss: 0.22474920749664307
Total epoch: 84. epoch loss: 0.22208306193351746
Total epoch: 85. epoch loss: 0.21948568522930145
Total epoch: 86. epoch loss: 0.2169562429189682
Total epoch: 87. epoch loss: 0.21449168026447296
Total epoch: 88. epoch loss: 0.21208921074867249
Total epoch: 89. epoch loss: 0.20974455773830414
Total epoch: 90. epoch loss: 0.20745670795440674
Total epoch: 91. epoch loss: 0.20522233843803406
Total epoch: 92. epoch loss: 0.20303954184055328
Total epoch: 93. epoch loss: 0.20090699195861816
Total epoch: 94. epoch loss: 0.1988212615251541
Total epoch: 95. epoch loss: 0.1967824548482895
Total epoch: 96. epoch loss: 0.19478839635849
Total epoch: 97. epoch loss: 0.1928369402885437
Total epoch: 98. epoch loss: 0.19092510640621185
Total epoch: 99. epoch loss: 0.18905454874038696
Total epoch: 100. epoch loss: 0.18722131848335266
Total epoch: 101. epoch loss: 0.18542495369911194
Total epoch: 102. epoch loss: 0.18366383016109467
Total epoch: 103. epoch loss: 0.18193954229354858
Total epoch: 104. epoch loss: 0.18024614453315735
Total epoch: 105. epoch loss: 0.1785862147808075
Total epoch: 106. epoch loss: 0.17695698142051697
Total epoch: 107. epoch loss: 0.1753591150045395
Total epoch: 108. epoch loss: 0.17378894984722137
Total epoch: 109. epoch loss: 0.17224931716918945
Total epoch: 110. epoch loss: 0.17073588073253632
Total epoch: 111. epoch loss: 0.1692502200603485
Total epoch: 112. epoch loss: 0.16778932511806488
Total epoch: 113. epoch loss: 0.16635650396347046
Total epoch: 114. epoch loss: 0.1649477481842041
Total epoch: 115. epoch loss: 0.16356107592582703
Total epoch: 116. epoch loss: 0.16219812631607056
Total epoch: 117. epoch loss: 0.16086018085479736
Total epoch: 118. epoch loss: 0.15954262018203735
Total epoch: 119. epoch loss: 0.1582472175359726
Total epoch: 120. epoch loss: 0.15697260200977325
Total epoch: 121. epoch loss: 0.1557193398475647
Total epoch: 122. epoch loss: 0.15448486804962158
Total epoch: 123. epoch loss: 0.15327104926109314
Total epoch: 124. epoch loss: 0.1520763635635376
Total epoch: 125. epoch loss: 0.1508992612361908
Total epoch: 126. epoch loss: 0.14974060654640198
Total epoch: 127. epoch loss: 0.14860136806964874
Total epoch: 128. epoch loss: 0.14747679233551025
Total epoch: 129. epoch loss: 0.14637097716331482
Total epoch: 130. epoch loss: 0.14528155326843262
Total epoch: 131. epoch loss: 0.1442078948020935
Total epoch: 132. epoch loss: 0.14315004646778107
Total epoch: 133. epoch loss: 0.14210861921310425
Total epoch: 134. epoch loss: 0.14108151197433472
Total epoch: 135. epoch loss: 0.14007022976875305
Total epoch: 136. epoch loss: 0.13907279074192047
Total epoch: 137. epoch loss: 0.13808861374855042
Total epoch: 138. epoch loss: 0.1371191442012787
Total epoch: 139. epoch loss: 0.13616427779197693
Total epoch: 140. epoch loss: 0.13522221148014069
Total epoch: 141. epoch loss: 0.13429412245750427
Total epoch: 142. epoch loss: 0.13337723910808563
Total epoch: 143. epoch loss: 0.13247407972812653
Total epoch: 144. epoch loss: 0.13158272206783295
Total epoch: 145. epoch loss: 0.13070444762706757
Total epoch: 146. epoch loss: 0.12983708083629608
Total epoch: 147. epoch loss: 0.12898223102092743
Total epoch: 148. epoch loss: 0.1281387358903885
Total epoch: 149. epoch loss: 0.12730622291564941
Total epoch: 149. DecT loss: 0.12730622291564941
Training time: 0.6377294063568115
APL_precision: 0.2222222222222222, APL_recall: 0.36470588235294116, APL_f1: 0.27616926503340755, APL_number: 170
CMT_precision: 0.2777777777777778, CMT_recall: 0.28205128205128205, CMT_f1: 0.27989821882951654, CMT_number: 195
DSC_precision: 0.39325842696629215, DSC_recall: 0.32036613272311215, DSC_f1: 0.3530895334174023, DSC_number: 437
MAT_precision: 0.5938697318007663, MAT_recall: 0.45454545454545453, MAT_f1: 0.5149501661129567, MAT_number: 682
PRO_precision: 0.3287981859410431, PRO_recall: 0.1880674448767834, PRO_f1: 0.23927392739273926, PRO_number: 771
SMT_precision: 0.21691176470588236, SMT_recall: 0.34502923976608185, SMT_f1: 0.2663656884875847, SMT_number: 171
SPL_precision: 0.37662337662337664, SPL_recall: 0.38666666666666666, SPL_f1: 0.381578947368421, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.372960372960373, overall_recall: 0.3198720511795282, overall_f1: 0.34438226431338786, overall_accuracy: 0.7810735472803946
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/150 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:45:12 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:45:13 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-90b165f8431f5e0a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.008, batch_size:32, shot:8, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1151.81it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/24 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:45:20 - INFO - __main__ - ***** Running training *****
05/30/2023 12:45:20 - INFO - __main__ -   Num examples = 24
05/30/2023 12:45:20 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:45:20 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:45:20 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:45:20 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:45:20 - INFO - __main__ -   Total optimization steps = 100
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.565006256103516
Total epoch: 1. epoch loss: 14.47504997253418
Total epoch: 2. epoch loss: 13.408926963806152
Total epoch: 3. epoch loss: 12.371368408203125
Total epoch: 4. epoch loss: 11.367459297180176
Total epoch: 5. epoch loss: 10.403132438659668
Total epoch: 6. epoch loss: 9.484621047973633
Total epoch: 7. epoch loss: 8.6177396774292
Total epoch: 8. epoch loss: 7.807050704956055
Total epoch: 9. epoch loss: 7.055383205413818
Total epoch: 10. epoch loss: 6.363803386688232
Total epoch: 11. epoch loss: 5.731661796569824
Total epoch: 12. epoch loss: 5.157182693481445
Total epoch: 13. epoch loss: 4.637698650360107
Total epoch: 14. epoch loss: 4.1699323654174805
Total epoch: 15. epoch loss: 3.749880790710449
Total epoch: 16. epoch loss: 3.3728950023651123
Total epoch: 17. epoch loss: 3.033951997756958
Total epoch: 18. epoch loss: 2.7283568382263184
Total epoch: 19. epoch loss: 2.4533402919769287
Total epoch: 20. epoch loss: 2.2069828510284424
Total epoch: 21. epoch loss: 1.9874111413955688
Total epoch: 22. epoch loss: 1.792729139328003
Total epoch: 23. epoch loss: 1.6208736896514893
Total epoch: 24. epoch loss: 1.469862461090088
Total epoch: 25. epoch loss: 1.3376349210739136
Total epoch: 26. epoch loss: 1.2222541570663452
Total epoch: 27. epoch loss: 1.1218026876449585
Total epoch: 28. epoch loss: 1.0344517230987549
Total epoch: 29. epoch loss: 0.9584557414054871
Total epoch: 30. epoch loss: 0.892208456993103
Total epoch: 31. epoch loss: 0.8342302441596985
Total epoch: 32. epoch loss: 0.7832359075546265
Total epoch: 33. epoch loss: 0.7381365895271301
Total epoch: 34. epoch loss: 0.698016881942749
Total epoch: 35. epoch loss: 0.6621408462524414
Total epoch: 36. epoch loss: 0.6298911571502686
Total epoch: 37. epoch loss: 0.6007588505744934
Total epoch: 38. epoch loss: 0.5743464827537537
Total epoch: 39. epoch loss: 0.5502761006355286
Total epoch: 40. epoch loss: 0.5282600522041321
Total epoch: 41. epoch loss: 0.5080459713935852
Total epoch: 42. epoch loss: 0.48942261934280396
Total epoch: 43. epoch loss: 0.47220703959465027
Total epoch: 44. epoch loss: 0.4562608599662781
Total epoch: 45. epoch loss: 0.441448450088501
Total epoch: 46. epoch loss: 0.4276541471481323
Total epoch: 47. epoch loss: 0.41478824615478516
Total epoch: 48. epoch loss: 0.4027572572231293
Total epoch: 49. epoch loss: 0.3914969265460968
Total epoch: 50. epoch loss: 0.38093841075897217
Total epoch: 51. epoch loss: 0.3710196018218994
Total epoch: 52. epoch loss: 0.36169669032096863
Total epoch: 53. epoch loss: 0.3529183268547058
Total epoch: 54. epoch loss: 0.3446439206600189
Total epoch: 55. epoch loss: 0.3368355929851532
Total epoch: 56. epoch loss: 0.32944661378860474
Total epoch: 57. epoch loss: 0.3224485218524933
Total epoch: 58. epoch loss: 0.31580960750579834
Total epoch: 59. epoch loss: 0.3094940483570099
Total epoch: 60. epoch loss: 0.30348116159439087
Total epoch: 61. epoch loss: 0.29774755239486694
Total epoch: 62. epoch loss: 0.292270302772522
Total epoch: 63. epoch loss: 0.28703299164772034
Total epoch: 64. epoch loss: 0.28201213479042053
Total epoch: 65. epoch loss: 0.2772035598754883
Total epoch: 66. epoch loss: 0.27258801460266113
Total epoch: 67. epoch loss: 0.2681541442871094
Total epoch: 68. epoch loss: 0.26389041543006897
Total epoch: 69. epoch loss: 0.25978800654411316
Total epoch: 70. epoch loss: 0.2558342516422272
Total epoch: 71. epoch loss: 0.25202640891075134
Total epoch: 72. epoch loss: 0.24835233390331268
Total epoch: 73. epoch loss: 0.24480493366718292
Total epoch: 74. epoch loss: 0.24137578904628754
Total epoch: 75. epoch loss: 0.23806223273277283
Total epoch: 76. epoch loss: 0.23485490679740906
Total epoch: 77. epoch loss: 0.23174796998500824
Total epoch: 78. epoch loss: 0.22873643040657043
Total epoch: 79. epoch loss: 0.22581669688224792
Total epoch: 80. epoch loss: 0.22298285365104675
Total epoch: 81. epoch loss: 0.2202296108007431
Total epoch: 82. epoch loss: 0.21755406260490417
Total epoch: 83. epoch loss: 0.2149532586336136
Total epoch: 84. epoch loss: 0.21241897344589233
Total epoch: 85. epoch loss: 0.20995502173900604
Total epoch: 86. epoch loss: 0.2075539231300354
Total epoch: 87. epoch loss: 0.20521414279937744
Total epoch: 88. epoch loss: 0.20293371379375458
Total epoch: 89. epoch loss: 0.20070558786392212
Total epoch: 90. epoch loss: 0.19853301346302032
Total epoch: 91. epoch loss: 0.19641254842281342
Total epoch: 92. epoch loss: 0.19433853030204773
Total epoch: 93. epoch loss: 0.19231431186199188
Total epoch: 94. epoch loss: 0.1903339922428131
Total epoch: 95. epoch loss: 0.18839745223522186
Total epoch: 96. epoch loss: 0.18650497496128082
Total epoch: 97. epoch loss: 0.18464882671833038
Total epoch: 98. epoch loss: 0.18283455073833466
Total epoch: 99. epoch loss: 0.18105657398700714
Total epoch: 99. DecT loss: 0.18105657398700714
Training time: 0.4478318691253662
APL_precision: 0.20833333333333334, APL_recall: 0.35294117647058826, APL_f1: 0.26200873362445415, APL_number: 170
CMT_precision: 0.2925531914893617, CMT_recall: 0.28205128205128205, CMT_f1: 0.28720626631853785, CMT_number: 195
DSC_precision: 0.38337801608579086, DSC_recall: 0.32723112128146453, DSC_f1: 0.3530864197530864, DSC_number: 437
MAT_precision: 0.5874125874125874, MAT_recall: 0.49266862170087977, MAT_f1: 0.5358851674641147, MAT_number: 682
PRO_precision: 0.34451901565995524, PRO_recall: 0.19974059662775617, PRO_f1: 0.2528735632183908, PRO_number: 771
SMT_precision: 0.22140221402214022, SMT_recall: 0.3508771929824561, SMT_f1: 0.27149321266968324, SMT_number: 171
SPL_precision: 0.3253012048192771, SPL_recall: 0.36, SPL_f1: 0.34177215189873417, SPL_number: 75
overall_precision: 0.3757875787578758, overall_recall: 0.3338664534186325, overall_f1: 0.3535888206648316, overall_accuracy: 0.7832892573797441
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:07:42 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:07:44 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-90b165f8431f5e0a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:8, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1046.74it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/24 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4456.33 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:549: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:08:01 - INFO - __main__ - ***** Running training *****
05/31/2023 13:08:01 - INFO - __main__ -   Num examples = 24
05/31/2023 13:08:01 - INFO - __main__ -   Num Epochs = 35
05/31/2023 13:08:01 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:08:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:08:01 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:08:01 - INFO - __main__ -   Total optimization steps = 35
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.572137832641602
Total epoch: 1. epoch loss: 14.890909194946289
Total epoch: 2. epoch loss: 14.217843055725098
Total epoch: 3. epoch loss: 13.553807258605957
Total epoch: 4. epoch loss: 12.899667739868164
Total epoch: 5. epoch loss: 12.25648021697998
Total epoch: 6. epoch loss: 11.6255464553833
Total epoch: 7. epoch loss: 11.008368492126465
Total epoch: 8. epoch loss: 10.406634330749512
Total epoch: 9. epoch loss: 9.822044372558594
Total epoch: 10. epoch loss: 9.256300926208496
Total epoch: 11. epoch loss: 8.71086597442627
Total epoch: 12. epoch loss: 8.187044143676758
Total epoch: 13. epoch loss: 7.685830116271973
Total epoch: 14. epoch loss: 7.207918167114258
Total epoch: 15. epoch loss: 6.753708362579346
Total epoch: 16. epoch loss: 6.323317050933838
Total epoch: 17. epoch loss: 5.9166460037231445
Total epoch: 18. epoch loss: 5.533302307128906
Total epoch: 19. epoch loss: 5.172747611999512
Total epoch: 20. epoch loss: 4.834233283996582
Total epoch: 21. epoch loss: 4.5167436599731445
Total epoch: 22. epoch loss: 4.219192981719971
Total epoch: 23. epoch loss: 3.940321683883667
Total epoch: 24. epoch loss: 3.6788790225982666
Total epoch: 25. epoch loss: 3.433598041534424
Total epoch: 26. epoch loss: 3.203310966491699
Total epoch: 27. epoch loss: 2.987042188644409
Total epoch: 28. epoch loss: 2.7839415073394775
Total epoch: 29. epoch loss: 2.5938851833343506
Total epoch: 30. epoch loss: 2.4168689250946045
Total epoch: 31. epoch loss: 2.2526986598968506
Total epoch: 32. epoch loss: 2.1010336875915527
Total epoch: 33. epoch loss: 1.961419701576233
Total epoch: 34. epoch loss: 1.8333182334899902
Total epoch: 34. DecT loss: 1.8333182334899902
Training time: 0.2430574893951416
APL_precision: 0.15550239234449761, APL_recall: 0.38235294117647056, APL_f1: 0.22108843537414966, APL_number: 170
CMT_precision: 0.2727272727272727, CMT_recall: 0.3230769230769231, CMT_f1: 0.29577464788732394, CMT_number: 195
DSC_precision: 0.22345483359746435, DSC_recall: 0.32265446224256294, DSC_f1: 0.2640449438202247, DSC_number: 437
MAT_precision: 0.5267441860465116, MAT_recall: 0.6642228739002932, MAT_f1: 0.5875486381322957, MAT_number: 682
PRO_precision: 0.31145251396648044, PRO_recall: 0.2892347600518807, PRO_f1: 0.2999327505043713, PRO_number: 771
SMT_precision: 0.2084592145015106, SMT_recall: 0.40350877192982454, SMT_f1: 0.27490039840637454, SMT_number: 171
SPL_precision: 0.2222222222222222, SPL_recall: 0.3466666666666667, SPL_f1: 0.2708333333333333, SPL_number: 75
overall_precision: 0.31476997578692495, overall_recall: 0.41583366653338666, overall_f1: 0.35831180017226527, overall_accuracy: 0.7818597669930669
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 971, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 794, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:14:15 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:14:17 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-90b165f8431f5e0a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:150,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:2.0, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1209.43it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/24 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5037.04 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:14:24 - INFO - __main__ - ***** Running training *****
05/31/2023 13:14:24 - INFO - __main__ -   Num examples = 24
05/31/2023 13:14:24 - INFO - __main__ -   Num Epochs = 150
05/31/2023 13:14:24 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:14:24 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:14:24 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:14:24 - INFO - __main__ -   Total optimization steps = 150
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/150 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 16.368276596069336
Total epoch: 1. epoch loss: 15.263809204101562
Total epoch: 2. epoch loss: 14.176420211791992
Total epoch: 3. epoch loss: 13.11154842376709
Total epoch: 4. epoch loss: 12.076478958129883
Total epoch: 5. epoch loss: 11.078965187072754
Total epoch: 6. epoch loss: 10.126335144042969
Total epoch: 7. epoch loss: 9.2247953414917
Total epoch: 8. epoch loss: 8.379169464111328
Total epoch: 9. epoch loss: 7.59272575378418
Total epoch: 10. epoch loss: 6.866999626159668
Total epoch: 11. epoch loss: 6.20196008682251
Total epoch: 12. epoch loss: 5.596218585968018
Total epoch: 13. epoch loss: 5.047352313995361
Total epoch: 14. epoch loss: 4.552072525024414
Total epoch: 15. epoch loss: 4.106259822845459
Total epoch: 16. epoch loss: 3.705085515975952
Total epoch: 17. epoch loss: 3.3434841632843018
Total epoch: 18. epoch loss: 3.016613245010376
Total epoch: 19. epoch loss: 2.720404624938965
Total epoch: 20. epoch loss: 2.452951669692993
Total epoch: 21. epoch loss: 2.2127835750579834
Total epoch: 22. epoch loss: 1.9983627796173096
Total epoch: 23. epoch loss: 1.808014988899231
Total epoch: 24. epoch loss: 1.6399224996566772
Total epoch: 25. epoch loss: 1.4921737909317017
Total epoch: 26. epoch loss: 1.3628350496292114
Total epoch: 27. epoch loss: 1.249969244003296
Total epoch: 28. epoch loss: 1.1516876220703125
Total epoch: 29. epoch loss: 1.0661559104919434
Total epoch: 30. epoch loss: 0.9916380047798157
Total epoch: 31. epoch loss: 0.9265169501304626
Total epoch: 32. epoch loss: 0.8693270087242126
Total epoch: 33. epoch loss: 0.8188109993934631
Total epoch: 34. epoch loss: 0.7738907337188721
Total epoch: 35. epoch loss: 0.7337120175361633
Total epoch: 36. epoch loss: 0.6975488066673279
Total epoch: 37. epoch loss: 0.6648741960525513
Total epoch: 38. epoch loss: 0.6352145671844482
Total epoch: 39. epoch loss: 0.6082057356834412
Total epoch: 40. epoch loss: 0.583524227142334
Total epoch: 41. epoch loss: 0.5608941316604614
Total epoch: 42. epoch loss: 0.5400792360305786
Total epoch: 43. epoch loss: 0.5208812355995178
Total epoch: 44. epoch loss: 0.503126323223114
Total epoch: 45. epoch loss: 0.4866664409637451
Total epoch: 46. epoch loss: 0.47137677669525146
Total epoch: 47. epoch loss: 0.4571424722671509
Total epoch: 48. epoch loss: 0.443865567445755
Total epoch: 49. epoch loss: 0.4314540922641754
Total epoch: 50. epoch loss: 0.41982316970825195
Total epoch: 51. epoch loss: 0.408904492855072
Total epoch: 52. epoch loss: 0.39863064885139465
Total epoch: 53. epoch loss: 0.38894426822662354
Total epoch: 54. epoch loss: 0.3797985911369324
Total epoch: 55. epoch loss: 0.37114500999450684
Total epoch: 56. epoch loss: 0.3629511594772339
Total epoch: 57. epoch loss: 0.35517746210098267
Total epoch: 58. epoch loss: 0.34779804944992065
Total epoch: 59. epoch loss: 0.3407825529575348
Total epoch: 60. epoch loss: 0.33409878611564636
Total epoch: 61. epoch loss: 0.32773056626319885
Total epoch: 62. epoch loss: 0.32165592908859253
Total epoch: 63. epoch loss: 0.3158491551876068
Total epoch: 64. epoch loss: 0.31029361486434937
Total epoch: 65. epoch loss: 0.30497410893440247
Total epoch: 66. epoch loss: 0.299873024225235
Total epoch: 67. epoch loss: 0.29497599601745605
Total epoch: 68. epoch loss: 0.2902706563472748
Total epoch: 69. epoch loss: 0.2857449948787689
Total epoch: 70. epoch loss: 0.28138405084609985
Total epoch: 71. epoch loss: 0.27718040347099304
Total epoch: 72. epoch loss: 0.2731243968009949
Total epoch: 73. epoch loss: 0.2692074477672577
Total epoch: 74. epoch loss: 0.2654213309288025
Total epoch: 75. epoch loss: 0.261761337518692
Total epoch: 76. epoch loss: 0.2582172155380249
Total epoch: 77. epoch loss: 0.2547854781150818
Total epoch: 78. epoch loss: 0.2514601945877075
Total epoch: 79. epoch loss: 0.24823671579360962
Total epoch: 80. epoch loss: 0.24510681629180908
Total epoch: 81. epoch loss: 0.24206854403018951
Total epoch: 82. epoch loss: 0.23911529779434204
Total epoch: 83. epoch loss: 0.23624542355537415
Total epoch: 84. epoch loss: 0.23345167934894562
Total epoch: 85. epoch loss: 0.23073457181453705
Total epoch: 86. epoch loss: 0.22808857262134552
Total epoch: 87. epoch loss: 0.22551067173480988
Total epoch: 88. epoch loss: 0.22299513220787048
Total epoch: 89. epoch loss: 0.22054272890090942
Total epoch: 90. epoch loss: 0.2181502878665924
Total epoch: 91. epoch loss: 0.21581289172172546
Total epoch: 92. epoch loss: 0.21353276073932648
Total epoch: 93. epoch loss: 0.21130338311195374
Total epoch: 94. epoch loss: 0.20912577211856842
Total epoch: 95. epoch loss: 0.20699487626552582
Total epoch: 96. epoch loss: 0.20491163432598114
Total epoch: 97. epoch loss: 0.20287436246871948
Total epoch: 98. epoch loss: 0.20087771117687225
Total epoch: 99. epoch loss: 0.19892561435699463
Total epoch: 100. epoch loss: 0.1970115602016449
Total epoch: 101. epoch loss: 0.1951369196176529
Total epoch: 102. epoch loss: 0.19329994916915894
Total epoch: 103. epoch loss: 0.19149872660636902
Total epoch: 104. epoch loss: 0.18973417580127716
Total epoch: 105. epoch loss: 0.18800237774848938
Total epoch: 106. epoch loss: 0.1863032877445221
Total epoch: 107. epoch loss: 0.1846376210451126
Total epoch: 108. epoch loss: 0.1830012947320938
Total epoch: 109. epoch loss: 0.18139596283435822
Total epoch: 110. epoch loss: 0.1798190474510193
Total epoch: 111. epoch loss: 0.1782701313495636
Total epoch: 112. epoch loss: 0.17674918472766876
Total epoch: 113. epoch loss: 0.17525482177734375
Total epoch: 114. epoch loss: 0.17378509044647217
Total epoch: 115. epoch loss: 0.17234225571155548
Total epoch: 116. epoch loss: 0.17092418670654297
Total epoch: 117. epoch loss: 0.16952823102474213
Total epoch: 118. epoch loss: 0.16815659403800964
Total epoch: 119. epoch loss: 0.16680781543254852
Total epoch: 120. epoch loss: 0.16548117995262146
Total epoch: 121. epoch loss: 0.16417676210403442
Total epoch: 122. epoch loss: 0.16289122402668
Total epoch: 123. epoch loss: 0.16162648797035217
Total epoch: 124. epoch loss: 0.16038307547569275
Total epoch: 125. epoch loss: 0.15915827453136444
Total epoch: 126. epoch loss: 0.1579519659280777
Total epoch: 127. epoch loss: 0.15676432847976685
Total epoch: 128. epoch loss: 0.15559685230255127
Total epoch: 129. epoch loss: 0.1544448733329773
Total epoch: 130. epoch loss: 0.15331122279167175
Total epoch: 131. epoch loss: 0.15219424664974213
Total epoch: 132. epoch loss: 0.1510942280292511
Total epoch: 133. epoch loss: 0.15000951290130615
Total epoch: 134. epoch loss: 0.14894148707389832
Total epoch: 135. epoch loss: 0.14788833260536194
Total epoch: 136. epoch loss: 0.1468505561351776
Total epoch: 137. epoch loss: 0.1458277404308319
Total epoch: 138. epoch loss: 0.14482060074806213
Total epoch: 139. epoch loss: 0.14382608234882355
Total epoch: 140. epoch loss: 0.14284516870975494
Total epoch: 141. epoch loss: 0.14188042283058167
Total epoch: 142. epoch loss: 0.14092740416526794
Total epoch: 143. epoch loss: 0.13998717069625854
Total epoch: 144. epoch loss: 0.13906070590019226
Total epoch: 145. epoch loss: 0.13814599812030792
Total epoch: 146. epoch loss: 0.13724476099014282
Total epoch: 147. epoch loss: 0.1363561600446701
Total epoch: 148. epoch loss: 0.1354777067899704
Total epoch: 149. epoch loss: 0.13461242616176605
Total epoch: 149. DecT loss: 0.13461242616176605
Training time: 0.6459119319915771
APL_precision: 0.22614840989399293, APL_recall: 0.3764705882352941, APL_f1: 0.28256070640176595, APL_number: 170
CMT_precision: 0.27941176470588236, CMT_recall: 0.2923076923076923, CMT_f1: 0.28571428571428575, CMT_number: 195
DSC_precision: 0.38903394255874674, DSC_recall: 0.34096109839816935, DSC_f1: 0.3634146341463415, DSC_number: 437
MAT_precision: 0.6081632653061224, MAT_recall: 0.436950146627566, MAT_f1: 0.508532423208191, MAT_number: 682
PRO_precision: 0.33489461358313816, PRO_recall: 0.185473411154345, PRO_f1: 0.23873121869782976, PRO_number: 771
SMT_precision: 0.21403508771929824, SMT_recall: 0.3567251461988304, SMT_f1: 0.2675438596491228, SMT_number: 171
SPL_precision: 0.3493975903614458, SPL_recall: 0.38666666666666666, SPL_f1: 0.3670886075949367, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.3716937354988399, overall_recall: 0.3202718912435026, overall_f1: 0.3440721649484536, overall_accuracy: 0.7806447001643914
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/150 [00:04<?, ?it/s]