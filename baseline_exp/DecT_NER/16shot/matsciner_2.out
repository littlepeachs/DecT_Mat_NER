/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:33:40 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:33:41 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-545026288f6cba7d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1188.69it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/46 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:34:17 - INFO - __main__ - ***** Running training *****
05/30/2023 12:34:17 - INFO - __main__ -   Num examples = 46
05/30/2023 12:34:17 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:34:17 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:34:17 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:34:17 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:34:17 - INFO - __main__ -   Total optimization steps = 70
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/70 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.870931625366211
Total epoch: 1. epoch loss: 15.264142036437988
Total epoch: 2. epoch loss: 14.66710090637207
Total epoch: 3. epoch loss: 14.080023765563965
Total epoch: 4. epoch loss: 13.503341674804688
Total epoch: 5. epoch loss: 12.937657356262207
Total epoch: 6. epoch loss: 12.383724212646484
Total epoch: 7. epoch loss: 11.842303276062012
Total epoch: 8. epoch loss: 11.314179420471191
Total epoch: 9. epoch loss: 10.800058364868164
Total epoch: 10. epoch loss: 10.300525665283203
Total epoch: 11. epoch loss: 9.816052436828613
Total epoch: 12. epoch loss: 9.347084999084473
Total epoch: 13. epoch loss: 8.893940925598145
Total epoch: 14. epoch loss: 8.456907272338867
Total epoch: 15. epoch loss: 8.036259651184082
Total epoch: 16. epoch loss: 7.632218360900879
Total epoch: 17. epoch loss: 7.244870185852051
Total epoch: 18. epoch loss: 6.87421989440918
Total epoch: 19. epoch loss: 6.520219802856445
Total epoch: 20. epoch loss: 6.1826677322387695
Total epoch: 21. epoch loss: 5.861220359802246
Total epoch: 22. epoch loss: 5.555453300476074
Total epoch: 23. epoch loss: 5.2648701667785645
Total epoch: 24. epoch loss: 4.98887825012207
Total epoch: 25. epoch loss: 4.7268900871276855
Total epoch: 26. epoch loss: 4.478280544281006
Total epoch: 27. epoch loss: 4.242480278015137
Total epoch: 28. epoch loss: 4.018948078155518
Total epoch: 29. epoch loss: 3.807161808013916
Total epoch: 30. epoch loss: 3.606893301010132
Total epoch: 31. epoch loss: 3.4183349609375
Total epoch: 32. epoch loss: 3.2413980960845947
Total epoch: 33. epoch loss: 3.0758461952209473
Total epoch: 34. epoch loss: 2.9213335514068604
Total epoch: 34. DecT loss: 2.9213335514068604
Training time: 0.23919916152954102
APL_precision: 0.11598746081504702, APL_recall: 0.21764705882352942, APL_f1: 0.15132924335378323, APL_number: 170
CMT_precision: 0.130879345603272, CMT_recall: 0.3282051282051282, CMT_f1: 0.1871345029239766, CMT_number: 195
DSC_precision: 0.28773584905660377, DSC_recall: 0.41876430205949655, DSC_f1: 0.34109972041006525, DSC_number: 437
MAT_precision: 0.4946351931330472, MAT_recall: 0.6759530791788856, MAT_f1: 0.5712515489467163, MAT_number: 682
PRO_precision: 0.33363719234275296, PRO_recall: 0.47470817120622566, PRO_f1: 0.39186295503211993, PRO_number: 771
SMT_precision: 0.2191780821917808, SMT_recall: 0.4678362573099415, SMT_f1: 0.29850746268656714, SMT_number: 171
SPL_precision: 0.30952380952380953, SPL_recall: 0.52, SPL_f1: 0.3880597014925373, SPL_number: 75
overall_precision: 0.31029263370333, overall_recall: 0.4918032786885246, overall_f1: 0.38051044083526686, overall_accuracy: 0.7694946751483096
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/70 [00:01<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:37:11 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:37:12 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-545026288f6cba7d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:128, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1072.57it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/46 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:37:18 - INFO - __main__ - ***** Running training *****
05/30/2023 12:37:18 - INFO - __main__ -   Num examples = 46
05/30/2023 12:37:18 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:37:18 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:37:18 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:37:18 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:37:18 - INFO - __main__ -   Total optimization steps = 200
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.869954109191895
Total epoch: 1. epoch loss: 15.01534366607666
Total epoch: 2. epoch loss: 14.180416107177734
Total epoch: 3. epoch loss: 13.366652488708496
Total epoch: 4. epoch loss: 12.576353073120117
Total epoch: 5. epoch loss: 11.812165260314941
Total epoch: 6. epoch loss: 11.076480865478516
Total epoch: 7. epoch loss: 10.37104606628418
Total epoch: 8. epoch loss: 9.696913719177246
Total epoch: 9. epoch loss: 9.054664611816406
Total epoch: 10. epoch loss: 8.444769859313965
Total epoch: 11. epoch loss: 7.867804527282715
Total epoch: 12. epoch loss: 7.324240684509277
Total epoch: 13. epoch loss: 6.814322471618652
Total epoch: 14. epoch loss: 6.337773323059082
Total epoch: 15. epoch loss: 5.893725395202637
Total epoch: 16. epoch loss: 5.4807891845703125
Total epoch: 17. epoch loss: 5.097286224365234
Total epoch: 18. epoch loss: 4.7413506507873535
Total epoch: 19. epoch loss: 4.4110822677612305
Total epoch: 20. epoch loss: 4.104597091674805
Total epoch: 21. epoch loss: 3.8201944828033447
Total epoch: 22. epoch loss: 3.556821823120117
Total epoch: 23. epoch loss: 3.3139586448669434
Total epoch: 24. epoch loss: 3.0908570289611816
Total epoch: 25. epoch loss: 2.886587142944336
Total epoch: 26. epoch loss: 2.700073003768921
Total epoch: 27. epoch loss: 2.5302131175994873
Total epoch: 28. epoch loss: 2.3758363723754883
Total epoch: 29. epoch loss: 2.235747814178467
Total epoch: 30. epoch loss: 2.1087281703948975
Total epoch: 31. epoch loss: 1.993564486503601
Total epoch: 32. epoch loss: 1.8890693187713623
Total epoch: 33. epoch loss: 1.794127345085144
Total epoch: 34. epoch loss: 1.707651972770691
Total epoch: 35. epoch loss: 1.6287198066711426
Total epoch: 36. epoch loss: 1.5564862489700317
Total epoch: 37. epoch loss: 1.490234136581421
Total epoch: 38. epoch loss: 1.429343342781067
Total epoch: 39. epoch loss: 1.373295783996582
Total epoch: 40. epoch loss: 1.3216181993484497
Total epoch: 41. epoch loss: 1.2739086151123047
Total epoch: 42. epoch loss: 1.2298026084899902
Total epoch: 43. epoch loss: 1.1889859437942505
Total epoch: 44. epoch loss: 1.151153802871704
Total epoch: 45. epoch loss: 1.1160356998443604
Total epoch: 46. epoch loss: 1.0833953619003296
Total epoch: 47. epoch loss: 1.0530056953430176
Total epoch: 48. epoch loss: 1.0246632099151611
Total epoch: 49. epoch loss: 0.9981799125671387
Total epoch: 50. epoch loss: 0.9733813405036926
Total epoch: 51. epoch loss: 0.9501122236251831
Total epoch: 52. epoch loss: 0.9282269477844238
Total epoch: 53. epoch loss: 0.9075970649719238
Total epoch: 54. epoch loss: 0.8881118297576904
Total epoch: 55. epoch loss: 0.8696685433387756
Total epoch: 56. epoch loss: 0.8521788120269775
Total epoch: 57. epoch loss: 0.835560142993927
Total epoch: 58. epoch loss: 0.8197454214096069
Total epoch: 59. epoch loss: 0.8046723008155823
Total epoch: 60. epoch loss: 0.7902825474739075
Total epoch: 61. epoch loss: 0.7765253186225891
Total epoch: 62. epoch loss: 0.7633554339408875
Total epoch: 63. epoch loss: 0.7507299184799194
Total epoch: 64. epoch loss: 0.7386150360107422
Total epoch: 65. epoch loss: 0.726972758769989
Total epoch: 66. epoch loss: 0.71576988697052
Total epoch: 67. epoch loss: 0.7049819231033325
Total epoch: 68. epoch loss: 0.6945798397064209
Total epoch: 69. epoch loss: 0.6845399737358093
Total epoch: 70. epoch loss: 0.674838125705719
Total epoch: 71. epoch loss: 0.6654573082923889
Total epoch: 72. epoch loss: 0.6563819646835327
Total epoch: 73. epoch loss: 0.6475928425788879
Total epoch: 74. epoch loss: 0.639076828956604
Total epoch: 75. epoch loss: 0.6308175325393677
Total epoch: 76. epoch loss: 0.6228004693984985
Total epoch: 77. epoch loss: 0.6150202751159668
Total epoch: 78. epoch loss: 0.6074589490890503
Total epoch: 79. epoch loss: 0.6001052260398865
Total epoch: 80. epoch loss: 0.5929506421089172
Total epoch: 81. epoch loss: 0.5859851241111755
Total epoch: 82. epoch loss: 0.5792011022567749
Total epoch: 83. epoch loss: 0.5725886821746826
Total epoch: 84. epoch loss: 0.5661422610282898
Total epoch: 85. epoch loss: 0.5598536729812622
Total epoch: 86. epoch loss: 0.5537172555923462
Total epoch: 87. epoch loss: 0.5477254986763
Total epoch: 88. epoch loss: 0.5418742895126343
Total epoch: 89. epoch loss: 0.53615403175354
Total epoch: 90. epoch loss: 0.5305635929107666
Total epoch: 91. epoch loss: 0.5250976085662842
Total epoch: 92. epoch loss: 0.5197514891624451
Total epoch: 93. epoch loss: 0.5145204663276672
Total epoch: 94. epoch loss: 0.5093976855278015
Total epoch: 95. epoch loss: 0.5043806433677673
Total epoch: 96. epoch loss: 0.4994688928127289
Total epoch: 97. epoch loss: 0.4946529269218445
Total epoch: 98. epoch loss: 0.48993778228759766
Total epoch: 99. epoch loss: 0.48531368374824524
Total epoch: 99. DecT loss: 0.48531368374824524
Training time: 0.5049459934234619
APL_precision: 0.3192488262910798, APL_recall: 0.4, APL_f1: 0.35509138381201044, APL_number: 170
CMT_precision: 0.20491803278688525, CMT_recall: 0.38461538461538464, CMT_f1: 0.267379679144385, CMT_number: 195
DSC_precision: 0.3962765957446808, DSC_recall: 0.34096109839816935, DSC_f1: 0.36654366543665434, DSC_number: 437
MAT_precision: 0.5917808219178082, MAT_recall: 0.6334310850439883, MAT_f1: 0.6118980169971671, MAT_number: 682
PRO_precision: 0.4030261348005502, PRO_recall: 0.38002594033722437, PRO_f1: 0.39118825100133514, PRO_number: 771
SMT_precision: 0.2938775510204082, SMT_recall: 0.42105263157894735, SMT_f1: 0.3461538461538462, SMT_number: 171
SPL_precision: 0.39325842696629215, SPL_recall: 0.4666666666666667, SPL_f1: 0.426829268292683, SPL_number: 75
overall_precision: 0.4093226511289148, overall_recall: 0.4494202319072371, overall_f1: 0.4284352963598247, overall_accuracy: 0.8130941319419627
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:40:43 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:40:43 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-545026288f6cba7d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1022.13it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/46 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:40:50 - INFO - __main__ - ***** Running training *****
05/30/2023 12:40:50 - INFO - __main__ -   Num examples = 46
05/30/2023 12:40:50 - INFO - __main__ -   Num Epochs = 150
05/30/2023 12:40:50 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:40:50 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:40:50 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:40:50 - INFO - __main__ -   Total optimization steps = 300
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 15.835775375366211
Total epoch: 1. epoch loss: 14.886560440063477
Total epoch: 2. epoch loss: 13.960262298583984
Total epoch: 3. epoch loss: 13.059818267822266
Total epoch: 4. epoch loss: 12.189188957214355
Total epoch: 5. epoch loss: 11.352375984191895
Total epoch: 6. epoch loss: 10.552621841430664
Total epoch: 7. epoch loss: 9.791975975036621
Total epoch: 8. epoch loss: 9.07140064239502
Total epoch: 9. epoch loss: 8.391533851623535
Total epoch: 10. epoch loss: 7.752948760986328
Total epoch: 11. epoch loss: 7.156260013580322
Total epoch: 12. epoch loss: 6.601626396179199
Total epoch: 13. epoch loss: 6.088457107543945
Total epoch: 14. epoch loss: 5.615278720855713
Total epoch: 15. epoch loss: 5.17988920211792
Total epoch: 16. epoch loss: 4.779551982879639
Total epoch: 17. epoch loss: 4.411473274230957
Total epoch: 18. epoch loss: 4.072922706604004
Total epoch: 19. epoch loss: 3.7614450454711914
Total epoch: 20. epoch loss: 3.475656270980835
Total epoch: 21. epoch loss: 3.2146894931793213
Total epoch: 22. epoch loss: 2.977367401123047
Total epoch: 23. epoch loss: 2.7622649669647217
Total epoch: 24. epoch loss: 2.5678489208221436
Total epoch: 25. epoch loss: 2.3925342559814453
Total epoch: 26. epoch loss: 2.234731912612915
Total epoch: 27. epoch loss: 2.092850685119629
Total epoch: 28. epoch loss: 1.9653507471084595
Total epoch: 29. epoch loss: 1.8507094383239746
Total epoch: 30. epoch loss: 1.747483253479004
Total epoch: 31. epoch loss: 1.654339075088501
Total epoch: 32. epoch loss: 1.5700581073760986
Total epoch: 33. epoch loss: 1.4935762882232666
Total epoch: 34. epoch loss: 1.4240001440048218
Total epoch: 35. epoch loss: 1.360554575920105
Total epoch: 36. epoch loss: 1.3025884628295898
Total epoch: 37. epoch loss: 1.2495412826538086
Total epoch: 38. epoch loss: 1.200921654701233
Total epoch: 39. epoch loss: 1.156288981437683
Total epoch: 40. epoch loss: 1.1152637004852295
Total epoch: 41. epoch loss: 1.0774822235107422
Total epoch: 42. epoch loss: 1.0426253080368042
Total epoch: 43. epoch loss: 1.0103963613510132
Total epoch: 44. epoch loss: 0.9805477261543274
Total epoch: 45. epoch loss: 0.9528378844261169
Total epoch: 46. epoch loss: 0.9270496964454651
Total epoch: 47. epoch loss: 0.9029965400695801
Total epoch: 48. epoch loss: 0.8805100917816162
Total epoch: 49. epoch loss: 0.859430193901062
Total epoch: 50. epoch loss: 0.8396252989768982
Total epoch: 51. epoch loss: 0.8209720253944397
Total epoch: 52. epoch loss: 0.803358793258667
Total epoch: 53. epoch loss: 0.7866944670677185
Total epoch: 54. epoch loss: 0.7708933353424072
Total epoch: 55. epoch loss: 0.7558742165565491
Total epoch: 56. epoch loss: 0.7415800094604492
Total epoch: 57. epoch loss: 0.7279476523399353
Total epoch: 58. epoch loss: 0.7149301767349243
Total epoch: 59. epoch loss: 0.7024816274642944
Total epoch: 60. epoch loss: 0.6905626654624939
Total epoch: 61. epoch loss: 0.6791313886642456
Total epoch: 62. epoch loss: 0.6681607961654663
Total epoch: 63. epoch loss: 0.6576147079467773
Total epoch: 64. epoch loss: 0.647467851638794
Total epoch: 65. epoch loss: 0.6376914978027344
Total epoch: 66. epoch loss: 0.6282705664634705
Total epoch: 67. epoch loss: 0.6191718578338623
Total epoch: 68. epoch loss: 0.6103855967521667
Total epoch: 69. epoch loss: 0.6018902063369751
Total epoch: 70. epoch loss: 0.5936673879623413
Total epoch: 71. epoch loss: 0.5857100486755371
Total epoch: 72. epoch loss: 0.5779969692230225
Total epoch: 73. epoch loss: 0.5705139636993408
Total epoch: 74. epoch loss: 0.5632548332214355
Total epoch: 75. epoch loss: 0.556203305721283
Total epoch: 76. epoch loss: 0.5493516325950623
Total epoch: 77. epoch loss: 0.5426878333091736
Total epoch: 78. epoch loss: 0.5362054705619812
Total epoch: 79. epoch loss: 0.5298916697502136
Total epoch: 80. epoch loss: 0.5237452983856201
Total epoch: 81. epoch loss: 0.5177546739578247
Total epoch: 82. epoch loss: 0.5119144916534424
Total epoch: 83. epoch loss: 0.5062177777290344
Total epoch: 84. epoch loss: 0.5006577968597412
Total epoch: 85. epoch loss: 0.4952305257320404
Total epoch: 86. epoch loss: 0.48993080854415894
Total epoch: 87. epoch loss: 0.48475274443626404
Total epoch: 88. epoch loss: 0.47968900203704834
Total epoch: 89. epoch loss: 0.47474104166030884
Total epoch: 90. epoch loss: 0.46989747881889343
Total epoch: 91. epoch loss: 0.46515902876853943
Total epoch: 92. epoch loss: 0.46052223443984985
Total epoch: 93. epoch loss: 0.4559802711009979
Total epoch: 94. epoch loss: 0.4515341818332672
Total epoch: 95. epoch loss: 0.4471752643585205
Total epoch: 96. epoch loss: 0.44290658831596375
Total epoch: 97. epoch loss: 0.4387204647064209
Total epoch: 98. epoch loss: 0.4346156716346741
Total epoch: 99. epoch loss: 0.43059250712394714
Total epoch: 100. epoch loss: 0.42664533853530884
Total epoch: 101. epoch loss: 0.422771692276001
Total epoch: 102. epoch loss: 0.41897109150886536
Total epoch: 103. epoch loss: 0.4152385890483856
Total epoch: 104. epoch loss: 0.41157498955726624
Total epoch: 105. epoch loss: 0.4079788625240326
Total epoch: 106. epoch loss: 0.4044460356235504
Total epoch: 107. epoch loss: 0.4009752869606018
Total epoch: 108. epoch loss: 0.3975644111633301
Total epoch: 109. epoch loss: 0.39421242475509644
Total epoch: 110. epoch loss: 0.3909192383289337
Total epoch: 111. epoch loss: 0.38768017292022705
Total epoch: 112. epoch loss: 0.384499192237854
Total epoch: 113. epoch loss: 0.38136687874794006
Total epoch: 114. epoch loss: 0.37828826904296875
Total epoch: 115. epoch loss: 0.3752596080303192
Total epoch: 116. epoch loss: 0.37227997183799744
Total epoch: 117. epoch loss: 0.36934831738471985
Total epoch: 118. epoch loss: 0.36646315455436707
Total epoch: 119. epoch loss: 0.363623708486557
Total epoch: 120. epoch loss: 0.3608294427394867
Total epoch: 121. epoch loss: 0.35807764530181885
Total epoch: 122. epoch loss: 0.35536903142929077
Total epoch: 123. epoch loss: 0.35270214080810547
Total epoch: 124. epoch loss: 0.35007569193840027
Total epoch: 125. epoch loss: 0.34748855233192444
Total epoch: 126. epoch loss: 0.344940721988678
Total epoch: 127. epoch loss: 0.342428058385849
Total epoch: 128. epoch loss: 0.3399563133716583
Total epoch: 129. epoch loss: 0.33751896023750305
Total epoch: 130. epoch loss: 0.335117369890213
Total epoch: 131. epoch loss: 0.3327493667602539
Total epoch: 132. epoch loss: 0.3304169178009033
Total epoch: 133. epoch loss: 0.3281170129776001
Total epoch: 134. epoch loss: 0.32584983110427856
Total epoch: 135. epoch loss: 0.3236154615879059
Total epoch: 136. epoch loss: 0.3214111328125
Total epoch: 137. epoch loss: 0.31923919916152954
Total epoch: 138. epoch loss: 0.3170945346355438
Total epoch: 139. epoch loss: 0.3149814009666443
Total epoch: 140. epoch loss: 0.31289711594581604
Total epoch: 141. epoch loss: 0.3108397424221039
Total epoch: 142. epoch loss: 0.30881041288375854
Total epoch: 143. epoch loss: 0.3068093955516815
Total epoch: 144. epoch loss: 0.30483511090278625
Total epoch: 145. epoch loss: 0.30288639664649963
Total epoch: 146. epoch loss: 0.3009624481201172
Total epoch: 147. epoch loss: 0.29906436800956726
Total epoch: 148. epoch loss: 0.2971910536289215
Total epoch: 149. epoch loss: 0.2953423261642456
Total epoch: 149. DecT loss: 0.2953423261642456
Training time: 0.6910903453826904
APL_precision: 0.34, APL_recall: 0.4, APL_f1: 0.3675675675675676, APL_number: 170
CMT_precision: 0.21329639889196675, CMT_recall: 0.39487179487179486, CMT_f1: 0.27697841726618705, CMT_number: 195
DSC_precision: 0.40114613180515757, DSC_recall: 0.32036613272311215, DSC_f1: 0.356234096692112, DSC_number: 437
MAT_precision: 0.6129518072289156, MAT_recall: 0.5967741935483871, MAT_f1: 0.6047548291233283, MAT_number: 682
PRO_precision: 0.4041916167664671, PRO_recall: 0.35019455252918286, PRO_f1: 0.3752605976372481, PRO_number: 771
SMT_precision: 0.31555555555555553, SMT_recall: 0.4152046783625731, SMT_f1: 0.35858585858585856, SMT_number: 171
SPL_precision: 0.43209876543209874, SPL_recall: 0.4666666666666667, SPL_f1: 0.4487179487179487, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.41915227629513346, overall_recall: 0.42702918832467013, overall_f1: 0.42305407011289364, overall_accuracy: 0.8125938103066257
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]