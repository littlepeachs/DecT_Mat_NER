/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:33:40 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:33:41 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-2a29c2bd2933348e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1242.57it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/36 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:34:17 - INFO - __main__ - ***** Running training *****
05/30/2023 12:34:17 - INFO - __main__ -   Num examples = 36
05/30/2023 12:34:17 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:34:17 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:34:17 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:34:17 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:34:17 - INFO - __main__ -   Total optimization steps = 70
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/70 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.786243438720703
Total epoch: 1. epoch loss: 15.13707160949707
Total epoch: 2. epoch loss: 14.499072074890137
Total epoch: 3. epoch loss: 13.87206745147705
Total epoch: 4. epoch loss: 13.256052017211914
Total epoch: 5. epoch loss: 12.651215553283691
Total epoch: 6. epoch loss: 12.057990074157715
Total epoch: 7. epoch loss: 11.477121353149414
Total epoch: 8. epoch loss: 10.909603118896484
Total epoch: 9. epoch loss: 10.356534957885742
Total epoch: 10. epoch loss: 9.8190279006958
Total epoch: 11. epoch loss: 9.298101425170898
Total epoch: 12. epoch loss: 8.794744491577148
Total epoch: 13. epoch loss: 8.309772491455078
Total epoch: 14. epoch loss: 7.843866348266602
Total epoch: 15. epoch loss: 7.397593021392822
Total epoch: 16. epoch loss: 6.97132682800293
Total epoch: 17. epoch loss: 6.565341949462891
Total epoch: 18. epoch loss: 6.179655075073242
Total epoch: 19. epoch loss: 5.814150333404541
Total epoch: 20. epoch loss: 5.468461036682129
Total epoch: 21. epoch loss: 5.142161846160889
Total epoch: 22. epoch loss: 4.834568500518799
Total epoch: 23. epoch loss: 4.5449137687683105
Total epoch: 24. epoch loss: 4.272326946258545
Total epoch: 25. epoch loss: 4.01593017578125
Total epoch: 26. epoch loss: 3.774817943572998
Total epoch: 27. epoch loss: 3.548124074935913
Total epoch: 28. epoch loss: 3.3350770473480225
Total epoch: 29. epoch loss: 3.1349620819091797
Total epoch: 30. epoch loss: 2.947887420654297
Total epoch: 31. epoch loss: 2.7736926078796387
Total epoch: 32. epoch loss: 2.6119778156280518
Total epoch: 33. epoch loss: 2.462305784225464
Total epoch: 34. epoch loss: 2.324066400527954
Total epoch: 34. DecT loss: 2.324066400527954
Training time: 0.19115424156188965
APL_precision: 0.1343612334801762, APL_recall: 0.3588235294117647, APL_f1: 0.1955128205128205, APL_number: 170
CMT_precision: 0.3484848484848485, CMT_recall: 0.35384615384615387, CMT_f1: 0.351145038167939, CMT_number: 195
DSC_precision: 0.32461538461538464, DSC_recall: 0.482837528604119, DSC_f1: 0.38822447102115915, DSC_number: 437
MAT_precision: 0.5301775147928994, MAT_recall: 0.656891495601173, MAT_f1: 0.5867714472822528, MAT_number: 682
PRO_precision: 0.31590181430096054, PRO_recall: 0.383916990920882, PRO_f1: 0.34660421545667447, PRO_number: 771
SMT_precision: 0.21573604060913706, SMT_recall: 0.49707602339181284, SMT_f1: 0.3008849557522124, SMT_number: 171
SPL_precision: 0.2396694214876033, SPL_recall: 0.38666666666666666, SPL_f1: 0.2959183673469387, SPL_number: 75
overall_precision: 0.33314809669352596, overall_recall: 0.47940823670531785, overall_f1: 0.39311475409836066, overall_accuracy: 0.7849331713244229
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/70 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:37:11 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:37:12 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-2a29c2bd2933348e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:128, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1072.30it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/36 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:37:18 - INFO - __main__ - ***** Running training *****
05/30/2023 12:37:18 - INFO - __main__ -   Num examples = 36
05/30/2023 12:37:18 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:37:18 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:37:18 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:37:18 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:37:18 - INFO - __main__ -   Total optimization steps = 200
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.778780937194824
Total epoch: 1. epoch loss: 14.865754127502441
Total epoch: 2. epoch loss: 13.97434139251709
Total epoch: 3. epoch loss: 13.105284690856934
Total epoch: 4. epoch loss: 12.259923934936523
Total epoch: 5. epoch loss: 11.440278053283691
Total epoch: 6. epoch loss: 10.64879035949707
Total epoch: 7. epoch loss: 9.888044357299805
Total epoch: 8. epoch loss: 9.160642623901367
Total epoch: 9. epoch loss: 8.46889591217041
Total epoch: 10. epoch loss: 7.814896583557129
Total epoch: 11. epoch loss: 7.200272560119629
Total epoch: 12. epoch loss: 6.626077651977539
Total epoch: 13. epoch loss: 6.092737674713135
Total epoch: 14. epoch loss: 5.5997724533081055
Total epoch: 15. epoch loss: 5.145987510681152
Total epoch: 16. epoch loss: 4.729477405548096
Total epoch: 17. epoch loss: 4.347902774810791
Total epoch: 18. epoch loss: 3.9986164569854736
Total epoch: 19. epoch loss: 3.678833246231079
Total epoch: 20. epoch loss: 3.385828971862793
Total epoch: 21. epoch loss: 3.1171693801879883
Total epoch: 22. epoch loss: 2.8720273971557617
Total epoch: 23. epoch loss: 2.649338960647583
Total epoch: 24. epoch loss: 2.447827100753784
Total epoch: 25. epoch loss: 2.266096830368042
Total epoch: 26. epoch loss: 2.1026346683502197
Total epoch: 27. epoch loss: 1.955925464630127
Total epoch: 28. epoch loss: 1.8244389295578003
Total epoch: 29. epoch loss: 1.7067276239395142
Total epoch: 30. epoch loss: 1.601391077041626
Total epoch: 31. epoch loss: 1.5071316957473755
Total epoch: 32. epoch loss: 1.4227343797683716
Total epoch: 33. epoch loss: 1.3470349311828613
Total epoch: 34. epoch loss: 1.2790024280548096
Total epoch: 35. epoch loss: 1.217672348022461
Total epoch: 36. epoch loss: 1.162213921546936
Total epoch: 37. epoch loss: 1.1119027137756348
Total epoch: 38. epoch loss: 1.0661214590072632
Total epoch: 39. epoch loss: 1.0243314504623413
Total epoch: 40. epoch loss: 0.9860872626304626
Total epoch: 41. epoch loss: 0.9509874582290649
Total epoch: 42. epoch loss: 0.9186825752258301
Total epoch: 43. epoch loss: 0.8888833522796631
Total epoch: 44. epoch loss: 0.8612943887710571
Total epoch: 45. epoch loss: 0.8357016444206238
Total epoch: 46. epoch loss: 0.811891496181488
Total epoch: 47. epoch loss: 0.7896852493286133
Total epoch: 48. epoch loss: 0.7689282894134521
Total epoch: 49. epoch loss: 0.7494884133338928
Total epoch: 50. epoch loss: 0.7312385439872742
Total epoch: 51. epoch loss: 0.7140733003616333
Total epoch: 52. epoch loss: 0.697894275188446
Total epoch: 53. epoch loss: 0.6826131343841553
Total epoch: 54. epoch loss: 0.668146550655365
Total epoch: 55. epoch loss: 0.654434859752655
Total epoch: 56. epoch loss: 0.6414162516593933
Total epoch: 57. epoch loss: 0.6290295124053955
Total epoch: 58. epoch loss: 0.6172321438789368
Total epoch: 59. epoch loss: 0.6059805154800415
Total epoch: 60. epoch loss: 0.5952357649803162
Total epoch: 61. epoch loss: 0.5849628448486328
Total epoch: 62. epoch loss: 0.5751287937164307
Total epoch: 63. epoch loss: 0.5657014846801758
Total epoch: 64. epoch loss: 0.5566601157188416
Total epoch: 65. epoch loss: 0.5479756593704224
Total epoch: 66. epoch loss: 0.5396215915679932
Total epoch: 67. epoch loss: 0.5315847396850586
Total epoch: 68. epoch loss: 0.5238368511199951
Total epoch: 69. epoch loss: 0.5163708329200745
Total epoch: 70. epoch loss: 0.5091575384140015
Total epoch: 71. epoch loss: 0.5021915435791016
Total epoch: 72. epoch loss: 0.4954528510570526
Total epoch: 73. epoch loss: 0.48893246054649353
Total epoch: 74. epoch loss: 0.48261821269989014
Total epoch: 75. epoch loss: 0.47649532556533813
Total epoch: 76. epoch loss: 0.4705612063407898
Total epoch: 77. epoch loss: 0.4648004472255707
Total epoch: 78. epoch loss: 0.45920735597610474
Total epoch: 79. epoch loss: 0.4537712633609772
Total epoch: 80. epoch loss: 0.4484848380088806
Total epoch: 81. epoch loss: 0.44334524869918823
Total epoch: 82. epoch loss: 0.4383423626422882
Total epoch: 83. epoch loss: 0.43346789479255676
Total epoch: 84. epoch loss: 0.42871910333633423
Total epoch: 85. epoch loss: 0.42408862709999084
Total epoch: 86. epoch loss: 0.4195689260959625
Total epoch: 87. epoch loss: 0.41516315937042236
Total epoch: 88. epoch loss: 0.4108617305755615
Total epoch: 89. epoch loss: 0.40665680170059204
Total epoch: 90. epoch loss: 0.40255075693130493
Total epoch: 91. epoch loss: 0.3985392153263092
Total epoch: 92. epoch loss: 0.39461347460746765
Total epoch: 93. epoch loss: 0.3907732367515564
Total epoch: 94. epoch loss: 0.38701847195625305
Total epoch: 95. epoch loss: 0.38333871960639954
Total epoch: 96. epoch loss: 0.37974002957344055
Total epoch: 97. epoch loss: 0.376213014125824
Total epoch: 98. epoch loss: 0.3727564513683319
Total epoch: 99. epoch loss: 0.3693698048591614
Total epoch: 99. DecT loss: 0.3693698048591614
Training time: 0.48525238037109375
APL_precision: 0.21364985163204747, APL_recall: 0.4235294117647059, APL_f1: 0.28402366863905326, APL_number: 170
CMT_precision: 0.46408839779005523, CMT_recall: 0.4307692307692308, CMT_f1: 0.44680851063829785, CMT_number: 195
DSC_precision: 0.4444444444444444, DSC_recall: 0.4576659038901602, DSC_f1: 0.4509582863585118, DSC_number: 437
MAT_precision: 0.6244131455399061, MAT_recall: 0.5850439882697948, MAT_f1: 0.6040878122634368, MAT_number: 682
PRO_precision: 0.326984126984127, PRO_recall: 0.26718547341115434, PRO_f1: 0.29407566024268383, PRO_number: 771
SMT_precision: 0.2574626865671642, SMT_recall: 0.40350877192982454, SMT_f1: 0.31435079726651477, SMT_number: 171
SPL_precision: 0.3333333333333333, SPL_recall: 0.4666666666666667, SPL_f1: 0.3888888888888889, SPL_number: 75
overall_precision: 0.40804597701149425, overall_recall: 0.4258296681327469, overall_f1: 0.4167481901780473, overall_accuracy: 0.8128797083839611
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:40:42 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:40:43 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-2a29c2bd2933348e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1045.57it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/36 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:40:49 - INFO - __main__ - ***** Running training *****
05/30/2023 12:40:49 - INFO - __main__ -   Num examples = 36
05/30/2023 12:40:49 - INFO - __main__ -   Num Epochs = 150
05/30/2023 12:40:49 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:40:49 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:40:49 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:40:49 - INFO - __main__ -   Total optimization steps = 300
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 15.747600555419922
Total epoch: 1. epoch loss: 14.733047485351562
Total epoch: 2. epoch loss: 13.744932174682617
Total epoch: 3. epoch loss: 12.784586906433105
Total epoch: 4. epoch loss: 11.854156494140625
Total epoch: 5. epoch loss: 10.956930160522461
Total epoch: 6. epoch loss: 10.096773147583008
Total epoch: 7. epoch loss: 9.27737045288086
Total epoch: 8. epoch loss: 8.501891136169434
Total epoch: 9. epoch loss: 7.7729692459106445
Total epoch: 10. epoch loss: 7.092790126800537
Total epoch: 11. epoch loss: 6.462778568267822
Total epoch: 12. epoch loss: 5.883433818817139
Total epoch: 13. epoch loss: 5.353935718536377
Total epoch: 14. epoch loss: 4.872311592102051
Total epoch: 15. epoch loss: 4.435567378997803
Total epoch: 16. epoch loss: 4.040102005004883
Total epoch: 17. epoch loss: 3.6820602416992188
Total epoch: 18. epoch loss: 3.3576555252075195
Total epoch: 19. epoch loss: 3.0635640621185303
Total epoch: 20. epoch loss: 2.7983617782592773
Total epoch: 21. epoch loss: 2.560326099395752
Total epoch: 22. epoch loss: 2.3475449085235596
Total epoch: 23. epoch loss: 2.157927989959717
Total epoch: 24. epoch loss: 1.9893457889556885
Total epoch: 25. epoch loss: 1.8396949768066406
Total epoch: 26. epoch loss: 1.7070188522338867
Total epoch: 27. epoch loss: 1.5894696712493896
Total epoch: 28. epoch loss: 1.48534095287323
Total epoch: 29. epoch loss: 1.3930546045303345
Total epoch: 30. epoch loss: 1.3111510276794434
Total epoch: 31. epoch loss: 1.2382721900939941
Total epoch: 32. epoch loss: 1.1732274293899536
Total epoch: 33. epoch loss: 1.1149476766586304
Total epoch: 34. epoch loss: 1.0625321865081787
Total epoch: 35. epoch loss: 1.0152058601379395
Total epoch: 36. epoch loss: 0.9723311066627502
Total epoch: 37. epoch loss: 0.933351457118988
Total epoch: 38. epoch loss: 0.8978015780448914
Total epoch: 39. epoch loss: 0.8652750849723816
Total epoch: 40. epoch loss: 0.8354166746139526
Total epoch: 41. epoch loss: 0.8079171180725098
Total epoch: 42. epoch loss: 0.7825126647949219
Total epoch: 43. epoch loss: 0.7589699625968933
Total epoch: 44. epoch loss: 0.7370936870574951
Total epoch: 45. epoch loss: 0.716718852519989
Total epoch: 46. epoch loss: 0.6976913213729858
Total epoch: 47. epoch loss: 0.6798812747001648
Total epoch: 48. epoch loss: 0.6631762385368347
Total epoch: 49. epoch loss: 0.6474699974060059
Total epoch: 50. epoch loss: 0.6326743960380554
Total epoch: 51. epoch loss: 0.618706464767456
Total epoch: 52. epoch loss: 0.6054938435554504
Total epoch: 53. epoch loss: 0.5929736495018005
Total epoch: 54. epoch loss: 0.5810922980308533
Total epoch: 55. epoch loss: 0.5697965025901794
Total epoch: 56. epoch loss: 0.5590388178825378
Total epoch: 57. epoch loss: 0.5487841963768005
Total epoch: 58. epoch loss: 0.5389882922172546
Total epoch: 59. epoch loss: 0.5296227931976318
Total epoch: 60. epoch loss: 0.5206590890884399
Total epoch: 61. epoch loss: 0.5120657682418823
Total epoch: 62. epoch loss: 0.503822922706604
Total epoch: 63. epoch loss: 0.4959060549736023
Total epoch: 64. epoch loss: 0.488295316696167
Total epoch: 65. epoch loss: 0.4809736907482147
Total epoch: 66. epoch loss: 0.47392138838768005
Total epoch: 67. epoch loss: 0.4671214818954468
Total epoch: 68. epoch loss: 0.4605634808540344
Total epoch: 69. epoch loss: 0.45422568917274475
Total epoch: 70. epoch loss: 0.44810330867767334
Total epoch: 71. epoch loss: 0.4421786963939667
Total epoch: 72. epoch loss: 0.4364446997642517
Total epoch: 73. epoch loss: 0.4308896064758301
Total epoch: 74. epoch loss: 0.4255039095878601
Total epoch: 75. epoch loss: 0.42027926445007324
Total epoch: 76. epoch loss: 0.4152072072029114
Total epoch: 77. epoch loss: 0.41028136014938354
Total epoch: 78. epoch loss: 0.40549197793006897
Total epoch: 79. epoch loss: 0.40083393454551697
Total epoch: 80. epoch loss: 0.39630091190338135
Total epoch: 81. epoch loss: 0.391888827085495
Total epoch: 82. epoch loss: 0.3875894546508789
Total epoch: 83. epoch loss: 0.38339778780937195
Total epoch: 84. epoch loss: 0.37931028008461
Total epoch: 85. epoch loss: 0.37532180547714233
Total epoch: 86. epoch loss: 0.37143033742904663
Total epoch: 87. epoch loss: 0.3676297068595886
Total epoch: 88. epoch loss: 0.36391690373420715
Total epoch: 89. epoch loss: 0.3602883219718933
Total epoch: 90. epoch loss: 0.35674044489860535
Total epoch: 91. epoch loss: 0.3532688021659851
Total epoch: 92. epoch loss: 0.3498733639717102
Total epoch: 93. epoch loss: 0.3465496599674225
Total epoch: 94. epoch loss: 0.34329336881637573
Total epoch: 95. epoch loss: 0.3401064872741699
Total epoch: 96. epoch loss: 0.33698317408561707
Total epoch: 97. epoch loss: 0.3339206576347351
Total epoch: 98. epoch loss: 0.3309188485145569
Total epoch: 99. epoch loss: 0.3279764652252197
Total epoch: 100. epoch loss: 0.32509055733680725
Total epoch: 101. epoch loss: 0.32225626707077026
Total epoch: 102. epoch loss: 0.3194766640663147
Total epoch: 103. epoch loss: 0.3167482316493988
Total epoch: 104. epoch loss: 0.31406840682029724
Total epoch: 105. epoch loss: 0.31143850088119507
Total epoch: 106. epoch loss: 0.30885347723960876
Total epoch: 107. epoch loss: 0.3063151240348816
Total epoch: 108. epoch loss: 0.3038191795349121
Total epoch: 109. epoch loss: 0.301366925239563
Total epoch: 110. epoch loss: 0.2989566922187805
Total epoch: 111. epoch loss: 0.2965857684612274
Total epoch: 112. epoch loss: 0.29425492882728577
Total epoch: 113. epoch loss: 0.2919634282588959
Total epoch: 114. epoch loss: 0.2897074818611145
Total epoch: 115. epoch loss: 0.2874879837036133
Total epoch: 116. epoch loss: 0.28530529141426086
Total epoch: 117. epoch loss: 0.28315508365631104
Total epoch: 118. epoch loss: 0.2810404300689697
Total epoch: 119. epoch loss: 0.2789584994316101
Total epoch: 120. epoch loss: 0.27690795063972473
Total epoch: 121. epoch loss: 0.2748897075653076
Total epoch: 122. epoch loss: 0.2729010283946991
Total epoch: 123. epoch loss: 0.27094268798828125
Total epoch: 124. epoch loss: 0.2690144181251526
Total epoch: 125. epoch loss: 0.2671127915382385
Total epoch: 126. epoch loss: 0.26524049043655396
Total epoch: 127. epoch loss: 0.2633964717388153
Total epoch: 128. epoch loss: 0.2615762948989868
Total epoch: 129. epoch loss: 0.25978395342826843
Total epoch: 130. epoch loss: 0.2580169439315796
Total epoch: 131. epoch loss: 0.25627511739730835
Total epoch: 132. epoch loss: 0.2545569837093353
Total epoch: 133. epoch loss: 0.2528630793094635
Total epoch: 134. epoch loss: 0.251194030046463
Total epoch: 135. epoch loss: 0.24954701960086823
Total epoch: 136. epoch loss: 0.24792182445526123
Total epoch: 137. epoch loss: 0.24631981551647186
Total epoch: 138. epoch loss: 0.24473987519741058
Total epoch: 139. epoch loss: 0.24318043887615204
Total epoch: 140. epoch loss: 0.24164119362831116
Total epoch: 141. epoch loss: 0.2401224821805954
Total epoch: 142. epoch loss: 0.23862352967262268
Total epoch: 143. epoch loss: 0.23714575171470642
Total epoch: 144. epoch loss: 0.2356864959001541
Total epoch: 145. epoch loss: 0.2342456579208374
Total epoch: 146. epoch loss: 0.23282398283481598
Total epoch: 147. epoch loss: 0.23141974210739136
Total epoch: 148. epoch loss: 0.23003354668617249
Total epoch: 149. epoch loss: 0.2286643087863922
Total epoch: 149. DecT loss: 0.2286643087863922
Training time: 0.6837158203125
APL_precision: 0.21495327102803738, APL_recall: 0.40588235294117647, APL_f1: 0.2810590631364562, APL_number: 170
CMT_precision: 0.4748603351955307, CMT_recall: 0.4358974358974359, CMT_f1: 0.45454545454545453, CMT_number: 195
DSC_precision: 0.4607594936708861, DSC_recall: 0.41647597254004576, DSC_f1: 0.4375, DSC_number: 437
MAT_precision: 0.6335616438356164, MAT_recall: 0.5425219941348973, MAT_f1: 0.584518167456556, MAT_number: 682
PRO_precision: 0.3136, PRO_recall: 0.25421530479896237, PRO_f1: 0.2808022922636103, PRO_number: 771
SMT_precision: 0.26907630522088355, SMT_recall: 0.391812865497076, SMT_f1: 0.319047619047619, SMT_number: 171
SPL_precision: 0.3617021276595745, SPL_recall: 0.4533333333333333, SPL_f1: 0.40236686390532544, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.40988966080915407, overall_recall: 0.40103958416633345, overall_f1: 0.40541632983023446, overall_accuracy: 0.8098063040526052
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:02<?, ?it/s]