/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:33:40 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:33:41 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-f187e4508a2c5921/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1147.55it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/45 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:34:17 - INFO - __main__ - ***** Running training *****
05/30/2023 12:34:17 - INFO - __main__ -   Num examples = 45
05/30/2023 12:34:17 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:34:17 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:34:17 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:34:17 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:34:17 - INFO - __main__ -   Total optimization steps = 70
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/70 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.75481128692627
Total epoch: 1. epoch loss: 15.135334014892578
Total epoch: 2. epoch loss: 14.525164604187012
Total epoch: 3. epoch loss: 13.924483299255371
Total epoch: 4. epoch loss: 13.333381652832031
Total epoch: 5. epoch loss: 12.75206184387207
Total epoch: 6. epoch loss: 12.180910110473633
Total epoch: 7. epoch loss: 11.620624542236328
Total epoch: 8. epoch loss: 11.072175025939941
Total epoch: 9. epoch loss: 10.536698341369629
Total epoch: 10. epoch loss: 10.015460968017578
Total epoch: 11. epoch loss: 9.509743690490723
Total epoch: 12. epoch loss: 9.020687103271484
Total epoch: 13. epoch loss: 8.549277305603027
Total epoch: 14. epoch loss: 8.096285820007324
Total epoch: 15. epoch loss: 7.662282943725586
Total epoch: 16. epoch loss: 7.247666835784912
Total epoch: 17. epoch loss: 6.852658748626709
Total epoch: 18. epoch loss: 6.477280616760254
Total epoch: 19. epoch loss: 6.121364116668701
Total epoch: 20. epoch loss: 5.78466796875
Total epoch: 21. epoch loss: 5.466578483581543
Total epoch: 22. epoch loss: 5.1664605140686035
Total epoch: 23. epoch loss: 4.883399963378906
Total epoch: 24. epoch loss: 4.616440296173096
Total epoch: 25. epoch loss: 4.364497661590576
Total epoch: 26. epoch loss: 4.1265482902526855
Total epoch: 27. epoch loss: 3.901646852493286
Total epoch: 28. epoch loss: 3.6889710426330566
Total epoch: 29. epoch loss: 3.4879868030548096
Total epoch: 30. epoch loss: 3.29874587059021
Total epoch: 31. epoch loss: 3.121107339859009
Total epoch: 32. epoch loss: 2.9547922611236572
Total epoch: 33. epoch loss: 2.7994308471679688
Total epoch: 34. epoch loss: 2.6546082496643066
Total epoch: 34. DecT loss: 2.6546082496643066
Training time: 0.22797417640686035
APL_precision: 0.20802919708029197, APL_recall: 0.3352941176470588, APL_f1: 0.2567567567567568, APL_number: 170
CMT_precision: 0.2551440329218107, CMT_recall: 0.31794871794871793, CMT_f1: 0.28310502283105027, CMT_number: 195
DSC_precision: 0.3466204506065858, DSC_recall: 0.4576659038901602, DSC_f1: 0.39447731755424065, DSC_number: 437
MAT_precision: 0.3046153846153846, MAT_recall: 0.43548387096774194, MAT_f1: 0.35847917923958966, MAT_number: 682
PRO_precision: 0.39315352697095435, PRO_recall: 0.4915693904020752, PRO_f1: 0.4368876080691642, PRO_number: 771
SMT_precision: 0.19950738916256158, SMT_recall: 0.47368421052631576, SMT_f1: 0.2807625649913345, SMT_number: 171
SPL_precision: 0.26356589147286824, SPL_recall: 0.4533333333333333, SPL_f1: 0.3333333333333333, SPL_number: 75
overall_precision: 0.31109865470852016, overall_recall: 0.4438224710115954, overall_f1: 0.36579337617399904, overall_accuracy: 0.7759988564076906
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/70 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:37:11 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:37:12 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-f187e4508a2c5921/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:128, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1119.68it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/45 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4377.37 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:37:19 - INFO - __main__ - ***** Running training *****
05/30/2023 12:37:19 - INFO - __main__ -   Num examples = 45
05/30/2023 12:37:19 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:37:19 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:37:19 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:37:19 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:37:19 - INFO - __main__ -   Total optimization steps = 200
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.74885368347168
Total epoch: 1. epoch loss: 14.881736755371094
Total epoch: 2. epoch loss: 14.033879280090332
Total epoch: 3. epoch loss: 13.205296516418457
Total epoch: 4. epoch loss: 12.396313667297363
Total epoch: 5. epoch loss: 11.6082763671875
Total epoch: 6. epoch loss: 10.84370231628418
Total epoch: 7. epoch loss: 10.106050491333008
Total epoch: 8. epoch loss: 9.398981094360352
Total epoch: 9. epoch loss: 8.725913047790527
Total epoch: 10. epoch loss: 8.08955192565918
Total epoch: 11. epoch loss: 7.491678237915039
Total epoch: 12. epoch loss: 6.933122634887695
Total epoch: 13. epoch loss: 6.413822650909424
Total epoch: 14. epoch loss: 5.9330735206604
Total epoch: 15. epoch loss: 5.4895806312561035
Total epoch: 16. epoch loss: 5.081602573394775
Total epoch: 17. epoch loss: 4.706843376159668
Total epoch: 18. epoch loss: 4.3626017570495605
Total epoch: 19. epoch loss: 4.045886516571045
Total epoch: 20. epoch loss: 3.75380277633667
Total epoch: 21. epoch loss: 3.4839041233062744
Total epoch: 22. epoch loss: 3.235140562057495
Total epoch: 23. epoch loss: 3.006528854370117
Total epoch: 24. epoch loss: 2.7970645427703857
Total epoch: 25. epoch loss: 2.605687141418457
Total epoch: 26. epoch loss: 2.4312524795532227
Total epoch: 27. epoch loss: 2.272642135620117
Total epoch: 28. epoch loss: 2.1286723613739014
Total epoch: 29. epoch loss: 1.998214840888977
Total epoch: 30. epoch loss: 1.8801411390304565
Total epoch: 31. epoch loss: 1.7733924388885498
Total epoch: 32. epoch loss: 1.6769022941589355
Total epoch: 33. epoch loss: 1.5896425247192383
Total epoch: 34. epoch loss: 1.510653018951416
Total epoch: 35. epoch loss: 1.4390060901641846
Total epoch: 36. epoch loss: 1.3738977909088135
Total epoch: 37. epoch loss: 1.3145843744277954
Total epoch: 38. epoch loss: 1.2604247331619263
Total epoch: 39. epoch loss: 1.2108700275421143
Total epoch: 40. epoch loss: 1.165423035621643
Total epoch: 41. epoch loss: 1.123660683631897
Total epoch: 42. epoch loss: 1.0852069854736328
Total epoch: 43. epoch loss: 1.0496999025344849
Total epoch: 44. epoch loss: 1.0168625116348267
Total epoch: 45. epoch loss: 0.9864290952682495
Total epoch: 46. epoch loss: 0.9581524133682251
Total epoch: 47. epoch loss: 0.9318374395370483
Total epoch: 48. epoch loss: 0.9072891473770142
Total epoch: 49. epoch loss: 0.8843509554862976
Total epoch: 50. epoch loss: 0.8628618717193604
Total epoch: 51. epoch loss: 0.8426945209503174
Total epoch: 52. epoch loss: 0.8237200379371643
Total epoch: 53. epoch loss: 0.8058269619941711
Total epoch: 54. epoch loss: 0.7889187932014465
Total epoch: 55. epoch loss: 0.7729144096374512
Total epoch: 56. epoch loss: 0.7577316761016846
Total epoch: 57. epoch loss: 0.7433034181594849
Total epoch: 58. epoch loss: 0.7295705676078796
Total epoch: 59. epoch loss: 0.7164801955223083
Total epoch: 60. epoch loss: 0.7039840817451477
Total epoch: 61. epoch loss: 0.6920346021652222
Total epoch: 62. epoch loss: 0.6806004643440247
Total epoch: 63. epoch loss: 0.6696398854255676
Total epoch: 64. epoch loss: 0.6591224670410156
Total epoch: 65. epoch loss: 0.6490222811698914
Total epoch: 66. epoch loss: 0.6393067240715027
Total epoch: 67. epoch loss: 0.6299575567245483
Total epoch: 68. epoch loss: 0.6209431290626526
Total epoch: 69. epoch loss: 0.6122497916221619
Total epoch: 70. epoch loss: 0.603855550289154
Total epoch: 71. epoch loss: 0.5957412719726562
Total epoch: 72. epoch loss: 0.587893545627594
Total epoch: 73. epoch loss: 0.5802946090698242
Total epoch: 74. epoch loss: 0.5729278326034546
Total epoch: 75. epoch loss: 0.5657899379730225
Total epoch: 76. epoch loss: 0.5588559508323669
Total epoch: 77. epoch loss: 0.5521278977394104
Total epoch: 78. epoch loss: 0.5455876588821411
Total epoch: 79. epoch loss: 0.5392307043075562
Total epoch: 80. epoch loss: 0.5330409407615662
Total epoch: 81. epoch loss: 0.5270144939422607
Total epoch: 82. epoch loss: 0.5211479067802429
Total epoch: 83. epoch loss: 0.5154317617416382
Total epoch: 84. epoch loss: 0.5098521113395691
Total epoch: 85. epoch loss: 0.504412055015564
Total epoch: 86. epoch loss: 0.4991030991077423
Total epoch: 87. epoch loss: 0.493916392326355
Total epoch: 88. epoch loss: 0.48885226249694824
Total epoch: 89. epoch loss: 0.4839029610157013
Total epoch: 90. epoch loss: 0.47906193137168884
Total epoch: 91. epoch loss: 0.47432658076286316
Total epoch: 92. epoch loss: 0.46969738602638245
Total epoch: 93. epoch loss: 0.4651636481285095
Total epoch: 94. epoch loss: 0.46072685718536377
Total epoch: 95. epoch loss: 0.45637938380241394
Total epoch: 96. epoch loss: 0.45212051272392273
Total epoch: 97. epoch loss: 0.4479478895664215
Total epoch: 98. epoch loss: 0.44385644793510437
Total epoch: 99. epoch loss: 0.43984857201576233
Total epoch: 99. DecT loss: 0.43984857201576233
Training time: 0.5086157321929932
APL_precision: 0.34545454545454546, APL_recall: 0.4470588235294118, APL_f1: 0.3897435897435898, APL_number: 170
CMT_precision: 0.42857142857142855, CMT_recall: 0.49230769230769234, CMT_f1: 0.4582338902147971, CMT_number: 195
DSC_precision: 0.5051020408163265, DSC_recall: 0.45308924485125857, DSC_f1: 0.4776839565741857, DSC_number: 437
MAT_precision: 0.4727272727272727, MAT_recall: 0.533724340175953, MAT_f1: 0.5013774104683196, MAT_number: 682
PRO_precision: 0.46410684474123537, PRO_recall: 0.36057068741893644, PRO_f1: 0.40583941605839413, PRO_number: 771
SMT_precision: 0.27350427350427353, SMT_recall: 0.3742690058479532, SMT_f1: 0.3160493827160494, SMT_number: 171
SPL_precision: 0.297029702970297, SPL_recall: 0.4, SPL_f1: 0.34090909090909094, SPL_number: 75
overall_precision: 0.43543307086614175, overall_recall: 0.4422231107556977, overall_f1: 0.4388018250347153, overall_accuracy: 0.8243871059967122
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:40:43 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:40:43 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-f187e4508a2c5921/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1134.36it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/45 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:40:50 - INFO - __main__ - ***** Running training *****
05/30/2023 12:40:50 - INFO - __main__ -   Num examples = 45
05/30/2023 12:40:50 - INFO - __main__ -   Num Epochs = 150
05/30/2023 12:40:50 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:40:50 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:40:50 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:40:50 - INFO - __main__ -   Total optimization steps = 300
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 15.696444511413574
Total epoch: 1. epoch loss: 14.731444358825684
Total epoch: 2. epoch loss: 13.789670944213867
Total epoch: 3. epoch loss: 12.872032165527344
Total epoch: 4. epoch loss: 11.979921340942383
Total epoch: 5. epoch loss: 11.116201400756836
Total epoch: 6. epoch loss: 10.285016059875488
Total epoch: 7. epoch loss: 9.491128921508789
Total epoch: 8. epoch loss: 8.739014625549316
Total epoch: 9. epoch loss: 8.0322847366333
Total epoch: 10. epoch loss: 7.373349666595459
Total epoch: 11. epoch loss: 6.763326168060303
Total epoch: 12. epoch loss: 6.20205020904541
Total epoch: 13. epoch loss: 5.688329219818115
Total epoch: 14. epoch loss: 5.220028877258301
Total epoch: 15. epoch loss: 4.794203758239746
Total epoch: 16. epoch loss: 4.407224655151367
Total epoch: 17. epoch loss: 4.055070400238037
Total epoch: 18. epoch loss: 3.733690023422241
Total epoch: 19. epoch loss: 3.4397506713867188
Total epoch: 20. epoch loss: 3.171473264694214
Total epoch: 21. epoch loss: 2.927145481109619
Total epoch: 22. epoch loss: 2.705199718475342
Total epoch: 23. epoch loss: 2.5040974617004395
Total epoch: 24. epoch loss: 2.32230544090271
Total epoch: 25. epoch loss: 2.1583333015441895
Total epoch: 26. epoch loss: 2.01072359085083
Total epoch: 27. epoch loss: 1.8781013488769531
Total epoch: 28. epoch loss: 1.7591241598129272
Total epoch: 29. epoch loss: 1.65251624584198
Total epoch: 30. epoch loss: 1.5570018291473389
Total epoch: 31. epoch loss: 1.4713832139968872
Total epoch: 32. epoch loss: 1.3945120573043823
Total epoch: 33. epoch loss: 1.3253238201141357
Total epoch: 34. epoch loss: 1.2628885507583618
Total epoch: 35. epoch loss: 1.2063812017440796
Total epoch: 36. epoch loss: 1.155085802078247
Total epoch: 37. epoch loss: 1.1084002256393433
Total epoch: 38. epoch loss: 1.0657891035079956
Total epoch: 39. epoch loss: 1.0267834663391113
Total epoch: 40. epoch loss: 0.9909960031509399
Total epoch: 41. epoch loss: 0.9580618143081665
Total epoch: 42. epoch loss: 0.9276746511459351
Total epoch: 43. epoch loss: 0.899581789970398
Total epoch: 44. epoch loss: 0.8735422492027283
Total epoch: 45. epoch loss: 0.8493555784225464
Total epoch: 46. epoch loss: 0.8268380761146545
Total epoch: 47. epoch loss: 0.8058255910873413
Total epoch: 48. epoch loss: 0.7861706018447876
Total epoch: 49. epoch loss: 0.7677369713783264
Total epoch: 50. epoch loss: 0.7504099011421204
Total epoch: 51. epoch loss: 0.7340819835662842
Total epoch: 52. epoch loss: 0.7186571955680847
Total epoch: 53. epoch loss: 0.704060435295105
Total epoch: 54. epoch loss: 0.6902191638946533
Total epoch: 55. epoch loss: 0.677065372467041
Total epoch: 56. epoch loss: 0.6645447015762329
Total epoch: 57. epoch loss: 0.6526116728782654
Total epoch: 58. epoch loss: 0.6412149667739868
Total epoch: 59. epoch loss: 0.6303209066390991
Total epoch: 60. epoch loss: 0.6198898553848267
Total epoch: 61. epoch loss: 0.6098908185958862
Total epoch: 62. epoch loss: 0.6002964377403259
Total epoch: 63. epoch loss: 0.5910834670066833
Total epoch: 64. epoch loss: 0.5822204351425171
Total epoch: 65. epoch loss: 0.573689877986908
Total epoch: 66. epoch loss: 0.5654659271240234
Total epoch: 67. epoch loss: 0.5575324296951294
Total epoch: 68. epoch loss: 0.5498741269111633
Total epoch: 69. epoch loss: 0.5424701571464539
Total epoch: 70. epoch loss: 0.5353066325187683
Total epoch: 71. epoch loss: 0.5283713340759277
Total epoch: 72. epoch loss: 0.5216494202613831
Total epoch: 73. epoch loss: 0.5151281356811523
Total epoch: 74. epoch loss: 0.5088003873825073
Total epoch: 75. epoch loss: 0.5026543140411377
Total epoch: 76. epoch loss: 0.49668222665786743
Total epoch: 77. epoch loss: 0.4908759891986847
Total epoch: 78. epoch loss: 0.48522451519966125
Total epoch: 79. epoch loss: 0.4797259271144867
Total epoch: 80. epoch loss: 0.4743672311306
Total epoch: 81. epoch loss: 0.4691452383995056
Total epoch: 82. epoch loss: 0.464057058095932
Total epoch: 83. epoch loss: 0.4590916931629181
Total epoch: 84. epoch loss: 0.45424818992614746
Total epoch: 85. epoch loss: 0.44951537251472473
Total epoch: 86. epoch loss: 0.44489410519599915
Total epoch: 87. epoch loss: 0.44037944078445435
Total epoch: 88. epoch loss: 0.4359629154205322
Total epoch: 89. epoch loss: 0.4316452741622925
Total epoch: 90. epoch loss: 0.4274216592311859
Total epoch: 91. epoch loss: 0.42329034209251404
Total epoch: 92. epoch loss: 0.4192420542240143
Total epoch: 93. epoch loss: 0.41528013348579407
Total epoch: 94. epoch loss: 0.4113982617855072
Total epoch: 95. epoch loss: 0.4075944423675537
Total epoch: 96. epoch loss: 0.4038670063018799
Total epoch: 97. epoch loss: 0.4002133011817932
Total epoch: 98. epoch loss: 0.3966301679611206
Total epoch: 99. epoch loss: 0.39311638474464417
Total epoch: 100. epoch loss: 0.38966840505599976
Total epoch: 101. epoch loss: 0.3862847685813904
Total epoch: 102. epoch loss: 0.38296428322792053
Total epoch: 103. epoch loss: 0.3797050416469574
Total epoch: 104. epoch loss: 0.37650439143180847
Total epoch: 105. epoch loss: 0.3733592629432678
Total epoch: 106. epoch loss: 0.37027162313461304
Total epoch: 107. epoch loss: 0.36723753809928894
Total epoch: 108. epoch loss: 0.3642575740814209
Total epoch: 109. epoch loss: 0.36132773756980896
Total epoch: 110. epoch loss: 0.3584478795528412
Total epoch: 111. epoch loss: 0.3556162118911743
Total epoch: 112. epoch loss: 0.35283175110816956
Total epoch: 113. epoch loss: 0.3500950336456299
Total epoch: 114. epoch loss: 0.34739962220191956
Total epoch: 115. epoch loss: 0.34475037455558777
Total epoch: 116. epoch loss: 0.34214329719543457
Total epoch: 117. epoch loss: 0.3395771086215973
Total epoch: 118. epoch loss: 0.3370516896247864
Total epoch: 119. epoch loss: 0.33456864953041077
Total epoch: 120. epoch loss: 0.33212006092071533
Total epoch: 121. epoch loss: 0.3297114968299866
Total epoch: 122. epoch loss: 0.32733723521232605
Total epoch: 123. epoch loss: 0.3250023126602173
Total epoch: 124. epoch loss: 0.3227018713951111
Total epoch: 125. epoch loss: 0.3204350471496582
Total epoch: 126. epoch loss: 0.31820210814476013
Total epoch: 127. epoch loss: 0.3160025179386139
Total epoch: 128. epoch loss: 0.31383341550827026
Total epoch: 129. epoch loss: 0.3116978406906128
Total epoch: 130. epoch loss: 0.30959147214889526
Total epoch: 131. epoch loss: 0.30751675367355347
Total epoch: 132. epoch loss: 0.3054690361022949
Total epoch: 133. epoch loss: 0.30345189571380615
Total epoch: 134. epoch loss: 0.30146247148513794
Total epoch: 135. epoch loss: 0.2995004653930664
Total epoch: 136. epoch loss: 0.297566682100296
Total epoch: 137. epoch loss: 0.2956587076187134
Total epoch: 138. epoch loss: 0.29377618432044983
Total epoch: 139. epoch loss: 0.2919189929962158
Total epoch: 140. epoch loss: 0.2900888919830322
Total epoch: 141. epoch loss: 0.2882816195487976
Total epoch: 142. epoch loss: 0.28649839758872986
Total epoch: 143. epoch loss: 0.28473854064941406
Total epoch: 144. epoch loss: 0.2830010652542114
Total epoch: 145. epoch loss: 0.28128716349601746
Total epoch: 146. epoch loss: 0.279596209526062
Total epoch: 147. epoch loss: 0.27792441844940186
Total epoch: 148. epoch loss: 0.27627745270729065
Total epoch: 149. epoch loss: 0.2746499478816986
Total epoch: 149. DecT loss: 0.2746499478816986
Training time: 0.7243223190307617
APL_precision: 0.34080717488789236, APL_recall: 0.4470588235294118, APL_f1: 0.3867684478371501, APL_number: 170
CMT_precision: 0.44339622641509435, CMT_recall: 0.48205128205128206, CMT_f1: 0.46191646191646185, CMT_number: 195
DSC_precision: 0.5337078651685393, DSC_recall: 0.43478260869565216, DSC_f1: 0.4791929382093316, DSC_number: 437
MAT_precision: 0.47270306258322237, MAT_recall: 0.5205278592375366, MAT_f1: 0.4954640614096302, MAT_number: 682
PRO_precision: 0.44652908067542213, PRO_recall: 0.3086900129701686, PRO_f1: 0.3650306748466258, PRO_number: 771
SMT_precision: 0.29743589743589743, SMT_recall: 0.3391812865497076, SMT_f1: 0.31693989071038253, SMT_number: 171
SPL_precision: 0.32608695652173914, SPL_recall: 0.4, SPL_f1: 0.3592814371257485, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.44072819644369177, overall_recall: 0.4162335065973611, overall_f1: 0.42813078346699573, overall_accuracy: 0.8206704309913516
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]