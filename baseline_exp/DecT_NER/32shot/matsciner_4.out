/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:34:28 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:34:30 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1206.30it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:34:47 - INFO - __main__ - ***** Running training *****
05/30/2023 12:34:47 - INFO - __main__ -   Num examples = 72
05/30/2023 12:34:47 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:34:47 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:34:47 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:34:47 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:34:47 - INFO - __main__ -   Total optimization steps = 105
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/105 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.979442596435547
Total epoch: 1. epoch loss: 15.405595779418945
Total epoch: 2. epoch loss: 14.84089183807373
Total epoch: 3. epoch loss: 14.285367012023926
Total epoch: 4. epoch loss: 13.739087104797363
Total epoch: 5. epoch loss: 13.202226638793945
Total epoch: 6. epoch loss: 12.675272941589355
Total epoch: 7. epoch loss: 12.158992767333984
Total epoch: 8. epoch loss: 11.654438972473145
Total epoch: 9. epoch loss: 11.16265869140625
Total epoch: 10. epoch loss: 10.684738159179688
Total epoch: 11. epoch loss: 10.221559524536133
Total epoch: 12. epoch loss: 9.77385139465332
Total epoch: 13. epoch loss: 9.342168807983398
Total epoch: 14. epoch loss: 8.926887512207031
Total epoch: 15. epoch loss: 8.528182029724121
Total epoch: 16. epoch loss: 8.146138191223145
Total epoch: 17. epoch loss: 7.780736446380615
Total epoch: 18. epoch loss: 7.431825160980225
Total epoch: 19. epoch loss: 7.0991950035095215
Total epoch: 20. epoch loss: 6.782501220703125
Total epoch: 21. epoch loss: 6.4812750816345215
Total epoch: 22. epoch loss: 6.194931983947754
Total epoch: 23. epoch loss: 5.92274284362793
Total epoch: 24. epoch loss: 5.66401481628418
Total epoch: 25. epoch loss: 5.417869567871094
Total epoch: 26. epoch loss: 5.183493614196777
Total epoch: 27. epoch loss: 4.960155487060547
Total epoch: 28. epoch loss: 4.747194290161133
Total epoch: 29. epoch loss: 4.544065475463867
Total epoch: 30. epoch loss: 4.350874900817871
Total epoch: 31. epoch loss: 4.167527198791504
Total epoch: 32. epoch loss: 3.993849277496338
Total epoch: 33. epoch loss: 3.8295693397521973
Total epoch: 34. epoch loss: 3.6743969917297363
Total epoch: 34. DecT loss: 3.6743969917297363
Training time: 0.2497713565826416
APL_precision: 0.1514360313315927, APL_recall: 0.3411764705882353, APL_f1: 0.20976491862567814, APL_number: 170
CMT_precision: 0.33175355450236965, CMT_recall: 0.358974358974359, CMT_f1: 0.3448275862068966, CMT_number: 195
DSC_precision: 0.30609597924773024, DSC_recall: 0.540045766590389, DSC_f1: 0.3907284768211921, DSC_number: 437
MAT_precision: 0.3521531100478469, MAT_recall: 0.5395894428152492, MAT_f1: 0.4261725535610886, MAT_number: 682
PRO_precision: 0.3875338753387534, PRO_recall: 0.556420233463035, PRO_f1: 0.4568690095846646, PRO_number: 771
SMT_precision: 0.2971246006389776, SMT_recall: 0.543859649122807, SMT_f1: 0.384297520661157, SMT_number: 171
SPL_precision: 0.20161290322580644, SPL_recall: 0.3333333333333333, SPL_f1: 0.25125628140703515, SPL_number: 75
overall_precision: 0.3234699038947901, overall_recall: 0.5113954418232707, overall_f1: 0.39628195197521293, overall_accuracy: 0.7748552640983489
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/105 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:37:30 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:37:31 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:128, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1159.61it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:37:38 - INFO - __main__ - ***** Running training *****
05/30/2023 12:37:38 - INFO - __main__ -   Num examples = 72
05/30/2023 12:37:38 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:37:38 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:37:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:37:38 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:37:38 - INFO - __main__ -   Total optimization steps = 300
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.973989486694336
Total epoch: 1. epoch loss: 15.172347068786621
Total epoch: 2. epoch loss: 14.388314247131348
Total epoch: 3. epoch loss: 13.622418403625488
Total epoch: 4. epoch loss: 12.87585163116455
Total epoch: 5. epoch loss: 12.150651931762695
Total epoch: 6. epoch loss: 11.449525833129883
Total epoch: 7. epoch loss: 10.775270462036133
Total epoch: 8. epoch loss: 10.13025951385498
Total epoch: 9. epoch loss: 9.516275405883789
Total epoch: 10. epoch loss: 8.934463500976562
Total epoch: 11. epoch loss: 8.385486602783203
Total epoch: 12. epoch loss: 7.869476795196533
Total epoch: 13. epoch loss: 7.38624382019043
Total epoch: 14. epoch loss: 6.935177803039551
Total epoch: 15. epoch loss: 6.515267372131348
Total epoch: 16. epoch loss: 6.125067234039307
Total epoch: 17. epoch loss: 5.762701034545898
Total epoch: 18. epoch loss: 5.425907611846924
Total epoch: 19. epoch loss: 5.112239837646484
Total epoch: 20. epoch loss: 4.819379806518555
Total epoch: 21. epoch loss: 4.545297622680664
Total epoch: 22. epoch loss: 4.2892842292785645
Total epoch: 23. epoch loss: 4.050670146942139
Total epoch: 24. epoch loss: 3.8286476135253906
Total epoch: 25. epoch loss: 3.6223599910736084
Total epoch: 26. epoch loss: 3.4308629035949707
Total epoch: 27. epoch loss: 3.253289222717285
Total epoch: 28. epoch loss: 3.0888497829437256
Total epoch: 29. epoch loss: 2.936763286590576
Total epoch: 30. epoch loss: 2.7962982654571533
Total epoch: 31. epoch loss: 2.6666622161865234
Total epoch: 32. epoch loss: 2.5470361709594727
Total epoch: 33. epoch loss: 2.4365968704223633
Total epoch: 34. epoch loss: 2.3344945907592773
Total epoch: 35. epoch loss: 2.2399628162384033
Total epoch: 36. epoch loss: 2.152296781539917
Total epoch: 37. epoch loss: 2.070876359939575
Total epoch: 38. epoch loss: 1.9951725006103516
Total epoch: 39. epoch loss: 1.924682378768921
Total epoch: 40. epoch loss: 1.8589979410171509
Total epoch: 41. epoch loss: 1.7976996898651123
Total epoch: 42. epoch loss: 1.7404297590255737
Total epoch: 43. epoch loss: 1.686858057975769
Total epoch: 44. epoch loss: 1.6366939544677734
Total epoch: 45. epoch loss: 1.5896639823913574
Total epoch: 46. epoch loss: 1.5455259084701538
Total epoch: 47. epoch loss: 1.5040547847747803
Total epoch: 48. epoch loss: 1.465043067932129
Total epoch: 49. epoch loss: 1.4282983541488647
Total epoch: 50. epoch loss: 1.3936326503753662
Total epoch: 51. epoch loss: 1.360877275466919
Total epoch: 52. epoch loss: 1.3298882246017456
Total epoch: 53. epoch loss: 1.3005237579345703
Total epoch: 54. epoch loss: 1.272659182548523
Total epoch: 55. epoch loss: 1.2461882829666138
Total epoch: 56. epoch loss: 1.2210081815719604
Total epoch: 57. epoch loss: 1.197031021118164
Total epoch: 58. epoch loss: 1.174170732498169
Total epoch: 59. epoch loss: 1.1523568630218506
Total epoch: 60. epoch loss: 1.131524682044983
Total epoch: 61. epoch loss: 1.1116098165512085
Total epoch: 62. epoch loss: 1.0925554037094116
Total epoch: 63. epoch loss: 1.0743125677108765
Total epoch: 64. epoch loss: 1.0568265914916992
Total epoch: 65. epoch loss: 1.0400581359863281
Total epoch: 66. epoch loss: 1.0239636898040771
Total epoch: 67. epoch loss: 1.0085039138793945
Total epoch: 68. epoch loss: 0.9936387538909912
Total epoch: 69. epoch loss: 0.9793373942375183
Total epoch: 70. epoch loss: 0.9655674695968628
Total epoch: 71. epoch loss: 0.952301561832428
Total epoch: 72. epoch loss: 0.9395027756690979
Total epoch: 73. epoch loss: 0.9271531105041504
Total epoch: 74. epoch loss: 0.915230929851532
Total epoch: 75. epoch loss: 0.9037033915519714
Total epoch: 76. epoch loss: 0.8925610184669495
Total epoch: 77. epoch loss: 0.8817703723907471
Total epoch: 78. epoch loss: 0.8713198304176331
Total epoch: 79. epoch loss: 0.8611944317817688
Total epoch: 80. epoch loss: 0.8513714075088501
Total epoch: 81. epoch loss: 0.8418384790420532
Total epoch: 82. epoch loss: 0.8325796723365784
Total epoch: 83. epoch loss: 0.8235810995101929
Total epoch: 84. epoch loss: 0.8148345351219177
Total epoch: 85. epoch loss: 0.8063206672668457
Total epoch: 86. epoch loss: 0.7980335354804993
Total epoch: 87. epoch loss: 0.7899658679962158
Total epoch: 88. epoch loss: 0.7821047902107239
Total epoch: 89. epoch loss: 0.7744390368461609
Total epoch: 90. epoch loss: 0.7669645547866821
Total epoch: 91. epoch loss: 0.7596709132194519
Total epoch: 92. epoch loss: 0.7525530457496643
Total epoch: 93. epoch loss: 0.7455973029136658
Total epoch: 94. epoch loss: 0.738804280757904
Total epoch: 95. epoch loss: 0.7321637272834778
Total epoch: 96. epoch loss: 0.7256758213043213
Total epoch: 97. epoch loss: 0.7193240523338318
Total epoch: 98. epoch loss: 0.7131108641624451
Total epoch: 99. epoch loss: 0.7070348262786865
Total epoch: 99. DecT loss: 0.7070348262786865
Training time: 0.5543608665466309
APL_precision: 0.2706270627062706, APL_recall: 0.4823529411764706, APL_f1: 0.346723044397463, APL_number: 170
CMT_precision: 0.5213270142180095, CMT_recall: 0.5641025641025641, CMT_f1: 0.541871921182266, CMT_number: 195
DSC_precision: 0.4570383912248629, DSC_recall: 0.5720823798627003, DSC_f1: 0.508130081300813, DSC_number: 437
MAT_precision: 0.5235223160434258, MAT_recall: 0.6363636363636364, MAT_f1: 0.5744540039708802, MAT_number: 682
PRO_precision: 0.4252232142857143, PRO_recall: 0.49416342412451364, PRO_f1: 0.4571085782843431, PRO_number: 771
SMT_precision: 0.3534136546184739, SMT_recall: 0.5146198830409356, SMT_f1: 0.41904761904761906, SMT_number: 171
SPL_precision: 0.32, SPL_recall: 0.4266666666666667, SPL_f1: 0.3657142857142857, SPL_number: 75
overall_precision: 0.43923444976076553, overall_recall: 0.5505797680927629, overall_f1: 0.48864442867281754, overall_accuracy: 0.8329640483167751
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:41:02 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:41:03 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1119.23it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:41:09 - INFO - __main__ - ***** Running training *****
05/30/2023 12:41:09 - INFO - __main__ -   Num examples = 72
05/30/2023 12:41:09 - INFO - __main__ -   Num Epochs = 150
05/30/2023 12:41:09 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:41:09 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:41:09 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:41:09 - INFO - __main__ -   Total optimization steps = 450
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/450 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 15.938867568969727
Total epoch: 1. epoch loss: 15.046720504760742
Total epoch: 2. epoch loss: 14.176850318908691
Total epoch: 3. epoch loss: 13.329522132873535
Total epoch: 4. epoch loss: 12.50645637512207
Total epoch: 5. epoch loss: 11.71106243133545
Total epoch: 6. epoch loss: 10.947495460510254
Total epoch: 7. epoch loss: 10.219600677490234
Total epoch: 8. epoch loss: 9.530247688293457
Total epoch: 9. epoch loss: 8.881240844726562
Total epoch: 10. epoch loss: 8.273415565490723
Total epoch: 11. epoch loss: 7.706955909729004
Total epoch: 12. epoch loss: 7.181488513946533
Total epoch: 13. epoch loss: 6.696088790893555
Total epoch: 14. epoch loss: 6.249222755432129
Total epoch: 15. epoch loss: 5.838602066040039
Total epoch: 16. epoch loss: 5.461269855499268
Total epoch: 17. epoch loss: 5.11373233795166
Total epoch: 18. epoch loss: 4.792462348937988
Total epoch: 19. epoch loss: 4.4944353103637695
Total epoch: 20. epoch loss: 4.218313217163086
Total epoch: 21. epoch loss: 3.9628849029541016
Total epoch: 22. epoch loss: 3.7269129753112793
Total epoch: 23. epoch loss: 3.5091395378112793
Total epoch: 24. epoch loss: 3.308321952819824
Total epoch: 25. epoch loss: 3.1232872009277344
Total epoch: 26. epoch loss: 2.952968120574951
Total epoch: 27. epoch loss: 2.7964210510253906
Total epoch: 28. epoch loss: 2.6527302265167236
Total epoch: 29. epoch loss: 2.5209782123565674
Total epoch: 30. epoch loss: 2.400202751159668
Total epoch: 31. epoch loss: 2.2893929481506348
Total epoch: 32. epoch loss: 2.1875758171081543
Total epoch: 33. epoch loss: 2.093817949295044
Total epoch: 34. epoch loss: 2.0073087215423584
Total epoch: 35. epoch loss: 1.927329421043396
Total epoch: 36. epoch loss: 1.8532828092575073
Total epoch: 37. epoch loss: 1.7846331596374512
Total epoch: 38. epoch loss: 1.7209126949310303
Total epoch: 39. epoch loss: 1.6616742610931396
Total epoch: 40. epoch loss: 1.606544852256775
Total epoch: 41. epoch loss: 1.5551512241363525
Total epoch: 42. epoch loss: 1.50718092918396
Total epoch: 43. epoch loss: 1.462355613708496
Total epoch: 44. epoch loss: 1.4204187393188477
Total epoch: 45. epoch loss: 1.38114595413208
Total epoch: 46. epoch loss: 1.3443111181259155
Total epoch: 47. epoch loss: 1.3097261190414429
Total epoch: 48. epoch loss: 1.2771986722946167
Total epoch: 49. epoch loss: 1.2465513944625854
Total epoch: 50. epoch loss: 1.2176324129104614
Total epoch: 51. epoch loss: 1.1903003454208374
Total epoch: 52. epoch loss: 1.164421796798706
Total epoch: 53. epoch loss: 1.1398910284042358
Total epoch: 54. epoch loss: 1.116595983505249
Total epoch: 55. epoch loss: 1.0944576263427734
Total epoch: 56. epoch loss: 1.073384404182434
Total epoch: 57. epoch loss: 1.0533093214035034
Total epoch: 58. epoch loss: 1.034165859222412
Total epoch: 59. epoch loss: 1.0158884525299072
Total epoch: 60. epoch loss: 0.9984226822853088
Total epoch: 61. epoch loss: 0.9817233681678772
Total epoch: 62. epoch loss: 0.9657450318336487
Total epoch: 63. epoch loss: 0.9504396915435791
Total epoch: 64. epoch loss: 0.9357720613479614
Total epoch: 65. epoch loss: 0.9216976165771484
Total epoch: 66. epoch loss: 0.9081888198852539
Total epoch: 67. epoch loss: 0.8952005505561829
Total epoch: 68. epoch loss: 0.8827076554298401
Total epoch: 69. epoch loss: 0.8706820011138916
Total epoch: 70. epoch loss: 0.8590863943099976
Total epoch: 71. epoch loss: 0.8479053378105164
Total epoch: 72. epoch loss: 0.8371075391769409
Total epoch: 73. epoch loss: 0.8266759514808655
Total epoch: 74. epoch loss: 0.8165865540504456
Total epoch: 75. epoch loss: 0.8068240880966187
Total epoch: 76. epoch loss: 0.7973639965057373
Total epoch: 77. epoch loss: 0.7882001399993896
Total epoch: 78. epoch loss: 0.7793086171150208
Total epoch: 79. epoch loss: 0.7706824541091919
Total epoch: 80. epoch loss: 0.7623011469841003
Total epoch: 81. epoch loss: 0.7541552186012268
Total epoch: 82. epoch loss: 0.7462354302406311
Total epoch: 83. epoch loss: 0.7385322451591492
Total epoch: 84. epoch loss: 0.7310331463813782
Total epoch: 85. epoch loss: 0.7237266302108765
Total epoch: 86. epoch loss: 0.716609537601471
Total epoch: 87. epoch loss: 0.7096673846244812
Total epoch: 88. epoch loss: 0.7029000520706177
Total epoch: 89. epoch loss: 0.6962946057319641
Total epoch: 90. epoch loss: 0.6898470520973206
Total epoch: 91. epoch loss: 0.6835499405860901
Total epoch: 92. epoch loss: 0.6773958802223206
Total epoch: 93. epoch loss: 0.6713812351226807
Total epoch: 94. epoch loss: 0.66550213098526
Total epoch: 95. epoch loss: 0.659747302532196
Total epoch: 96. epoch loss: 0.6541216969490051
Total epoch: 97. epoch loss: 0.64860999584198
Total epoch: 98. epoch loss: 0.6432177424430847
Total epoch: 99. epoch loss: 0.6379339694976807
Total epoch: 100. epoch loss: 0.6327547430992126
Total epoch: 101. epoch loss: 0.6276816725730896
Total epoch: 102. epoch loss: 0.6227056384086609
Total epoch: 103. epoch loss: 0.6178278923034668
Total epoch: 104. epoch loss: 0.6130379438400269
Total epoch: 105. epoch loss: 0.6083456873893738
Total epoch: 106. epoch loss: 0.6037375330924988
Total epoch: 107. epoch loss: 0.5992116928100586
Total epoch: 108. epoch loss: 0.5947695970535278
Total epoch: 109. epoch loss: 0.5904061198234558
Total epoch: 110. epoch loss: 0.5861205458641052
Total epoch: 111. epoch loss: 0.5819078683853149
Total epoch: 112. epoch loss: 0.5777686834335327
Total epoch: 113. epoch loss: 0.5737001299858093
Total epoch: 114. epoch loss: 0.5697008967399597
Total epoch: 115. epoch loss: 0.5657663345336914
Total epoch: 116. epoch loss: 0.5618993043899536
Total epoch: 117. epoch loss: 0.558091402053833
Total epoch: 118. epoch loss: 0.5543482303619385
Total epoch: 119. epoch loss: 0.5506631135940552
Total epoch: 120. epoch loss: 0.5470350384712219
Total epoch: 121. epoch loss: 0.5434669256210327
Total epoch: 122. epoch loss: 0.5399528741836548
Total epoch: 123. epoch loss: 0.536491334438324
Total epoch: 124. epoch loss: 0.5330840945243835
Total epoch: 125. epoch loss: 0.5297274589538574
Total epoch: 126. epoch loss: 0.5264219641685486
Total epoch: 127. epoch loss: 0.523162841796875
Total epoch: 128. epoch loss: 0.5199528336524963
Total epoch: 129. epoch loss: 0.5167906284332275
Total epoch: 130. epoch loss: 0.5136712789535522
Total epoch: 131. epoch loss: 0.5105991363525391
Total epoch: 132. epoch loss: 0.5075681805610657
Total epoch: 133. epoch loss: 0.5045819878578186
Total epoch: 134. epoch loss: 0.5016358494758606
Total epoch: 135. epoch loss: 0.4987313151359558
Total epoch: 136. epoch loss: 0.49586471915245056
Total epoch: 137. epoch loss: 0.49303901195526123
Total epoch: 138. epoch loss: 0.4902508854866028
Total epoch: 139. epoch loss: 0.4874996542930603
Total epoch: 140. epoch loss: 0.48478448390960693
Total epoch: 141. epoch loss: 0.48210468888282776
Total epoch: 142. epoch loss: 0.4794597327709198
Total epoch: 143. epoch loss: 0.47685155272483826
Total epoch: 144. epoch loss: 0.474275141954422
Total epoch: 145. epoch loss: 0.4717337191104889
Total epoch: 146. epoch loss: 0.4692230224609375
Total epoch: 147. epoch loss: 0.46674391627311707
Total epoch: 148. epoch loss: 0.46429383754730225
Total epoch: 149. epoch loss: 0.4618781805038452
Total epoch: 149. DecT loss: 0.4618781805038452
Training time: 0.6859793663024902
APL_precision: 0.2892857142857143, APL_recall: 0.4764705882352941, APL_f1: 0.36000000000000004, APL_number: 170
CMT_precision: 0.5631067961165048, CMT_recall: 0.5948717948717949, CMT_f1: 0.5785536159600998, CMT_number: 195
DSC_precision: 0.48526522593320237, DSC_recall: 0.5652173913043478, DSC_f1: 0.5221987315010571, DSC_number: 437
MAT_precision: 0.5617128463476071, MAT_recall: 0.6539589442815249, MAT_f1: 0.6043360433604337, MAT_number: 682
PRO_precision: 0.42080378250591016, PRO_recall: 0.46173800259403375, PRO_f1: 0.440321583178726, PRO_number: 771
SMT_precision: 0.41284403669724773, SMT_recall: 0.5263157894736842, SMT_f1: 0.46272493573264784, SMT_number: 171
SPL_precision: 0.3411764705882353, SPL_recall: 0.38666666666666666, SPL_f1: 0.3625, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.4646017699115044, overall_recall: 0.5457816873250699, overall_f1: 0.5019305019305019, overall_accuracy: 0.8383246372668144
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/450 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:14:51 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:14:53 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:150,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:2.0, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1149.12it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5177.60 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:15:00 - INFO - __main__ - ***** Running training *****
05/31/2023 13:15:00 - INFO - __main__ -   Num examples = 72
05/31/2023 13:15:00 - INFO - __main__ -   Num Epochs = 150
05/31/2023 13:15:00 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:15:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:15:00 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:15:00 - INFO - __main__ -   Total optimization steps = 450
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/450 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 16.45046043395996
Total epoch: 1. epoch loss: 15.548874855041504
Total epoch: 2. epoch loss: 14.659989356994629
Total epoch: 3. epoch loss: 13.786988258361816
Total epoch: 4. epoch loss: 12.934476852416992
Total epoch: 5. epoch loss: 12.107941627502441
Total epoch: 6. epoch loss: 11.313028335571289
Total epoch: 7. epoch loss: 10.554720878601074
Total epoch: 8. epoch loss: 9.836813926696777
Total epoch: 9. epoch loss: 9.161641120910645
Total epoch: 10. epoch loss: 8.530292510986328
Total epoch: 11. epoch loss: 7.9428253173828125
Total epoch: 12. epoch loss: 7.398670196533203
Total epoch: 13. epoch loss: 6.896685600280762
Total epoch: 14. epoch loss: 6.435211181640625
Total epoch: 15. epoch loss: 6.011759281158447
Total epoch: 16. epoch loss: 5.623124599456787
Total epoch: 17. epoch loss: 5.265533447265625
Total epoch: 18. epoch loss: 4.935266017913818
Total epoch: 19. epoch loss: 4.629016399383545
Total epoch: 20. epoch loss: 4.345041751861572
Total epoch: 21. epoch loss: 4.082125663757324
Total epoch: 22. epoch loss: 3.839019775390625
Total epoch: 23. epoch loss: 3.6144938468933105
Total epoch: 24. epoch loss: 3.4073941707611084
Total epoch: 25. epoch loss: 3.2166409492492676
Total epoch: 26. epoch loss: 3.0412440299987793
Total epoch: 27. epoch loss: 2.880300998687744
Total epoch: 28. epoch loss: 2.7328357696533203
Total epoch: 29. epoch loss: 2.597853899002075
Total epoch: 30. epoch loss: 2.474247932434082
Total epoch: 31. epoch loss: 2.3609416484832764
Total epoch: 32. epoch loss: 2.256869077682495
Total epoch: 33. epoch loss: 2.16103458404541
Total epoch: 34. epoch loss: 2.0725760459899902
Total epoch: 35. epoch loss: 1.9907430410385132
Total epoch: 36. epoch loss: 1.914890170097351
Total epoch: 37. epoch loss: 1.8444596529006958
Total epoch: 38. epoch loss: 1.7789818048477173
Total epoch: 39. epoch loss: 1.7180346250534058
Total epoch: 40. epoch loss: 1.6612327098846436
Total epoch: 41. epoch loss: 1.60824453830719
Total epoch: 42. epoch loss: 1.5587713718414307
Total epoch: 43. epoch loss: 1.5125296115875244
Total epoch: 44. epoch loss: 1.469254732131958
Total epoch: 45. epoch loss: 1.4287011623382568
Total epoch: 46. epoch loss: 1.3906340599060059
Total epoch: 47. epoch loss: 1.3548442125320435
Total epoch: 48. epoch loss: 1.321146845817566
Total epoch: 49. epoch loss: 1.2893574237823486
Total epoch: 50. epoch loss: 1.259332537651062
Total epoch: 51. epoch loss: 1.2309211492538452
Total epoch: 52. epoch loss: 1.2040070295333862
Total epoch: 53. epoch loss: 1.1784852743148804
Total epoch: 54. epoch loss: 1.1542530059814453
Total epoch: 55. epoch loss: 1.131223201751709
Total epoch: 56. epoch loss: 1.1093151569366455
Total epoch: 57. epoch loss: 1.0884555578231812
Total epoch: 58. epoch loss: 1.0685683488845825
Total epoch: 59. epoch loss: 1.049601674079895
Total epoch: 60. epoch loss: 1.031487226486206
Total epoch: 61. epoch loss: 1.0141745805740356
Total epoch: 62. epoch loss: 0.9976081848144531
Total epoch: 63. epoch loss: 0.9817439317703247
Total epoch: 64. epoch loss: 0.9665354490280151
Total epoch: 65. epoch loss: 0.951947033405304
Total epoch: 66. epoch loss: 0.9379321932792664
Total epoch: 67. epoch loss: 0.9244609475135803
Total epoch: 68. epoch loss: 0.9115003347396851
Total epoch: 69. epoch loss: 0.8990151286125183
Total epoch: 70. epoch loss: 0.8869845271110535
Total epoch: 71. epoch loss: 0.8753796815872192
Total epoch: 72. epoch loss: 0.8641747832298279
Total epoch: 73. epoch loss: 0.8533561825752258
Total epoch: 74. epoch loss: 0.8428875207901001
Total epoch: 75. epoch loss: 0.8327652215957642
Total epoch: 76. epoch loss: 0.8229616284370422
Total epoch: 77. epoch loss: 0.8134603500366211
Total epoch: 78. epoch loss: 0.8042495846748352
Total epoch: 79. epoch loss: 0.7953163385391235
Total epoch: 80. epoch loss: 0.7866385579109192
Total epoch: 81. epoch loss: 0.7782055139541626
Total epoch: 82. epoch loss: 0.7700101137161255
Total epoch: 83. epoch loss: 0.7620401382446289
Total epoch: 84. epoch loss: 0.7542809247970581
Total epoch: 85. epoch loss: 0.7467262744903564
Total epoch: 86. epoch loss: 0.739366352558136
Total epoch: 87. epoch loss: 0.7321935892105103
Total epoch: 88. epoch loss: 0.7251966595649719
Total epoch: 89. epoch loss: 0.7183704972267151
Total epoch: 90. epoch loss: 0.7117076516151428
Total epoch: 91. epoch loss: 0.7052028179168701
Total epoch: 92. epoch loss: 0.6988482475280762
Total epoch: 93. epoch loss: 0.6926363110542297
Total epoch: 94. epoch loss: 0.686565637588501
Total epoch: 95. epoch loss: 0.6806282997131348
Total epoch: 96. epoch loss: 0.6748192310333252
Total epoch: 97. epoch loss: 0.6691373586654663
Total epoch: 98. epoch loss: 0.6635703444480896
Total epoch: 99. epoch loss: 0.6581202149391174
Total epoch: 100. epoch loss: 0.6527822613716125
Total epoch: 101. epoch loss: 0.6475498080253601
Total epoch: 102. epoch loss: 0.6424235105514526
Total epoch: 103. epoch loss: 0.6373962759971619
Total epoch: 104. epoch loss: 0.6324638724327087
Total epoch: 105. epoch loss: 0.6276245713233948
Total epoch: 106. epoch loss: 0.622881293296814
Total epoch: 107. epoch loss: 0.6182242631912231
Total epoch: 108. epoch loss: 0.6136481165885925
Total epoch: 109. epoch loss: 0.6091563701629639
Total epoch: 110. epoch loss: 0.6047466397285461
Total epoch: 111. epoch loss: 0.6004112362861633
Total epoch: 112. epoch loss: 0.5961546301841736
Total epoch: 113. epoch loss: 0.591969907283783
Total epoch: 114. epoch loss: 0.5878546237945557
Total epoch: 115. epoch loss: 0.5838097333908081
Total epoch: 116. epoch loss: 0.5798308849334717
Total epoch: 117. epoch loss: 0.5759204626083374
Total epoch: 118. epoch loss: 0.5720727443695068
Total epoch: 119. epoch loss: 0.5682859420776367
Total epoch: 120. epoch loss: 0.5645612478256226
Total epoch: 121. epoch loss: 0.5608904361724854
Total epoch: 122. epoch loss: 0.5572810769081116
Total epoch: 123. epoch loss: 0.5537275075912476
Total epoch: 124. epoch loss: 0.550228476524353
Total epoch: 125. epoch loss: 0.5467811226844788
Total epoch: 126. epoch loss: 0.5433859825134277
Total epoch: 127. epoch loss: 0.5400403738021851
Total epoch: 128. epoch loss: 0.5367476344108582
Total epoch: 129. epoch loss: 0.5335019826889038
Total epoch: 130. epoch loss: 0.5303012132644653
Total epoch: 131. epoch loss: 0.5271480083465576
Total epoch: 132. epoch loss: 0.5240398049354553
Total epoch: 133. epoch loss: 0.5209729671478271
Total epoch: 134. epoch loss: 0.5179516077041626
Total epoch: 135. epoch loss: 0.5149735808372498
Total epoch: 136. epoch loss: 0.5120335817337036
Total epoch: 137. epoch loss: 0.5091364979743958
Total epoch: 138. epoch loss: 0.5062776803970337
Total epoch: 139. epoch loss: 0.5034573078155518
Total epoch: 140. epoch loss: 0.5006749033927917
Total epoch: 141. epoch loss: 0.497929185628891
Total epoch: 142. epoch loss: 0.49522048234939575
Total epoch: 143. epoch loss: 0.49254468083381653
Total epoch: 144. epoch loss: 0.4899049401283264
Total epoch: 145. epoch loss: 0.48729950189590454
Total epoch: 146. epoch loss: 0.48472660779953003
Total epoch: 147. epoch loss: 0.48218727111816406
Total epoch: 148. epoch loss: 0.479681134223938
Total epoch: 149. epoch loss: 0.4772048890590668
Total epoch: 149. DecT loss: 0.4772048890590668
Training time: 0.9740970134735107
APL_precision: 0.2789115646258503, APL_recall: 0.4823529411764706, APL_f1: 0.353448275862069, APL_number: 170
CMT_precision: 0.5603864734299517, CMT_recall: 0.5948717948717949, CMT_f1: 0.5771144278606966, CMT_number: 195
DSC_precision: 0.4766536964980545, DSC_recall: 0.5606407322654462, DSC_f1: 0.5152471083070452, DSC_number: 437
MAT_precision: 0.5469543147208121, MAT_recall: 0.6319648093841642, MAT_f1: 0.5863945578231292, MAT_number: 682
PRO_precision: 0.4427020506634499, PRO_recall: 0.47600518806744485, PRO_f1: 0.45875, PRO_number: 771
SMT_precision: 0.3548387096774194, SMT_recall: 0.5146198830409356, SMT_f1: 0.4200477326968974, SMT_number: 171
SPL_precision: 0.32, SPL_recall: 0.4266666666666667, SPL_f1: 0.3657142857142857, SPL_number: 75
overall_precision: 0.45671140939597316, overall_recall: 0.5441823270691724, overall_f1: 0.49662470352125526, overall_accuracy: 0.8386820098634836
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/450 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:25:02 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:25:03 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1076.84it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4084.80 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:25:40 - INFO - __main__ - ***** Running training *****
05/31/2023 13:25:40 - INFO - __main__ -   Num examples = 72
05/31/2023 13:25:40 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:25:40 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:25:40 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:25:40 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:25:40 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.967978477478027
Total epoch: 1. epoch loss: 15.346302032470703
Total epoch: 2. epoch loss: 14.495085716247559
Total epoch: 3. epoch loss: 13.298261642456055
Total epoch: 4. epoch loss: 11.815559387207031
Total epoch: 5. epoch loss: 10.2015962600708
Total epoch: 6. epoch loss: 8.638564109802246
Total epoch: 7. epoch loss: 7.251498699188232
Total epoch: 8. epoch loss: 6.078253746032715
Total epoch: 9. epoch loss: 5.086916923522949
Total epoch: 10. epoch loss: 4.254605293273926
Total epoch: 11. epoch loss: 3.5845282077789307
Total epoch: 12. epoch loss: 3.049225091934204
Total epoch: 13. epoch loss: 2.600731611251831
Total epoch: 14. epoch loss: 2.226504325866699
Total epoch: 15. epoch loss: 1.9295419454574585
Total epoch: 16. epoch loss: 1.6897454261779785
Total epoch: 17. epoch loss: 1.4824795722961426
Total epoch: 18. epoch loss: 1.3063561916351318
Total epoch: 19. epoch loss: 1.1700801849365234
Total epoch: 20. epoch loss: 1.0611295700073242
Total epoch: 21. epoch loss: 0.965137779712677
Total epoch: 22. epoch loss: 0.8849837183952332
Total epoch: 23. epoch loss: 0.8234907984733582
Total epoch: 24. epoch loss: 0.7726708054542542
Total epoch: 25. epoch loss: 0.7256456613540649
Total epoch: 26. epoch loss: 0.6833963990211487
Total epoch: 27. epoch loss: 0.6472982168197632
Total epoch: 28. epoch loss: 0.6153613328933716
Total epoch: 29. epoch loss: 0.5865244269371033
Total epoch: 30. epoch loss: 0.5609757900238037
Total epoch: 31. epoch loss: 0.537743091583252
Total epoch: 32. epoch loss: 0.5161491632461548
Total epoch: 33. epoch loss: 0.49654558300971985
Total epoch: 34. epoch loss: 0.47860315442085266
Total epoch: 35. epoch loss: 0.4615136981010437
Total epoch: 36. epoch loss: 0.44551804661750793
Total epoch: 37. epoch loss: 0.43109920620918274
Total epoch: 38. epoch loss: 0.41777169704437256
Total epoch: 39. epoch loss: 0.4050943851470947
Total epoch: 40. epoch loss: 0.39324793219566345
Total epoch: 41. epoch loss: 0.3821951448917389
Total epoch: 42. epoch loss: 0.3716451823711395
Total epoch: 43. epoch loss: 0.36170369386672974
Total epoch: 44. epoch loss: 0.3525139093399048
Total epoch: 45. epoch loss: 0.34378954768180847
Total epoch: 46. epoch loss: 0.33540743589401245
Total epoch: 47. epoch loss: 0.32746803760528564
Total epoch: 48. epoch loss: 0.3198028802871704
Total epoch: 49. epoch loss: 0.31222936511039734
Total epoch: 50. epoch loss: 0.30490806698799133
Total epoch: 51. epoch loss: 0.297882080078125
Total epoch: 52. epoch loss: 0.29105156660079956
Total epoch: 53. epoch loss: 0.2845112979412079
Total epoch: 54. epoch loss: 0.27829331159591675
Total epoch: 55. epoch loss: 0.27229252457618713
Total epoch: 56. epoch loss: 0.2665746510028839
Total epoch: 57. epoch loss: 0.26119932532310486
Total epoch: 58. epoch loss: 0.2560585141181946
Total epoch: 59. epoch loss: 0.2511178255081177
Total epoch: 60. epoch loss: 0.24637441337108612
Total epoch: 61. epoch loss: 0.2417542040348053
Total epoch: 62. epoch loss: 0.23727649450302124
Total epoch: 63. epoch loss: 0.23299077153205872
Total epoch: 64. epoch loss: 0.22884495556354523
Total epoch: 65. epoch loss: 0.2248242050409317
Total epoch: 66. epoch loss: 0.22094139456748962
Total epoch: 67. epoch loss: 0.2171664535999298
Total epoch: 68. epoch loss: 0.21351340413093567
Total epoch: 69. epoch loss: 0.21001064777374268
Total epoch: 70. epoch loss: 0.2066137045621872
Total epoch: 71. epoch loss: 0.20332281291484833
Total epoch: 72. epoch loss: 0.2001403421163559
Total epoch: 73. epoch loss: 0.19705450534820557
Total epoch: 74. epoch loss: 0.19408360123634338
Total epoch: 75. epoch loss: 0.19121992588043213
Total epoch: 76. epoch loss: 0.18844348192214966
Total epoch: 77. epoch loss: 0.18576401472091675
Total epoch: 78. epoch loss: 0.18317028880119324
Total epoch: 79. epoch loss: 0.18065227568149567
Total epoch: 80. epoch loss: 0.17821863293647766
Total epoch: 81. epoch loss: 0.17584270238876343
Total epoch: 82. epoch loss: 0.1735299676656723
Total epoch: 83. epoch loss: 0.1712774932384491
Total epoch: 84. epoch loss: 0.16907943785190582
Total epoch: 85. epoch loss: 0.16694778203964233
Total epoch: 86. epoch loss: 0.16487161815166473
Total epoch: 87. epoch loss: 0.16285479068756104
Total epoch: 88. epoch loss: 0.16090179979801178
Total epoch: 89. epoch loss: 0.15899941325187683
Total epoch: 90. epoch loss: 0.15715226531028748
Total epoch: 91. epoch loss: 0.15535588562488556
Total epoch: 92. epoch loss: 0.1536097377538681
Total epoch: 93. epoch loss: 0.15191861987113953
Total epoch: 94. epoch loss: 0.15027643740177155
Total epoch: 95. epoch loss: 0.14868561923503876
Total epoch: 96. epoch loss: 0.14713838696479797
Total epoch: 97. epoch loss: 0.14563947916030884
Total epoch: 98. epoch loss: 0.14418596029281616
Total epoch: 99. epoch loss: 0.14277327060699463
Total epoch: 99. DecT loss: 0.14277327060699463
Training time: 0.6762721538543701
APL_precision: 0.2841726618705036, APL_recall: 0.4647058823529412, APL_f1: 0.35267857142857145, APL_number: 170
CMT_precision: 0.48677248677248675, CMT_recall: 0.4717948717948718, CMT_f1: 0.47916666666666663, CMT_number: 195
DSC_precision: 0.47715736040609136, DSC_recall: 0.4302059496567506, DSC_f1: 0.4524669073405535, DSC_number: 437
MAT_precision: 0.5737704918032787, MAT_recall: 0.5131964809384164, MAT_f1: 0.541795665634675, MAT_number: 682
PRO_precision: 0.4205729166666667, PRO_recall: 0.41893644617380027, PRO_f1: 0.41975308641975306, PRO_number: 771
SMT_precision: 0.34782608695652173, SMT_recall: 0.3742690058479532, SMT_f1: 0.3605633802816901, SMT_number: 171
SPL_precision: 0.43548387096774194, SPL_recall: 0.36, SPL_f1: 0.39416058394160586, SPL_number: 75
overall_precision: 0.45191146881287725, overall_recall: 0.4490203918432627, overall_f1: 0.45046129161652626, overall_accuracy: 0.8208848545493531
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 489, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:28:17 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:28:20 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:0 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1045.57it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3548.37 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:28:25 - INFO - __main__ - ***** Running training *****
05/31/2023 13:28:25 - INFO - __main__ -   Num examples = 72
05/31/2023 13:28:25 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:28:25 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:28:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:28:25 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:28:25 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.95297622680664
Total epoch: 1. epoch loss: 15.065642356872559
Total epoch: 2. epoch loss: 14.201330184936523
Total epoch: 3. epoch loss: 13.359106063842773
Total epoch: 4. epoch loss: 12.539778709411621
Total epoch: 5. epoch loss: 11.746102333068848
Total epoch: 6. epoch loss: 10.98221492767334
Total epoch: 7. epoch loss: 10.252525329589844
Total epoch: 8. epoch loss: 9.560745239257812
Total epoch: 9. epoch loss: 8.909355163574219
Total epoch: 10. epoch loss: 8.299623489379883
Total epoch: 11. epoch loss: 7.731710433959961
Total epoch: 12. epoch loss: 7.205089569091797
Total epoch: 13. epoch loss: 6.71868371963501
Total epoch: 14. epoch loss: 6.270904541015625
Total epoch: 15. epoch loss: 5.859638690948486
Total epoch: 16. epoch loss: 5.482051849365234
Total epoch: 17. epoch loss: 5.134805679321289
Total epoch: 18. epoch loss: 4.814335823059082
Total epoch: 19. epoch loss: 4.517580032348633
Total epoch: 20. epoch loss: 4.242900848388672
Total epoch: 21. epoch loss: 3.98881196975708
Total epoch: 22. epoch loss: 3.7539329528808594
Total epoch: 23. epoch loss: 3.5369513034820557
Total epoch: 24. epoch loss: 3.336660861968994
Total epoch: 25. epoch loss: 3.151972532272339
Total epoch: 26. epoch loss: 2.9818782806396484
Total epoch: 27. epoch loss: 2.82550311088562
Total epoch: 28. epoch loss: 2.681999444961548
Total epoch: 29. epoch loss: 2.5504565238952637
Total epoch: 30. epoch loss: 2.4299302101135254
Total epoch: 31. epoch loss: 2.319420099258423
Total epoch: 32. epoch loss: 2.2179269790649414
Total epoch: 33. epoch loss: 2.124497890472412
Total epoch: 34. epoch loss: 2.038266897201538
Total epoch: 35. epoch loss: 1.9584906101226807
Total epoch: 36. epoch loss: 1.8845149278640747
Total epoch: 37. epoch loss: 1.8157930374145508
Total epoch: 38. epoch loss: 1.7518422603607178
Total epoch: 39. epoch loss: 1.6922534704208374
Total epoch: 40. epoch loss: 1.636653184890747
Total epoch: 41. epoch loss: 1.584720253944397
Total epoch: 42. epoch loss: 1.5361638069152832
Total epoch: 43. epoch loss: 1.4907195568084717
Total epoch: 44. epoch loss: 1.44815194606781
Total epoch: 45. epoch loss: 1.4082307815551758
Total epoch: 46. epoch loss: 1.3707540035247803
Total epoch: 47. epoch loss: 1.335516095161438
Total epoch: 48. epoch loss: 1.3023351430892944
Total epoch: 49. epoch loss: 1.271034598350525
Total epoch: 50. epoch loss: 1.2414618730545044
Total epoch: 51. epoch loss: 1.2134801149368286
Total epoch: 52. epoch loss: 1.1869603395462036
Total epoch: 53. epoch loss: 1.161791205406189
Total epoch: 54. epoch loss: 1.1378848552703857
Total epoch: 55. epoch loss: 1.115148901939392
Total epoch: 56. epoch loss: 1.0935096740722656
Total epoch: 57. epoch loss: 1.0728901624679565
Total epoch: 58. epoch loss: 1.0532270669937134
Total epoch: 59. epoch loss: 1.0344616174697876
Total epoch: 60. epoch loss: 1.0165399312973022
Total epoch: 61. epoch loss: 0.9994085431098938
Total epoch: 62. epoch loss: 0.9830198287963867
Total epoch: 63. epoch loss: 0.9673289656639099
Total epoch: 64. epoch loss: 0.9522896409034729
Total epoch: 65. epoch loss: 0.9378581643104553
Total epoch: 66. epoch loss: 0.9240015745162964
Total epoch: 67. epoch loss: 0.91068035364151
Total epoch: 68. epoch loss: 0.8978595733642578
Total epoch: 69. epoch loss: 0.8855159878730774
Total epoch: 70. epoch loss: 0.873616635799408
Total epoch: 71. epoch loss: 0.8621359467506409
Total epoch: 72. epoch loss: 0.8510509729385376
Total epoch: 73. epoch loss: 0.8403376340866089
Total epoch: 74. epoch loss: 0.8299865126609802
Total epoch: 75. epoch loss: 0.8199625611305237
Total epoch: 76. epoch loss: 0.8102593421936035
Total epoch: 77. epoch loss: 0.8008584380149841
Total epoch: 78. epoch loss: 0.7917425632476807
Total epoch: 79. epoch loss: 0.7828916907310486
Total epoch: 80. epoch loss: 0.7742984890937805
Total epoch: 81. epoch loss: 0.7659516334533691
Total epoch: 82. epoch loss: 0.7578380703926086
Total epoch: 83. epoch loss: 0.7499400973320007
Total epoch: 84. epoch loss: 0.7422547340393066
Total epoch: 85. epoch loss: 0.7347723841667175
Total epoch: 86. epoch loss: 0.7274799346923828
Total epoch: 87. epoch loss: 0.720371425151825
Total epoch: 88. epoch loss: 0.7134405970573425
Total epoch: 89. epoch loss: 0.7066774964332581
Total epoch: 90. epoch loss: 0.7000725865364075
Total epoch: 91. epoch loss: 0.6936288475990295
Total epoch: 92. epoch loss: 0.6873339414596558
Total epoch: 93. epoch loss: 0.681178629398346
Total epoch: 94. epoch loss: 0.6751645803451538
Total epoch: 95. epoch loss: 0.6692790985107422
Total epoch: 96. epoch loss: 0.6635236144065857
Total epoch: 97. epoch loss: 0.6578920483589172
Total epoch: 98. epoch loss: 0.6523808836936951
Total epoch: 99. epoch loss: 0.6469764113426208
Total epoch: 99. DecT loss: 0.6469764113426208
Training time: 0.7547554969787598
APL_precision: 0.2754098360655738, APL_recall: 0.49411764705882355, APL_f1: 0.3536842105263158, APL_number: 170
CMT_precision: 0.5495049504950495, CMT_recall: 0.5692307692307692, CMT_f1: 0.5591939546599497, CMT_number: 195
DSC_precision: 0.45318352059925093, DSC_recall: 0.5537757437070938, DSC_f1: 0.4984552008238929, DSC_number: 437
MAT_precision: 0.5035294117647059, MAT_recall: 0.6275659824046921, MAT_f1: 0.5587467362924282, MAT_number: 682
PRO_precision: 0.4258210645526614, PRO_recall: 0.4876783398184176, PRO_f1: 0.4546553808948005, PRO_number: 771
SMT_precision: 0.36885245901639346, SMT_recall: 0.5263157894736842, SMT_f1: 0.43373493975903615, SMT_number: 171
SPL_precision: 0.3106796116504854, SPL_recall: 0.4266666666666667, SPL_f1: 0.3595505617977528, SPL_number: 75
overall_precision: 0.4367190003204101, overall_recall: 0.5449820071971212, overall_f1: 0.4848808253290644, overall_accuracy: 0.8322493031234365
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:30:35 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:30:37 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1224.97it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4961.51 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:30:54 - INFO - __main__ - ***** Running training *****
05/31/2023 13:30:54 - INFO - __main__ -   Num examples = 72
05/31/2023 13:30:54 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:30:54 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:30:54 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:30:54 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:30:54 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.95297622680664
Total epoch: 1. epoch loss: 15.065643310546875
Total epoch: 2. epoch loss: 14.20133113861084
Total epoch: 3. epoch loss: 13.359107971191406
Total epoch: 4. epoch loss: 12.539776802062988
Total epoch: 5. epoch loss: 11.746097564697266
Total epoch: 6. epoch loss: 10.982207298278809
Total epoch: 7. epoch loss: 10.252513885498047
Total epoch: 8. epoch loss: 9.560726165771484
Total epoch: 9. epoch loss: 8.909330368041992
Total epoch: 10. epoch loss: 8.299599647521973
Total epoch: 11. epoch loss: 7.731672286987305
Total epoch: 12. epoch loss: 7.205049991607666
Total epoch: 13. epoch loss: 6.718638896942139
Total epoch: 14. epoch loss: 6.270860195159912
Total epoch: 15. epoch loss: 5.859574794769287
Total epoch: 16. epoch loss: 5.481991291046143
Total epoch: 17. epoch loss: 5.134730339050293
Total epoch: 18. epoch loss: 4.814243316650391
Total epoch: 19. epoch loss: 4.517457485198975
Total epoch: 20. epoch loss: 4.242752552032471
Total epoch: 21. epoch loss: 3.9886441230773926
Total epoch: 22. epoch loss: 3.7537546157836914
Total epoch: 23. epoch loss: 3.5367584228515625
Total epoch: 24. epoch loss: 3.3364737033843994
Total epoch: 25. epoch loss: 3.1517810821533203
Total epoch: 26. epoch loss: 2.981693744659424
Total epoch: 27. epoch loss: 2.8253495693206787
Total epoch: 28. epoch loss: 2.681877613067627
Total epoch: 29. epoch loss: 2.5503692626953125
Total epoch: 30. epoch loss: 2.4298787117004395
Total epoch: 31. epoch loss: 2.3194096088409424
Total epoch: 32. epoch loss: 2.217958688735962
Total epoch: 33. epoch loss: 2.12457275390625
Total epoch: 34. epoch loss: 2.0383799076080322
Total epoch: 35. epoch loss: 1.9586389064788818
Total epoch: 36. epoch loss: 1.8847002983093262
Total epoch: 37. epoch loss: 1.8160160779953003
Total epoch: 38. epoch loss: 1.7521084547042847
Total epoch: 39. epoch loss: 1.692556381225586
Total epoch: 40. epoch loss: 1.6370049715042114
Total epoch: 41. epoch loss: 1.585111379623413
Total epoch: 42. epoch loss: 1.5365993976593018
Total epoch: 43. epoch loss: 1.4911980628967285
Total epoch: 44. epoch loss: 1.4486761093139648
Total epoch: 45. epoch loss: 1.408807396888733
Total epoch: 46. epoch loss: 1.3713762760162354
Total epoch: 47. epoch loss: 1.3361917734146118
Total epoch: 48. epoch loss: 1.3030571937561035
Total epoch: 49. epoch loss: 1.27181077003479
Total epoch: 50. epoch loss: 1.2422884702682495
Total epoch: 51. epoch loss: 1.214350700378418
Total epoch: 52. epoch loss: 1.1878780126571655
Total epoch: 53. epoch loss: 1.1627660989761353
Total epoch: 54. epoch loss: 1.1389081478118896
Total epoch: 55. epoch loss: 1.116222858428955
Total epoch: 56. epoch loss: 1.0946301221847534
Total epoch: 57. epoch loss: 1.0740594863891602
Total epoch: 58. epoch loss: 1.054453730583191
Total epoch: 59. epoch loss: 1.0357404947280884
Total epoch: 60. epoch loss: 1.017871618270874
Total epoch: 61. epoch loss: 1.0007967948913574
Total epoch: 62. epoch loss: 0.9844582080841064
Total epoch: 63. epoch loss: 0.9688218832015991
Total epoch: 64. epoch loss: 0.9538332223892212
Total epoch: 65. epoch loss: 0.9394528865814209
Total epoch: 66. epoch loss: 0.925646960735321
Total epoch: 67. epoch loss: 0.9123795032501221
Total epoch: 68. epoch loss: 0.8996107578277588
Total epoch: 69. epoch loss: 0.8873165845870972
Total epoch: 70. epoch loss: 0.8754669427871704
Total epoch: 71. epoch loss: 0.8640367388725281
Total epoch: 72. epoch loss: 0.853003203868866
Total epoch: 73. epoch loss: 0.8423424959182739
Total epoch: 74. epoch loss: 0.8320364356040955
Total epoch: 75. epoch loss: 0.8220664262771606
Total epoch: 76. epoch loss: 0.8124119639396667
Total epoch: 77. epoch loss: 0.8030561208724976
Total epoch: 78. epoch loss: 0.7939860820770264
Total epoch: 79. epoch loss: 0.7851872444152832
Total epoch: 80. epoch loss: 0.7766437530517578
Total epoch: 81. epoch loss: 0.7683424353599548
Total epoch: 82. epoch loss: 0.7602723240852356
Total epoch: 83. epoch loss: 0.7524257898330688
Total epoch: 84. epoch loss: 0.7447873950004578
Total epoch: 85. epoch loss: 0.7373493909835815
Total epoch: 86. epoch loss: 0.7301058173179626
Total epoch: 87. epoch loss: 0.7230405211448669
Total epoch: 88. epoch loss: 0.7161540389060974
Total epoch: 89. epoch loss: 0.7094340324401855
Total epoch: 90. epoch loss: 0.7028783559799194
Total epoch: 91. epoch loss: 0.6964767575263977
Total epoch: 92. epoch loss: 0.6902227997779846
Total epoch: 93. epoch loss: 0.6841126680374146
Total epoch: 94. epoch loss: 0.6781381368637085
Total epoch: 95. epoch loss: 0.6722975969314575
Total epoch: 96. epoch loss: 0.666585385799408
Total epoch: 97. epoch loss: 0.6609975099563599
Total epoch: 98. epoch loss: 0.6555216312408447
Total epoch: 99. epoch loss: 0.6501638293266296
Total epoch: 99. DecT loss: 0.6501638293266296
Training time: 0.7437481880187988
APL_precision: 0.27631578947368424, APL_recall: 0.49411764705882355, APL_f1: 0.35443037974683544, APL_number: 170
CMT_precision: 0.5472636815920398, CMT_recall: 0.5641025641025641, CMT_f1: 0.5555555555555556, CMT_number: 195
DSC_precision: 0.45318352059925093, DSC_recall: 0.5537757437070938, DSC_f1: 0.4984552008238929, DSC_number: 437
MAT_precision: 0.5029377203290247, MAT_recall: 0.6275659824046921, MAT_f1: 0.5583822570123941, MAT_number: 682
PRO_precision: 0.4253393665158371, PRO_recall: 0.4876783398184176, PRO_f1: 0.45438066465256793, PRO_number: 771
SMT_precision: 0.36585365853658536, SMT_recall: 0.5263157894736842, SMT_f1: 0.4316546762589928, SMT_number: 171
SPL_precision: 0.3106796116504854, SPL_recall: 0.4266666666666667, SPL_f1: 0.3595505617977528, SPL_number: 75
overall_precision: 0.43611911623439004, overall_recall: 0.5445821671331468, overall_f1: 0.4843527738264581, overall_accuracy: 0.8322493031234365
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:33:27 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:33:28 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1075.46it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
05/31/2023 13:33:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3ecfe395a9f59f7e.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4751.31 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:33:35 - INFO - __main__ - ***** Running training *****
05/31/2023 13:33:35 - INFO - __main__ -   Num examples = 72
05/31/2023 13:33:35 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:33:35 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:33:35 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:33:35 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:33:35 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.952969551086426
Total epoch: 1. epoch loss: 15.073670387268066
Total epoch: 2. epoch loss: 14.231972694396973
Total epoch: 3. epoch loss: 13.42617416381836
Total epoch: 4. epoch loss: 12.655532836914062
Total epoch: 5. epoch loss: 11.920158386230469
Total epoch: 6. epoch loss: 11.220683097839355
Total epoch: 7. epoch loss: 10.5577974319458
Total epoch: 8. epoch loss: 9.931793212890625
Total epoch: 9. epoch loss: 9.342336654663086
Total epoch: 10. epoch loss: 8.788440704345703
Total epoch: 11. epoch loss: 8.268587112426758
Total epoch: 12. epoch loss: 7.780986309051514
Total epoch: 13. epoch loss: 7.323824405670166
Total epoch: 14. epoch loss: 6.8952531814575195
Total epoch: 15. epoch loss: 6.493537902832031
Total epoch: 16. epoch loss: 6.116968631744385
Total epoch: 17. epoch loss: 5.763822078704834
Total epoch: 18. epoch loss: 5.432408332824707
Total epoch: 19. epoch loss: 5.121667385101318
Total epoch: 20. epoch loss: 4.830950736999512
Total epoch: 21. epoch loss: 4.559447288513184
Total epoch: 22. epoch loss: 4.306224822998047
Total epoch: 23. epoch loss: 4.070308685302734
Total epoch: 24. epoch loss: 3.8506805896759033
Total epoch: 25. epoch loss: 3.646305561065674
Total epoch: 26. epoch loss: 3.4561479091644287
Total epoch: 27. epoch loss: 3.2792093753814697
Total epoch: 28. epoch loss: 3.114591121673584
Total epoch: 29. epoch loss: 2.9614105224609375
Total epoch: 30. epoch loss: 2.818866491317749
Total epoch: 31. epoch loss: 2.686220645904541
Total epoch: 32. epoch loss: 2.562744140625
Total epoch: 33. epoch loss: 2.4477477073669434
Total epoch: 34. epoch loss: 2.3405752182006836
Total epoch: 35. epoch loss: 2.240612030029297
Total epoch: 36. epoch loss: 2.14729905128479
Total epoch: 37. epoch loss: 2.0601000785827637
Total epoch: 38. epoch loss: 1.978531837463379
Total epoch: 39. epoch loss: 1.9021644592285156
Total epoch: 40. epoch loss: 1.8305895328521729
Total epoch: 41. epoch loss: 1.7634508609771729
Total epoch: 42. epoch loss: 1.7004121541976929
Total epoch: 43. epoch loss: 1.6411597728729248
Total epoch: 44. epoch loss: 1.5854172706604004
Total epoch: 45. epoch loss: 1.532949686050415
Total epoch: 46. epoch loss: 1.4834944009780884
Total epoch: 47. epoch loss: 1.4368590116500854
Total epoch: 48. epoch loss: 1.3928416967391968
Total epoch: 49. epoch loss: 1.3512630462646484
Total epoch: 50. epoch loss: 1.311953067779541
Total epoch: 51. epoch loss: 1.2747540473937988
Total epoch: 52. epoch loss: 1.2395268678665161
Total epoch: 53. epoch loss: 1.2061333656311035
Total epoch: 54. epoch loss: 1.1744506359100342
Total epoch: 55. epoch loss: 1.14436674118042
Total epoch: 56. epoch loss: 1.1157753467559814
Total epoch: 57. epoch loss: 1.0885661840438843
Total epoch: 58. epoch loss: 1.0626578330993652
Total epoch: 59. epoch loss: 1.0379654169082642
Total epoch: 60. epoch loss: 1.0144100189208984
Total epoch: 61. epoch loss: 0.9919171929359436
Total epoch: 62. epoch loss: 0.9704172611236572
Total epoch: 63. epoch loss: 0.9498526453971863
Total epoch: 64. epoch loss: 0.9301618933677673
Total epoch: 65. epoch loss: 0.9112966656684875
Total epoch: 66. epoch loss: 0.8932039737701416
Total epoch: 67. epoch loss: 0.8758350014686584
Total epoch: 68. epoch loss: 0.8591524362564087
Total epoch: 69. epoch loss: 0.8431122899055481
Total epoch: 70. epoch loss: 0.8276838064193726
Total epoch: 71. epoch loss: 0.8128204941749573
Total epoch: 72. epoch loss: 0.798504114151001
Total epoch: 73. epoch loss: 0.7846920490264893
Total epoch: 74. epoch loss: 0.7713683843612671
Total epoch: 75. epoch loss: 0.7585003972053528
Total epoch: 76. epoch loss: 0.7460652589797974
Total epoch: 77. epoch loss: 0.7340443134307861
Total epoch: 78. epoch loss: 0.7224134206771851
Total epoch: 79. epoch loss: 0.7111505270004272
Total epoch: 80. epoch loss: 0.7002452611923218
Total epoch: 81. epoch loss: 0.6896687746047974
Total epoch: 82. epoch loss: 0.6794127821922302
Total epoch: 83. epoch loss: 0.6694609522819519
Total epoch: 84. epoch loss: 0.6597984433174133
Total epoch: 85. epoch loss: 0.6504168510437012
Total epoch: 86. epoch loss: 0.6412955522537231
Total epoch: 87. epoch loss: 0.6324269771575928
Total epoch: 88. epoch loss: 0.6238015294075012
Total epoch: 89. epoch loss: 0.6154049038887024
Total epoch: 90. epoch loss: 0.6072306632995605
Total epoch: 91. epoch loss: 0.5992664098739624
Total epoch: 92. epoch loss: 0.5915077328681946
Total epoch: 93. epoch loss: 0.583945095539093
Total epoch: 94. epoch loss: 0.5765640735626221
Total epoch: 95. epoch loss: 0.5693656802177429
Total epoch: 96. epoch loss: 0.5623435378074646
Total epoch: 97. epoch loss: 0.5554807782173157
Total epoch: 98. epoch loss: 0.5487812161445618
Total epoch: 99. epoch loss: 0.5422356724739075
Total epoch: 99. DecT loss: 0.5422356724739075
Training time: 0.6777071952819824
APL_precision: 0.2814814814814815, APL_recall: 0.4470588235294118, APL_f1: 0.34545454545454546, APL_number: 170
CMT_precision: 0.5561224489795918, CMT_recall: 0.558974358974359, CMT_f1: 0.5575447570332481, CMT_number: 195
DSC_precision: 0.4291044776119403, DSC_recall: 0.5263157894736842, DSC_f1: 0.4727646454265159, DSC_number: 437
MAT_precision: 0.49110320284697506, MAT_recall: 0.6070381231671554, MAT_f1: 0.5429508196721312, MAT_number: 682
PRO_precision: 0.4201388888888889, PRO_recall: 0.4708171206225681, PRO_f1: 0.4440366972477064, PRO_number: 771
SMT_precision: 0.36283185840707965, SMT_recall: 0.47953216374269003, SMT_f1: 0.4130982367758186, SMT_number: 171
SPL_precision: 0.36363636363636365, SPL_recall: 0.4266666666666667, SPL_f1: 0.392638036809816, SPL_number: 75
overall_precision: 0.4320211710221634, overall_recall: 0.5221911235505797, overall_f1: 0.4728457639391745, overall_accuracy: 0.8296762204274176
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:35:15 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:35:16 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 991.68it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
05/31/2023 13:35:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3ecfe395a9f59f7e.arrow
05/31/2023 13:35:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-51ecaca92754e070.arrow
/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:35:23 - INFO - __main__ - ***** Running training *****
05/31/2023 13:35:23 - INFO - __main__ -   Num examples = 72
05/31/2023 13:35:23 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:35:23 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:35:23 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:35:23 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:35:23 - INFO - __main__ -   Total optimization steps = 300
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.801687240600586
Total epoch: 1. epoch loss: 14.91892147064209
Total epoch: 2. epoch loss: 14.059684753417969
Total epoch: 3. epoch loss: 13.222692489624023
Total epoch: 4. epoch loss: 12.408499717712402
Total epoch: 5. epoch loss: 11.619873046875
Total epoch: 6. epoch loss: 10.861096382141113
Total epoch: 7. epoch loss: 10.13679027557373
Total epoch: 8. epoch loss: 9.450789451599121
Total epoch: 9. epoch loss: 8.805652618408203
Total epoch: 10. epoch loss: 8.202534675598145
Total epoch: 11. epoch loss: 7.641463279724121
Total epoch: 12. epoch loss: 7.121654510498047
Total epoch: 13. epoch loss: 6.641798973083496
Total epoch: 14. epoch loss: 6.20014762878418
Total epoch: 15. epoch loss: 5.794482231140137
Total epoch: 16. epoch loss: 5.422018527984619
Total epoch: 17. epoch loss: 5.079461097717285
Total epoch: 18. epoch loss: 4.763293266296387
Total epoch: 19. epoch loss: 4.470715522766113
Total epoch: 20. epoch loss: 4.2000017166137695
Total epoch: 21. epoch loss: 3.9495904445648193
Total epoch: 22. epoch loss: 3.718097448348999
Total epoch: 23. epoch loss: 3.504260301589966
Total epoch: 24. epoch loss: 3.306896686553955
Total epoch: 25. epoch loss: 3.124937057495117
Total epoch: 26. epoch loss: 2.9573681354522705
Total epoch: 27. epoch loss: 2.803318738937378
Total epoch: 28. epoch loss: 2.661878824234009
Total epoch: 29. epoch loss: 2.5321767330169678
Total epoch: 30. epoch loss: 2.4132893085479736
Total epoch: 31. epoch loss: 2.3042428493499756
Total epoch: 32. epoch loss: 2.204057216644287
Total epoch: 33. epoch loss: 2.1118009090423584
Total epoch: 34. epoch loss: 2.0266175270080566
Total epoch: 35. epoch loss: 1.9477750062942505
Total epoch: 36. epoch loss: 1.8746272325515747
Total epoch: 37. epoch loss: 1.8066462278366089
Total epoch: 38. epoch loss: 1.7433483600616455
Total epoch: 39. epoch loss: 1.6843448877334595
Total epoch: 40. epoch loss: 1.6292712688446045
Total epoch: 41. epoch loss: 1.577811598777771
Total epoch: 42. epoch loss: 1.5296789407730103
Total epoch: 43. epoch loss: 1.4846144914627075
Total epoch: 44. epoch loss: 1.4423867464065552
Total epoch: 45. epoch loss: 1.4027795791625977
Total epoch: 46. epoch loss: 1.365587830543518
Total epoch: 47. epoch loss: 1.330610990524292
Total epoch: 48. epoch loss: 1.2976737022399902
Total epoch: 49. epoch loss: 1.2666094303131104
Total epoch: 50. epoch loss: 1.23725426197052
Total epoch: 51. epoch loss: 1.209482192993164
Total epoch: 52. epoch loss: 1.183157205581665
Total epoch: 53. epoch loss: 1.1581807136535645
Total epoch: 54. epoch loss: 1.1344496011734009
Total epoch: 55. epoch loss: 1.1118865013122559
Total epoch: 56. epoch loss: 1.0904139280319214
Total epoch: 57. epoch loss: 1.0699594020843506
Total epoch: 58. epoch loss: 1.0504498481750488
Total epoch: 59. epoch loss: 1.031837821006775
Total epoch: 60. epoch loss: 1.0140594244003296
Total epoch: 61. epoch loss: 0.9970671534538269
Total epoch: 62. epoch loss: 0.9808129668235779
Total epoch: 63. epoch loss: 0.9652494788169861
Total epoch: 64. epoch loss: 0.9503347277641296
Total epoch: 65. epoch loss: 0.9360233545303345
Total epoch: 66. epoch loss: 0.9222790598869324
Total epoch: 67. epoch loss: 0.9090708494186401
Total epoch: 68. epoch loss: 0.8963530659675598
Total epoch: 69. epoch loss: 0.8841116428375244
Total epoch: 70. epoch loss: 0.8723112940788269
Total epoch: 71. epoch loss: 0.86092609167099
Total epoch: 72. epoch loss: 0.8499308824539185
Total epoch: 73. epoch loss: 0.8393094539642334
Total epoch: 74. epoch loss: 0.8290391564369202
Total epoch: 75. epoch loss: 0.8191021680831909
Total epoch: 76. epoch loss: 0.809481143951416
Total epoch: 77. epoch loss: 0.8001590967178345
Total epoch: 78. epoch loss: 0.7911143898963928
Total epoch: 79. epoch loss: 0.7823417782783508
Total epoch: 80. epoch loss: 0.7738217711448669
Total epoch: 81. epoch loss: 0.7655439376831055
Total epoch: 82. epoch loss: 0.7574994564056396
Total epoch: 83. epoch loss: 0.7496697306632996
Total epoch: 84. epoch loss: 0.742050290107727
Total epoch: 85. epoch loss: 0.7346305847167969
Total epoch: 86. epoch loss: 0.7273991703987122
Total epoch: 87. epoch loss: 0.720354437828064
Total epoch: 88. epoch loss: 0.7134822607040405
Total epoch: 89. epoch loss: 0.7067806124687195
Total epoch: 90. epoch loss: 0.7002376317977905
Total epoch: 91. epoch loss: 0.6938462853431702
Total epoch: 92. epoch loss: 0.6876078248023987
Total epoch: 93. epoch loss: 0.681508481502533
Total epoch: 94. epoch loss: 0.6755456924438477
Total epoch: 95. epoch loss: 0.6697161793708801
Total epoch: 96. epoch loss: 0.6640109419822693
Total epoch: 97. epoch loss: 0.6584339141845703
Total epoch: 98. epoch loss: 0.6529662013053894
Total epoch: 99. epoch loss: 0.6476160883903503
Total epoch: 99. DecT loss: 0.6476160883903503
Training time: 0.7035276889801025
APL_precision: 0.27450980392156865, APL_recall: 0.49411764705882355, APL_f1: 0.35294117647058826, APL_number: 170
CMT_precision: 0.5522388059701493, CMT_recall: 0.5692307692307692, CMT_f1: 0.5606060606060606, CMT_number: 195
DSC_precision: 0.45387453874538747, DSC_recall: 0.562929061784897, DSC_f1: 0.5025536261491317, DSC_number: 437
MAT_precision: 0.5035545023696683, MAT_recall: 0.6231671554252199, MAT_f1: 0.5570117955439057, MAT_number: 682
PRO_precision: 0.42777155655095184, PRO_recall: 0.49546044098573283, PRO_f1: 0.45913461538461536, PRO_number: 771
SMT_precision: 0.34782608695652173, SMT_recall: 0.5146198830409356, SMT_f1: 0.41509433962264153, SMT_number: 171
SPL_precision: 0.3173076923076923, SPL_recall: 0.44, SPL_f1: 0.36871508379888274, SPL_number: 75
overall_precision: 0.43557111040407254, overall_recall: 0.5473810475809676, overall_f1: 0.4851169383416017, overall_accuracy: 0.8323207776427703
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]