/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:34:28 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:34:29 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1242.02it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:34:56 - INFO - __main__ - ***** Running training *****
05/30/2023 12:34:56 - INFO - __main__ -   Num examples = 64
05/30/2023 12:34:56 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:34:56 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:34:56 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:34:56 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:34:56 - INFO - __main__ -   Total optimization steps = 70
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/70 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.838404655456543
Total epoch: 1. epoch loss: 15.247427940368652
Total epoch: 2. epoch loss: 14.667569160461426
Total epoch: 3. epoch loss: 14.099339485168457
Total epoch: 4. epoch loss: 13.54318618774414
Total epoch: 5. epoch loss: 12.999812126159668
Total epoch: 6. epoch loss: 12.470027923583984
Total epoch: 7. epoch loss: 11.95463752746582
Total epoch: 8. epoch loss: 11.454216003417969
Total epoch: 9. epoch loss: 10.969161033630371
Total epoch: 10. epoch loss: 10.499641418457031
Total epoch: 11. epoch loss: 10.045669555664062
Total epoch: 12. epoch loss: 9.607166290283203
Total epoch: 13. epoch loss: 9.184075355529785
Total epoch: 14. epoch loss: 8.776342391967773
Total epoch: 15. epoch loss: 8.383992195129395
Total epoch: 16. epoch loss: 8.007057189941406
Total epoch: 17. epoch loss: 7.645534992218018
Total epoch: 18. epoch loss: 7.29940128326416
Total epoch: 19. epoch loss: 6.968607425689697
Total epoch: 20. epoch loss: 6.652935028076172
Total epoch: 21. epoch loss: 6.352174758911133
Total epoch: 22. epoch loss: 6.065969944000244
Total epoch: 23. epoch loss: 5.7938642501831055
Total epoch: 24. epoch loss: 5.535304546356201
Total epoch: 25. epoch loss: 5.289700984954834
Total epoch: 26. epoch loss: 5.056332588195801
Total epoch: 27. epoch loss: 4.8345136642456055
Total epoch: 28. epoch loss: 4.623615264892578
Total epoch: 29. epoch loss: 4.423182010650635
Total epoch: 30. epoch loss: 4.233170509338379
Total epoch: 31. epoch loss: 4.053380489349365
Total epoch: 32. epoch loss: 3.8834660053253174
Total epoch: 33. epoch loss: 3.7230727672576904
Total epoch: 34. epoch loss: 3.5717649459838867
Total epoch: 34. DecT loss: 3.5717649459838867
Training time: 0.2553095817565918
APL_precision: 0.19727891156462585, APL_recall: 0.3411764705882353, APL_f1: 0.25, APL_number: 170
CMT_precision: 0.25157232704402516, CMT_recall: 0.41025641025641024, CMT_f1: 0.31189083820662766, CMT_number: 195
DSC_precision: 0.2985257985257985, DSC_recall: 0.5560640732265446, DSC_f1: 0.3884892086330935, DSC_number: 437
MAT_precision: 0.46906636670416196, MAT_recall: 0.6114369501466276, MAT_f1: 0.5308720560152769, MAT_number: 682
PRO_precision: 0.35063663075416257, PRO_recall: 0.46433203631647213, PRO_f1: 0.39955357142857145, PRO_number: 771
SMT_precision: 0.27577937649880097, SMT_recall: 0.672514619883041, SMT_f1: 0.391156462585034, SMT_number: 171
SPL_precision: 0.3008849557522124, SPL_recall: 0.4533333333333333, SPL_f1: 0.36170212765957444, SPL_number: 75
overall_precision: 0.33755819968960166, overall_recall: 0.5217912834866053, overall_f1: 0.4099261818752945, overall_accuracy: 0.7858623400757629
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/70 [00:01<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:37:30 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:37:31 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:128, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1170.29it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4332.16 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:37:38 - INFO - __main__ - ***** Running training *****
05/30/2023 12:37:38 - INFO - __main__ -   Num examples = 64
05/30/2023 12:37:38 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:37:38 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:37:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:37:38 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:37:38 - INFO - __main__ -   Total optimization steps = 200
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.842459678649902
Total epoch: 1. epoch loss: 15.015201568603516
Total epoch: 2. epoch loss: 14.210083961486816
Total epoch: 3. epoch loss: 13.429703712463379
Total epoch: 4. epoch loss: 12.676666259765625
Total epoch: 5. epoch loss: 11.95318603515625
Total epoch: 6. epoch loss: 11.260713577270508
Total epoch: 7. epoch loss: 10.599627494812012
Total epoch: 8. epoch loss: 9.969576835632324
Total epoch: 9. epoch loss: 9.370033264160156
Total epoch: 10. epoch loss: 8.800684928894043
Total epoch: 11. epoch loss: 8.26164436340332
Total epoch: 12. epoch loss: 7.753263473510742
Total epoch: 13. epoch loss: 7.27569055557251
Total epoch: 14. epoch loss: 6.828767776489258
Total epoch: 15. epoch loss: 6.411768436431885
Total epoch: 16. epoch loss: 6.0235066413879395
Total epoch: 17. epoch loss: 5.662456512451172
Total epoch: 18. epoch loss: 5.326841354370117
Total epoch: 19. epoch loss: 5.014738082885742
Total epoch: 20. epoch loss: 4.724166393280029
Total epoch: 21. epoch loss: 4.453286170959473
Total epoch: 22. epoch loss: 4.201262474060059
Total epoch: 23. epoch loss: 3.967076063156128
Total epoch: 24. epoch loss: 3.749631404876709
Total epoch: 25. epoch loss: 3.5478408336639404
Total epoch: 26. epoch loss: 3.3607280254364014
Total epoch: 27. epoch loss: 3.1873042583465576
Total epoch: 28. epoch loss: 3.0266621112823486
Total epoch: 29. epoch loss: 2.877941608428955
Total epoch: 30. epoch loss: 2.74031400680542
Total epoch: 31. epoch loss: 2.612994432449341
Total epoch: 32. epoch loss: 2.4952080249786377
Total epoch: 33. epoch loss: 2.386202096939087
Total epoch: 34. epoch loss: 2.285223960876465
Total epoch: 35. epoch loss: 2.191584825515747
Total epoch: 36. epoch loss: 2.1046130657196045
Total epoch: 37. epoch loss: 2.0237021446228027
Total epoch: 38. epoch loss: 1.9483168125152588
Total epoch: 39. epoch loss: 1.8779679536819458
Total epoch: 40. epoch loss: 1.812252402305603
Total epoch: 41. epoch loss: 1.7508217096328735
Total epoch: 42. epoch loss: 1.6933709383010864
Total epoch: 43. epoch loss: 1.6395986080169678
Total epoch: 44. epoch loss: 1.5892748832702637
Total epoch: 45. epoch loss: 1.5421435832977295
Total epoch: 46. epoch loss: 1.4979846477508545
Total epoch: 47. epoch loss: 1.4565668106079102
Total epoch: 48. epoch loss: 1.4176918268203735
Total epoch: 49. epoch loss: 1.3811602592468262
Total epoch: 50. epoch loss: 1.3467779159545898
Total epoch: 51. epoch loss: 1.3143848180770874
Total epoch: 52. epoch loss: 1.2838160991668701
Total epoch: 53. epoch loss: 1.254919409751892
Total epoch: 54. epoch loss: 1.227570652961731
Total epoch: 55. epoch loss: 1.2016414403915405
Total epoch: 56. epoch loss: 1.1770193576812744
Total epoch: 57. epoch loss: 1.1536189317703247
Total epoch: 58. epoch loss: 1.13132905960083
Total epoch: 59. epoch loss: 1.1100765466690063
Total epoch: 60. epoch loss: 1.0897998809814453
Total epoch: 61. epoch loss: 1.0704222917556763
Total epoch: 62. epoch loss: 1.051889181137085
Total epoch: 63. epoch loss: 1.0341354608535767
Total epoch: 64. epoch loss: 1.0171217918395996
Total epoch: 65. epoch loss: 1.000795602798462
Total epoch: 66. epoch loss: 0.9851158857345581
Total epoch: 67. epoch loss: 0.9700452089309692
Total epoch: 68. epoch loss: 0.9555472135543823
Total epoch: 69. epoch loss: 0.9415791630744934
Total epoch: 70. epoch loss: 0.9281261563301086
Total epoch: 71. epoch loss: 0.9151484370231628
Total epoch: 72. epoch loss: 0.902626097202301
Total epoch: 73. epoch loss: 0.890525221824646
Total epoch: 74. epoch loss: 0.878834068775177
Total epoch: 75. epoch loss: 0.8675270676612854
Total epoch: 76. epoch loss: 0.8565848469734192
Total epoch: 77. epoch loss: 0.8459856510162354
Total epoch: 78. epoch loss: 0.8357192277908325
Total epoch: 79. epoch loss: 0.8257567882537842
Total epoch: 80. epoch loss: 0.8160946369171143
Total epoch: 81. epoch loss: 0.8067173361778259
Total epoch: 82. epoch loss: 0.7976007461547852
Total epoch: 83. epoch loss: 0.7887412309646606
Total epoch: 84. epoch loss: 0.7801226377487183
Total epoch: 85. epoch loss: 0.7717393636703491
Total epoch: 86. epoch loss: 0.7635743618011475
Total epoch: 87. epoch loss: 0.7556207180023193
Total epoch: 88. epoch loss: 0.7478716969490051
Total epoch: 89. epoch loss: 0.740320086479187
Total epoch: 90. epoch loss: 0.732950747013092
Total epoch: 91. epoch loss: 0.7257627844810486
Total epoch: 92. epoch loss: 0.7187463641166687
Total epoch: 93. epoch loss: 0.7118955254554749
Total epoch: 94. epoch loss: 0.7052032351493835
Total epoch: 95. epoch loss: 0.6986637115478516
Total epoch: 96. epoch loss: 0.6922719478607178
Total epoch: 97. epoch loss: 0.686021089553833
Total epoch: 98. epoch loss: 0.6799048185348511
Total epoch: 99. epoch loss: 0.6739218831062317
Total epoch: 99. DecT loss: 0.6739218831062317
Training time: 0.5767314434051514
APL_precision: 0.28793774319066145, APL_recall: 0.43529411764705883, APL_f1: 0.34660421545667447, APL_number: 170
CMT_precision: 0.38408304498269896, CMT_recall: 0.5692307692307692, CMT_f1: 0.45867768595041325, CMT_number: 195
DSC_precision: 0.4438860971524288, DSC_recall: 0.6064073226544623, DSC_f1: 0.5125725338491296, DSC_number: 437
MAT_precision: 0.5620052770448549, MAT_recall: 0.624633431085044, MAT_f1: 0.5916666666666666, MAT_number: 682
PRO_precision: 0.41728395061728396, PRO_recall: 0.4383916990920882, PRO_f1: 0.42757748260594564, PRO_number: 771
SMT_precision: 0.3217665615141956, SMT_recall: 0.5964912280701754, SMT_f1: 0.4180327868852459, SMT_number: 171
SPL_precision: 0.40540540540540543, SPL_recall: 0.4, SPL_f1: 0.4026845637583893, SPL_number: 75
overall_precision: 0.4339136041263701, overall_recall: 0.5381847261095561, overall_f1: 0.4804568980903087, overall_accuracy: 0.829461796869416
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:41:02 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:41:04 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1096.12it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:41:10 - INFO - __main__ - ***** Running training *****
05/30/2023 12:41:10 - INFO - __main__ -   Num examples = 64
05/30/2023 12:41:10 - INFO - __main__ -   Num Epochs = 150
05/30/2023 12:41:10 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:41:10 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:41:10 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:41:10 - INFO - __main__ -   Total optimization steps = 300
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 15.802309036254883
Total epoch: 1. epoch loss: 14.882513046264648
Total epoch: 2. epoch loss: 13.988947868347168
Total epoch: 3. epoch loss: 13.125557899475098
Total epoch: 4. epoch loss: 12.296527862548828
Total epoch: 5. epoch loss: 11.505143165588379
Total epoch: 6. epoch loss: 10.753244400024414
Total epoch: 7. epoch loss: 10.041120529174805
Total epoch: 8. epoch loss: 9.368193626403809
Total epoch: 9. epoch loss: 8.733813285827637
Total epoch: 10. epoch loss: 8.137801170349121
Total epoch: 11. epoch loss: 7.580341815948486
Total epoch: 12. epoch loss: 7.061503887176514
Total epoch: 13. epoch loss: 6.580857753753662
Total epoch: 14. epoch loss: 6.137203216552734
Total epoch: 15. epoch loss: 5.728673458099365
Total epoch: 16. epoch loss: 5.352811336517334
Total epoch: 17. epoch loss: 5.006824970245361
Total epoch: 18. epoch loss: 4.687763214111328
Total epoch: 19. epoch loss: 4.393008232116699
Total epoch: 20. epoch loss: 4.121108055114746
Total epoch: 21. epoch loss: 3.870422840118408
Total epoch: 22. epoch loss: 3.639350414276123
Total epoch: 23. epoch loss: 3.4263978004455566
Total epoch: 24. epoch loss: 3.230220079421997
Total epoch: 25. epoch loss: 3.0495357513427734
Total epoch: 26. epoch loss: 2.8831946849823
Total epoch: 27. epoch loss: 2.7301268577575684
Total epoch: 28. epoch loss: 2.5893232822418213
Total epoch: 29. epoch loss: 2.4598500728607178
Total epoch: 30. epoch loss: 2.3407773971557617
Total epoch: 31. epoch loss: 2.2312488555908203
Total epoch: 32. epoch loss: 2.130401134490967
Total epoch: 33. epoch loss: 2.0374183654785156
Total epoch: 34. epoch loss: 1.9515658617019653
Total epoch: 35. epoch loss: 1.8721288442611694
Total epoch: 36. epoch loss: 1.7985167503356934
Total epoch: 37. epoch loss: 1.7301827669143677
Total epoch: 38. epoch loss: 1.6666923761367798
Total epoch: 39. epoch loss: 1.6076518297195435
Total epoch: 40. epoch loss: 1.5527150630950928
Total epoch: 41. epoch loss: 1.5015852451324463
Total epoch: 42. epoch loss: 1.4539705514907837
Total epoch: 43. epoch loss: 1.4095978736877441
Total epoch: 44. epoch loss: 1.3682043552398682
Total epoch: 45. epoch loss: 1.3295388221740723
Total epoch: 46. epoch loss: 1.2933746576309204
Total epoch: 47. epoch loss: 1.2594877481460571
Total epoch: 48. epoch loss: 1.227685570716858
Total epoch: 49. epoch loss: 1.1977850198745728
Total epoch: 50. epoch loss: 1.1696304082870483
Total epoch: 51. epoch loss: 1.143071174621582
Total epoch: 52. epoch loss: 1.11797034740448
Total epoch: 53. epoch loss: 1.0942158699035645
Total epoch: 54. epoch loss: 1.0716863870620728
Total epoch: 55. epoch loss: 1.0502934455871582
Total epoch: 56. epoch loss: 1.029945969581604
Total epoch: 57. epoch loss: 1.0105663537979126
Total epoch: 58. epoch loss: 0.992084801197052
Total epoch: 59. epoch loss: 0.9744361639022827
Total epoch: 60. epoch loss: 0.9575716853141785
Total epoch: 61. epoch loss: 0.9414380192756653
Total epoch: 62. epoch loss: 0.9259843826293945
Total epoch: 63. epoch loss: 0.9111738204956055
Total epoch: 64. epoch loss: 0.8969628214836121
Total epoch: 65. epoch loss: 0.8833109736442566
Total epoch: 66. epoch loss: 0.8701866865158081
Total epoch: 67. epoch loss: 0.8575599789619446
Total epoch: 68. epoch loss: 0.8453987836837769
Total epoch: 69. epoch loss: 0.833674967288971
Total epoch: 70. epoch loss: 0.8223680853843689
Total epoch: 71. epoch loss: 0.8114526867866516
Total epoch: 72. epoch loss: 0.8009079694747925
Total epoch: 73. epoch loss: 0.7907142043113708
Total epoch: 74. epoch loss: 0.7808526754379272
Total epoch: 75. epoch loss: 0.7713050842285156
Total epoch: 76. epoch loss: 0.7620512843132019
Total epoch: 77. epoch loss: 0.7530790567398071
Total epoch: 78. epoch loss: 0.744377613067627
Total epoch: 79. epoch loss: 0.7359260320663452
Total epoch: 80. epoch loss: 0.7277171015739441
Total epoch: 81. epoch loss: 0.7197365760803223
Total epoch: 82. epoch loss: 0.7119759321212769
Total epoch: 83. epoch loss: 0.704420804977417
Total epoch: 84. epoch loss: 0.6970642805099487
Total epoch: 85. epoch loss: 0.6899012923240662
Total epoch: 86. epoch loss: 0.6829240918159485
Total epoch: 87. epoch loss: 0.6761149764060974
Total epoch: 88. epoch loss: 0.6694763898849487
Total epoch: 89. epoch loss: 0.6630022525787354
Total epoch: 90. epoch loss: 0.6566786766052246
Total epoch: 91. epoch loss: 0.650503933429718
Total epoch: 92. epoch loss: 0.64447021484375
Total epoch: 93. epoch loss: 0.6385757327079773
Total epoch: 94. epoch loss: 0.6328111886978149
Total epoch: 95. epoch loss: 0.6271749138832092
Total epoch: 96. epoch loss: 0.6216635704040527
Total epoch: 97. epoch loss: 0.6162680983543396
Total epoch: 98. epoch loss: 0.6109864711761475
Total epoch: 99. epoch loss: 0.6058148145675659
Total epoch: 100. epoch loss: 0.6007441282272339
Total epoch: 101. epoch loss: 0.5957844257354736
Total epoch: 102. epoch loss: 0.5909186005592346
Total epoch: 103. epoch loss: 0.5861501693725586
Total epoch: 104. epoch loss: 0.5814758539199829
Total epoch: 105. epoch loss: 0.5768848657608032
Total epoch: 106. epoch loss: 0.5723865628242493
Total epoch: 107. epoch loss: 0.5679711699485779
Total epoch: 108. epoch loss: 0.5636374354362488
Total epoch: 109. epoch loss: 0.5593822598457336
Total epoch: 110. epoch loss: 0.555202841758728
Total epoch: 111. epoch loss: 0.5511016249656677
Total epoch: 112. epoch loss: 0.5470712780952454
Total epoch: 113. epoch loss: 0.5431104898452759
Total epoch: 114. epoch loss: 0.539216935634613
Total epoch: 115. epoch loss: 0.5353906154632568
Total epoch: 116. epoch loss: 0.5316281318664551
Total epoch: 117. epoch loss: 0.5279316902160645
Total epoch: 118. epoch loss: 0.524294376373291
Total epoch: 119. epoch loss: 0.5207163095474243
Total epoch: 120. epoch loss: 0.5171985030174255
Total epoch: 121. epoch loss: 0.5137353539466858
Total epoch: 122. epoch loss: 0.5103278160095215
Total epoch: 123. epoch loss: 0.5069742798805237
Total epoch: 124. epoch loss: 0.5036729574203491
Total epoch: 125. epoch loss: 0.5004212856292725
Total epoch: 126. epoch loss: 0.49722185730934143
Total epoch: 127. epoch loss: 0.4940697252750397
Total epoch: 128. epoch loss: 0.49096542596817017
Total epoch: 129. epoch loss: 0.48790818452835083
Total epoch: 130. epoch loss: 0.48489677906036377
Total epoch: 131. epoch loss: 0.4819272458553314
Total epoch: 132. epoch loss: 0.47900280356407166
Total epoch: 133. epoch loss: 0.47612085938453674
Total epoch: 134. epoch loss: 0.4732787609100342
Total epoch: 135. epoch loss: 0.4704771935939789
Total epoch: 136. epoch loss: 0.4677163362503052
Total epoch: 137. epoch loss: 0.464993953704834
Total epoch: 138. epoch loss: 0.462307333946228
Total epoch: 139. epoch loss: 0.45966026186943054
Total epoch: 140. epoch loss: 0.45704859495162964
Total epoch: 141. epoch loss: 0.4544712007045746
Total epoch: 142. epoch loss: 0.4519283175468445
Total epoch: 143. epoch loss: 0.44942206144332886
Total epoch: 144. epoch loss: 0.4469473958015442
Total epoch: 145. epoch loss: 0.4445047080516815
Total epoch: 146. epoch loss: 0.4420924484729767
Total epoch: 147. epoch loss: 0.4397141933441162
Total epoch: 148. epoch loss: 0.4373655617237091
Total epoch: 149. epoch loss: 0.4350464940071106
Total epoch: 149. DecT loss: 0.4350464940071106
Training time: 0.7136657238006592
APL_precision: 0.2838983050847458, APL_recall: 0.3941176470588235, APL_f1: 0.33004926108374383, APL_number: 170
CMT_precision: 0.3971119133574007, CMT_recall: 0.5641025641025641, CMT_f1: 0.4661016949152542, CMT_number: 195
DSC_precision: 0.4746376811594203, DSC_recall: 0.5995423340961098, DSC_f1: 0.5298281092012133, DSC_number: 437
MAT_precision: 0.5852981969486823, MAT_recall: 0.6187683284457478, MAT_f1: 0.6015680684248039, MAT_number: 682
PRO_precision: 0.4036222509702458, PRO_recall: 0.4046692607003891, PRO_f1: 0.4041450777202073, PRO_number: 771
SMT_precision: 0.3709090909090909, SMT_recall: 0.5964912280701754, SMT_f1: 0.4573991031390134, SMT_number: 171
SPL_precision: 0.4383561643835616, SPL_recall: 0.4266666666666667, SPL_f1: 0.4324324324324324, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.4496044031647747, overall_recall: 0.5225909636145541, overall_f1: 0.4833579881656805, overall_accuracy: 0.8344650132227861
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:02<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:14:51 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:14:52 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:150,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:2.0, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1227.12it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4885.30 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:14:59 - INFO - __main__ - ***** Running training *****
05/31/2023 13:14:59 - INFO - __main__ -   Num examples = 64
05/31/2023 13:14:59 - INFO - __main__ -   Num Epochs = 150
05/31/2023 13:14:59 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:14:59 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:14:59 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:14:59 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 16.419172286987305
Total epoch: 1. epoch loss: 15.469401359558105
Total epoch: 2. epoch loss: 14.540933609008789
Total epoch: 3. epoch loss: 13.641307830810547
Total epoch: 4. epoch loss: 12.777667045593262
Total epoch: 5. epoch loss: 11.954181671142578
Total epoch: 6. epoch loss: 11.172175407409668
Total epoch: 7. epoch loss: 10.431467056274414
Total epoch: 8. epoch loss: 9.731502532958984
Total epoch: 9. epoch loss: 9.071986198425293
Total epoch: 10. epoch loss: 8.452919006347656
Total epoch: 11. epoch loss: 7.874411106109619
Total epoch: 12. epoch loss: 7.336228370666504
Total epoch: 13. epoch loss: 6.837627410888672
Total epoch: 14. epoch loss: 6.377098560333252
Total epoch: 15. epoch loss: 5.952648639678955
Total epoch: 16. epoch loss: 5.56170129776001
Total epoch: 17. epoch loss: 5.201495170593262
Total epoch: 18. epoch loss: 4.869074821472168
Total epoch: 19. epoch loss: 4.561586856842041
Total epoch: 20. epoch loss: 4.277243137359619
Total epoch: 21. epoch loss: 4.014585018157959
Total epoch: 22. epoch loss: 3.7721214294433594
Total epoch: 23. epoch loss: 3.5483853816986084
Total epoch: 24. epoch loss: 3.3420326709747314
Total epoch: 25. epoch loss: 3.1518564224243164
Total epoch: 26. epoch loss: 2.9767463207244873
Total epoch: 27. epoch loss: 2.815721273422241
Total epoch: 28. epoch loss: 2.6677937507629395
Total epoch: 29. epoch loss: 2.532033681869507
Total epoch: 30. epoch loss: 2.407470464706421
Total epoch: 31. epoch loss: 2.293158531188965
Total epoch: 32. epoch loss: 2.188150644302368
Total epoch: 33. epoch loss: 2.091536283493042
Total epoch: 34. epoch loss: 2.0024800300598145
Total epoch: 35. epoch loss: 1.9202171564102173
Total epoch: 36. epoch loss: 1.8440804481506348
Total epoch: 37. epoch loss: 1.7734922170639038
Total epoch: 38. epoch loss: 1.7079471349716187
Total epoch: 39. epoch loss: 1.6470147371292114
Total epoch: 40. epoch loss: 1.5903398990631104
Total epoch: 41. epoch loss: 1.5375887155532837
Total epoch: 42. epoch loss: 1.4884718656539917
Total epoch: 43. epoch loss: 1.442705512046814
Total epoch: 44. epoch loss: 1.4000340700149536
Total epoch: 45. epoch loss: 1.360202431678772
Total epoch: 46. epoch loss: 1.3229693174362183
Total epoch: 47. epoch loss: 1.2881100177764893
Total epoch: 48. epoch loss: 1.2554157972335815
Total epoch: 49. epoch loss: 1.224696159362793
Total epoch: 50. epoch loss: 1.1957759857177734
Total epoch: 51. epoch loss: 1.168502688407898
Total epoch: 52. epoch loss: 1.1427382230758667
Total epoch: 53. epoch loss: 1.1183534860610962
Total epoch: 54. epoch loss: 1.0952476263046265
Total epoch: 55. epoch loss: 1.073306679725647
Total epoch: 56. epoch loss: 1.0524513721466064
Total epoch: 57. epoch loss: 1.032594084739685
Total epoch: 58. epoch loss: 1.0136724710464478
Total epoch: 59. epoch loss: 0.9956086874008179
Total epoch: 60. epoch loss: 0.9783490300178528
Total epoch: 61. epoch loss: 0.9618483781814575
Total epoch: 62. epoch loss: 0.9460504055023193
Total epoch: 63. epoch loss: 0.9309110045433044
Total epoch: 64. epoch loss: 0.9163821935653687
Total epoch: 65. epoch loss: 0.9024389386177063
Total epoch: 66. epoch loss: 0.8890389204025269
Total epoch: 67. epoch loss: 0.8761430382728577
Total epoch: 68. epoch loss: 0.8637297749519348
Total epoch: 69. epoch loss: 0.8517646789550781
Total epoch: 70. epoch loss: 0.8402283191680908
Total epoch: 71. epoch loss: 0.8290899395942688
Total epoch: 72. epoch loss: 0.8183325529098511
Total epoch: 73. epoch loss: 0.8079316020011902
Total epoch: 74. epoch loss: 0.7978728413581848
Total epoch: 75. epoch loss: 0.7881366610527039
Total epoch: 76. epoch loss: 0.7787036895751953
Total epoch: 77. epoch loss: 0.769560694694519
Total epoch: 78. epoch loss: 0.7606961727142334
Total epoch: 79. epoch loss: 0.7520889043807983
Total epoch: 80. epoch loss: 0.7437306642532349
Total epoch: 81. epoch loss: 0.7356069087982178
Total epoch: 82. epoch loss: 0.7277114987373352
Total epoch: 83. epoch loss: 0.720027506351471
Total epoch: 84. epoch loss: 0.7125486135482788
Total epoch: 85. epoch loss: 0.7052631378173828
Total epoch: 86. epoch loss: 0.6981686353683472
Total epoch: 87. epoch loss: 0.6912501454353333
Total epoch: 88. epoch loss: 0.6845059394836426
Total epoch: 89. epoch loss: 0.6779211163520813
Total epoch: 90. epoch loss: 0.6714972257614136
Total epoch: 91. epoch loss: 0.6652222871780396
Total epoch: 92. epoch loss: 0.6590937376022339
Total epoch: 93. epoch loss: 0.6531047821044922
Total epoch: 94. epoch loss: 0.6472493410110474
Total epoch: 95. epoch loss: 0.6415261626243591
Total epoch: 96. epoch loss: 0.6359251141548157
Total epoch: 97. epoch loss: 0.6304438710212708
Total epoch: 98. epoch loss: 0.625080406665802
Total epoch: 99. epoch loss: 0.619828999042511
Total epoch: 100. epoch loss: 0.614685595035553
Total epoch: 101. epoch loss: 0.6096436381340027
Total epoch: 102. epoch loss: 0.604703962802887
Total epoch: 103. epoch loss: 0.5998609066009521
Total epoch: 104. epoch loss: 0.5951154828071594
Total epoch: 105. epoch loss: 0.5904573202133179
Total epoch: 106. epoch loss: 0.5858911275863647
Total epoch: 107. epoch loss: 0.5814070105552673
Total epoch: 108. epoch loss: 0.5770097374916077
Total epoch: 109. epoch loss: 0.5726878643035889
Total epoch: 110. epoch loss: 0.5684499740600586
Total epoch: 111. epoch loss: 0.5642836689949036
Total epoch: 112. epoch loss: 0.5601924061775208
Total epoch: 113. epoch loss: 0.5561728477478027
Total epoch: 114. epoch loss: 0.5522257089614868
Total epoch: 115. epoch loss: 0.5483414530754089
Total epoch: 116. epoch loss: 0.5445258021354675
Total epoch: 117. epoch loss: 0.5407739281654358
Total epoch: 118. epoch loss: 0.5370830297470093
Total epoch: 119. epoch loss: 0.5334544777870178
Total epoch: 120. epoch loss: 0.5298835635185242
Total epoch: 121. epoch loss: 0.5263702869415283
Total epoch: 122. epoch loss: 0.5229139924049377
Total epoch: 123. epoch loss: 0.5195121169090271
Total epoch: 124. epoch loss: 0.5161619782447815
Total epoch: 125. epoch loss: 0.5128676295280457
Total epoch: 126. epoch loss: 0.509621262550354
Total epoch: 127. epoch loss: 0.506425678730011
Total epoch: 128. epoch loss: 0.5032773613929749
Total epoch: 129. epoch loss: 0.5001775026321411
Total epoch: 130. epoch loss: 0.49712249636650085
Total epoch: 131. epoch loss: 0.49411559104919434
Total epoch: 132. epoch loss: 0.4911491572856903
Total epoch: 133. epoch loss: 0.48822537064552307
Total epoch: 134. epoch loss: 0.4853455126285553
Total epoch: 135. epoch loss: 0.4825059473514557
Total epoch: 136. epoch loss: 0.479708194732666
Total epoch: 137. epoch loss: 0.47694718837738037
Total epoch: 138. epoch loss: 0.4742252826690674
Total epoch: 139. epoch loss: 0.4715401530265808
Total epoch: 140. epoch loss: 0.4688941240310669
Total epoch: 141. epoch loss: 0.4662821590900421
Total epoch: 142. epoch loss: 0.4637063443660736
Total epoch: 143. epoch loss: 0.4611661434173584
Total epoch: 144. epoch loss: 0.4586571753025055
Total epoch: 145. epoch loss: 0.4561835825443268
Total epoch: 146. epoch loss: 0.453741192817688
Total epoch: 147. epoch loss: 0.4513317346572876
Total epoch: 148. epoch loss: 0.4489516615867615
Total epoch: 149. epoch loss: 0.446601927280426
Total epoch: 149. DecT loss: 0.446601927280426
Training time: 0.8887114524841309
APL_precision: 0.2713178294573643, APL_recall: 0.4117647058823529, APL_f1: 0.32710280373831774, APL_number: 170
CMT_precision: 0.3822525597269625, CMT_recall: 0.5743589743589743, CMT_f1: 0.45901639344262296, CMT_number: 195
DSC_precision: 0.4612676056338028, DSC_recall: 0.5995423340961098, DSC_f1: 0.5213930348258706, DSC_number: 437
MAT_precision: 0.5858156028368794, MAT_recall: 0.6055718475073314, MAT_f1: 0.5955299206921413, MAT_number: 682
PRO_precision: 0.4173441734417344, PRO_recall: 0.39948119325551235, PRO_f1: 0.40821736249171636, PRO_number: 771
SMT_precision: 0.3377049180327869, SMT_recall: 0.6023391812865497, SMT_f1: 0.4327731092436975, SMT_number: 171
SPL_precision: 0.3924050632911392, SPL_recall: 0.41333333333333333, SPL_f1: 0.40259740259740256, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.4409368635437882, overall_recall: 0.5193922431027589, overall_f1: 0.47695979438222874, overall_accuracy: 0.8316775069687656
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:25:02 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:25:04 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1090.85it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4614.02 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:25:40 - INFO - __main__ - ***** Running training *****
05/31/2023 13:25:40 - INFO - __main__ -   Num examples = 64
05/31/2023 13:25:40 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:25:40 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:25:40 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:25:40 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:25:40 - INFO - __main__ -   Total optimization steps = 200
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.825563430786133
Total epoch: 1. epoch loss: 15.173848152160645
Total epoch: 2. epoch loss: 14.270750999450684
Total epoch: 3. epoch loss: 13.017510414123535
Total epoch: 4. epoch loss: 11.551987648010254
Total epoch: 5. epoch loss: 10.098159790039062
Total epoch: 6. epoch loss: 8.740559577941895
Total epoch: 7. epoch loss: 7.473508834838867
Total epoch: 8. epoch loss: 6.327903747558594
Total epoch: 9. epoch loss: 5.334109783172607
Total epoch: 10. epoch loss: 4.497196197509766
Total epoch: 11. epoch loss: 3.792630434036255
Total epoch: 12. epoch loss: 3.186358690261841
Total epoch: 13. epoch loss: 2.6683948040008545
Total epoch: 14. epoch loss: 2.246793508529663
Total epoch: 15. epoch loss: 1.916617512702942
Total epoch: 16. epoch loss: 1.6512887477874756
Total epoch: 17. epoch loss: 1.4276347160339355
Total epoch: 18. epoch loss: 1.2406518459320068
Total epoch: 19. epoch loss: 1.0949933528900146
Total epoch: 20. epoch loss: 0.9871733784675598
Total epoch: 21. epoch loss: 0.9041268825531006
Total epoch: 22. epoch loss: 0.8345414996147156
Total epoch: 23. epoch loss: 0.7737131714820862
Total epoch: 24. epoch loss: 0.7207896709442139
Total epoch: 25. epoch loss: 0.6750325560569763
Total epoch: 26. epoch loss: 0.6353426575660706
Total epoch: 27. epoch loss: 0.6008089780807495
Total epoch: 28. epoch loss: 0.5706099271774292
Total epoch: 29. epoch loss: 0.5438432097434998
Total epoch: 30. epoch loss: 0.5195894241333008
Total epoch: 31. epoch loss: 0.497277170419693
Total epoch: 32. epoch loss: 0.4766310453414917
Total epoch: 33. epoch loss: 0.45768117904663086
Total epoch: 34. epoch loss: 0.44045278429985046
Total epoch: 35. epoch loss: 0.42482244968414307
Total epoch: 36. epoch loss: 0.41046276688575745
Total epoch: 37. epoch loss: 0.39711278676986694
Total epoch: 38. epoch loss: 0.38461706042289734
Total epoch: 39. epoch loss: 0.3729096055030823
Total epoch: 40. epoch loss: 0.3619498610496521
Total epoch: 41. epoch loss: 0.3517081141471863
Total epoch: 42. epoch loss: 0.3421279191970825
Total epoch: 43. epoch loss: 0.3331329822540283
Total epoch: 44. epoch loss: 0.3246416449546814
Total epoch: 45. epoch loss: 0.31663191318511963
Total epoch: 46. epoch loss: 0.30904367566108704
Total epoch: 47. epoch loss: 0.30180931091308594
Total epoch: 48. epoch loss: 0.2948903441429138
Total epoch: 49. epoch loss: 0.28824394941329956
Total epoch: 50. epoch loss: 0.2818472385406494
Total epoch: 51. epoch loss: 0.2756838798522949
Total epoch: 52. epoch loss: 0.26974254846572876
Total epoch: 53. epoch loss: 0.26400911808013916
Total epoch: 54. epoch loss: 0.25846701860427856
Total epoch: 55. epoch loss: 0.2531077265739441
Total epoch: 56. epoch loss: 0.24793337285518646
Total epoch: 57. epoch loss: 0.24294403195381165
Total epoch: 58. epoch loss: 0.2381453961133957
Total epoch: 59. epoch loss: 0.2335517406463623
Total epoch: 60. epoch loss: 0.22915999591350555
Total epoch: 61. epoch loss: 0.2249717116355896
Total epoch: 62. epoch loss: 0.22097717225551605
Total epoch: 63. epoch loss: 0.21715450286865234
Total epoch: 64. epoch loss: 0.21348178386688232
Total epoch: 65. epoch loss: 0.20993167161941528
Total epoch: 66. epoch loss: 0.20648875832557678
Total epoch: 67. epoch loss: 0.20315738022327423
Total epoch: 68. epoch loss: 0.19992834329605103
Total epoch: 69. epoch loss: 0.19681227207183838
Total epoch: 70. epoch loss: 0.19380061328411102
Total epoch: 71. epoch loss: 0.19090135395526886
Total epoch: 72. epoch loss: 0.18810278177261353
Total epoch: 73. epoch loss: 0.185400128364563
Total epoch: 74. epoch loss: 0.18278545141220093
Total epoch: 75. epoch loss: 0.1802511066198349
Total epoch: 76. epoch loss: 0.1777832806110382
Total epoch: 77. epoch loss: 0.1753852665424347
Total epoch: 78. epoch loss: 0.17305392026901245
Total epoch: 79. epoch loss: 0.17079249024391174
Total epoch: 80. epoch loss: 0.16860198974609375
Total epoch: 81. epoch loss: 0.1664900779724121
Total epoch: 82. epoch loss: 0.16445250809192657
Total epoch: 83. epoch loss: 0.16249068081378937
Total epoch: 84. epoch loss: 0.16060137748718262
Total epoch: 85. epoch loss: 0.1587762087583542
Total epoch: 86. epoch loss: 0.15700867772102356
Total epoch: 87. epoch loss: 0.15529459714889526
Total epoch: 88. epoch loss: 0.1536305993795395
Total epoch: 89. epoch loss: 0.15201762318611145
Total epoch: 90. epoch loss: 0.15045645833015442
Total epoch: 91. epoch loss: 0.14894236624240875
Total epoch: 92. epoch loss: 0.14747242629528046
Total epoch: 93. epoch loss: 0.14604321122169495
Total epoch: 94. epoch loss: 0.1446574628353119
Total epoch: 95. epoch loss: 0.14330971240997314
Total epoch: 96. epoch loss: 0.14200448989868164
Total epoch: 97. epoch loss: 0.14073516428470612
Total epoch: 98. epoch loss: 0.13950331509113312
Total epoch: 99. epoch loss: 0.13830597698688507
Total epoch: 99. DecT loss: 0.13830597698688507
Training time: 0.6706316471099854
APL_precision: 0.2536231884057971, APL_recall: 0.4117647058823529, APL_f1: 0.31390134529147984, APL_number: 170
CMT_precision: 0.3782771535580524, CMT_recall: 0.517948717948718, CMT_f1: 0.43722943722943725, CMT_number: 195
DSC_precision: 0.49869451697127937, DSC_recall: 0.43707093821510296, DSC_f1: 0.46585365853658534, DSC_number: 437
MAT_precision: 0.6269230769230769, MAT_recall: 0.4780058651026393, MAT_f1: 0.5424292845257904, MAT_number: 682
PRO_precision: 0.3515509601181684, PRO_recall: 0.3086900129701686, PRO_f1: 0.3287292817679558, PRO_number: 771
SMT_precision: 0.3146067415730337, SMT_recall: 0.49122807017543857, SMT_f1: 0.3835616438356165, SMT_number: 171
SPL_precision: 0.48214285714285715, SPL_recall: 0.36, SPL_f1: 0.4122137404580153, SPL_number: 75
overall_precision: 0.42395748160261654, overall_recall: 0.4146341463414634, overall_f1: 0.4192439862542955, overall_accuracy: 0.811950539632621
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 489, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:28:17 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:28:19 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:0 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1007.28it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4897.48 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:28:26 - INFO - __main__ - ***** Running training *****
05/31/2023 13:28:26 - INFO - __main__ -   Num examples = 64
05/31/2023 13:28:26 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:28:26 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:28:26 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:28:26 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:28:26 - INFO - __main__ -   Total optimization steps = 200
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.812236785888672
Total epoch: 1. epoch loss: 14.884940147399902
Total epoch: 2. epoch loss: 13.987934112548828
Total epoch: 3. epoch loss: 13.123373031616211
Total epoch: 4. epoch loss: 12.294142723083496
Total epoch: 5. epoch loss: 11.50300407409668
Total epoch: 6. epoch loss: 10.7516450881958
Total epoch: 7. epoch loss: 10.040327072143555
Total epoch: 8. epoch loss: 9.368453979492188
Total epoch: 9. epoch loss: 8.735363006591797
Total epoch: 10. epoch loss: 8.140830039978027
Total epoch: 11. epoch loss: 7.584916591644287
Total epoch: 12. epoch loss: 7.067605495452881
Total epoch: 13. epoch loss: 6.588271141052246
Total epoch: 14. epoch loss: 6.1455793380737305
Total epoch: 15. epoch loss: 5.737607479095459
Total epoch: 16. epoch loss: 5.361969947814941
Total epoch: 17. epoch loss: 5.016026020050049
Total epoch: 18. epoch loss: 4.696970462799072
Total epoch: 19. epoch loss: 4.402268886566162
Total epoch: 20. epoch loss: 4.130422592163086
Total epoch: 21. epoch loss: 3.8797292709350586
Total epoch: 22. epoch loss: 3.6485371589660645
Total epoch: 23. epoch loss: 3.435302495956421
Total epoch: 24. epoch loss: 3.2386507987976074
Total epoch: 25. epoch loss: 3.0573313236236572
Total epoch: 26. epoch loss: 2.890239953994751
Total epoch: 27. epoch loss: 2.7363319396972656
Total epoch: 28. epoch loss: 2.5946884155273438
Total epoch: 29. epoch loss: 2.464392900466919
Total epoch: 30. epoch loss: 2.3446125984191895
Total epoch: 31. epoch loss: 2.2344717979431152
Total epoch: 32. epoch loss: 2.133157968521118
Total epoch: 33. epoch loss: 2.0398616790771484
Total epoch: 34. epoch loss: 1.953802227973938
Total epoch: 35. epoch loss: 1.8742769956588745
Total epoch: 36. epoch loss: 1.8006433248519897
Total epoch: 37. epoch loss: 1.7323249578475952
Total epoch: 38. epoch loss: 1.6688597202301025
Total epoch: 39. epoch loss: 1.6098251342773438
Total epoch: 40. epoch loss: 1.5548725128173828
Total epoch: 41. epoch loss: 1.503699541091919
Total epoch: 42. epoch loss: 1.4560163021087646
Total epoch: 43. epoch loss: 1.4115687608718872
Total epoch: 44. epoch loss: 1.3701056241989136
Total epoch: 45. epoch loss: 1.3313794136047363
Total epoch: 46. epoch loss: 1.2951732873916626
Total epoch: 47. epoch loss: 1.2612667083740234
Total epoch: 48. epoch loss: 1.229467749595642
Total epoch: 49. epoch loss: 1.199588656425476
Total epoch: 50. epoch loss: 1.171461582183838
Total epoch: 51. epoch loss: 1.1449365615844727
Total epoch: 52. epoch loss: 1.1198667287826538
Total epoch: 53. epoch loss: 1.0961358547210693
Total epoch: 54. epoch loss: 1.073628306388855
Total epoch: 55. epoch loss: 1.052247405052185
Total epoch: 56. epoch loss: 1.03190016746521
Total epoch: 57. epoch loss: 1.0125186443328857
Total epoch: 58. epoch loss: 0.9940263628959656
Total epoch: 59. epoch loss: 0.9763602614402771
Total epoch: 60. epoch loss: 0.9594738483428955
Total epoch: 61. epoch loss: 0.9433197379112244
Total epoch: 62. epoch loss: 0.9278441667556763
Total epoch: 63. epoch loss: 0.9130100607872009
Total epoch: 64. epoch loss: 0.8987802267074585
Total epoch: 65. epoch loss: 0.8851081132888794
Total epoch: 66. epoch loss: 0.8719684481620789
Total epoch: 67. epoch loss: 0.8593257665634155
Total epoch: 68. epoch loss: 0.8471484184265137
Total epoch: 69. epoch loss: 0.835411548614502
Total epoch: 70. epoch loss: 0.8240881562232971
Total epoch: 71. epoch loss: 0.8131564259529114
Total epoch: 72. epoch loss: 0.8025963306427002
Total epoch: 73. epoch loss: 0.7923833727836609
Total epoch: 74. epoch loss: 0.7825040221214294
Total epoch: 75. epoch loss: 0.7729401588439941
Total epoch: 76. epoch loss: 0.7636734843254089
Total epoch: 77. epoch loss: 0.7546854615211487
Total epoch: 78. epoch loss: 0.7459691166877747
Total epoch: 79. epoch loss: 0.7375062704086304
Total epoch: 80. epoch loss: 0.7292887568473816
Total epoch: 81. epoch loss: 0.7212980389595032
Total epoch: 82. epoch loss: 0.7135262489318848
Total epoch: 83. epoch loss: 0.7059661149978638
Total epoch: 84. epoch loss: 0.6986042261123657
Total epoch: 85. epoch loss: 0.691436231136322
Total epoch: 86. epoch loss: 0.6844511032104492
Total epoch: 87. epoch loss: 0.6776405572891235
Total epoch: 88. epoch loss: 0.67099529504776
Total epoch: 89. epoch loss: 0.6645130515098572
Total epoch: 90. epoch loss: 0.6581805348396301
Total epoch: 91. epoch loss: 0.6520023345947266
Total epoch: 92. epoch loss: 0.6459662318229675
Total epoch: 93. epoch loss: 0.6400624513626099
Total epoch: 94. epoch loss: 0.6342947483062744
Total epoch: 95. epoch loss: 0.6286513805389404
Total epoch: 96. epoch loss: 0.623131275177002
Total epoch: 97. epoch loss: 0.6177302002906799
Total epoch: 98. epoch loss: 0.6124415993690491
Total epoch: 99. epoch loss: 0.60726398229599
Total epoch: 99. DecT loss: 0.60726398229599
Training time: 0.6980340480804443
APL_precision: 0.28515625, APL_recall: 0.4294117647058823, APL_f1: 0.34272300469483563, APL_number: 170
CMT_precision: 0.3806228373702422, CMT_recall: 0.5641025641025641, CMT_f1: 0.45454545454545453, CMT_number: 195
DSC_precision: 0.44880546075085326, DSC_recall: 0.6018306636155606, DSC_f1: 0.5141739980449658, DSC_number: 437
MAT_precision: 0.5661375661375662, MAT_recall: 0.6275659824046921, MAT_f1: 0.5952712100139083, MAT_number: 682
PRO_precision: 0.4175, PRO_recall: 0.43320363164721143, PRO_f1: 0.4252068746021642, PRO_number: 771
SMT_precision: 0.3355263157894737, SMT_recall: 0.5964912280701754, SMT_f1: 0.4294736842105263, SMT_number: 171
SPL_precision: 0.4025974025974026, SPL_recall: 0.41333333333333333, SPL_f1: 0.4078947368421053, SPL_number: 75
overall_precision: 0.43709256844850064, overall_recall: 0.5361855257896841, overall_f1: 0.4815945412102712, overall_accuracy: 0.8312486598527625
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:30:35 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:30:36 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 895.93it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5230.28 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:31:03 - INFO - __main__ - ***** Running training *****
05/31/2023 13:31:03 - INFO - __main__ -   Num examples = 64
05/31/2023 13:31:03 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:31:03 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:31:03 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:31:03 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:31:03 - INFO - __main__ -   Total optimization steps = 200
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.812236785888672
Total epoch: 1. epoch loss: 14.884943008422852
Total epoch: 2. epoch loss: 13.987939834594727
Total epoch: 3. epoch loss: 13.12337589263916
Total epoch: 4. epoch loss: 12.294137954711914
Total epoch: 5. epoch loss: 11.50299072265625
Total epoch: 6. epoch loss: 10.751618385314941
Total epoch: 7. epoch loss: 10.040287971496582
Total epoch: 8. epoch loss: 9.368400573730469
Total epoch: 9. epoch loss: 8.735298156738281
Total epoch: 10. epoch loss: 8.140758514404297
Total epoch: 11. epoch loss: 7.584843158721924
Total epoch: 12. epoch loss: 7.067523956298828
Total epoch: 13. epoch loss: 6.58819055557251
Total epoch: 14. epoch loss: 6.145500659942627
Total epoch: 15. epoch loss: 5.737521648406982
Total epoch: 16. epoch loss: 5.361868381500244
Total epoch: 17. epoch loss: 5.0159149169921875
Total epoch: 18. epoch loss: 4.6968488693237305
Total epoch: 19. epoch loss: 4.4021315574646
Total epoch: 20. epoch loss: 4.130269527435303
Total epoch: 21. epoch loss: 3.8795647621154785
Total epoch: 22. epoch loss: 3.648360013961792
Total epoch: 23. epoch loss: 3.435120105743408
Total epoch: 24. epoch loss: 3.238467216491699
Total epoch: 25. epoch loss: 3.057142734527588
Total epoch: 26. epoch loss: 2.890068292617798
Total epoch: 27. epoch loss: 2.736177444458008
Total epoch: 28. epoch loss: 2.594562530517578
Total epoch: 29. epoch loss: 2.4643003940582275
Total epoch: 30. epoch loss: 2.3445494174957275
Total epoch: 31. epoch loss: 2.2344541549682617
Total epoch: 32. epoch loss: 2.133178949356079
Total epoch: 33. epoch loss: 2.0399296283721924
Total epoch: 34. epoch loss: 1.9539111852645874
Total epoch: 35. epoch loss: 1.8744261264801025
Total epoch: 36. epoch loss: 1.8008394241333008
Total epoch: 37. epoch loss: 1.7325669527053833
Total epoch: 38. epoch loss: 1.6691439151763916
Total epoch: 39. epoch loss: 1.6101628541946411
Total epoch: 40. epoch loss: 1.5552583932876587
Total epoch: 41. epoch loss: 1.504123568534851
Total epoch: 42. epoch loss: 1.4565004110336304
Total epoch: 43. epoch loss: 1.4120954275131226
Total epoch: 44. epoch loss: 1.3706825971603394
Total epoch: 45. epoch loss: 1.3320081233978271
Total epoch: 46. epoch loss: 1.2958520650863647
Total epoch: 47. epoch loss: 1.2619999647140503
Total epoch: 48. epoch loss: 1.2302474975585938
Total epoch: 49. epoch loss: 1.2004170417785645
Total epoch: 50. epoch loss: 1.1723359823226929
Total epoch: 51. epoch loss: 1.1458566188812256
Total epoch: 52. epoch loss: 1.1208381652832031
Total epoch: 53. epoch loss: 1.0971559286117554
Total epoch: 54. epoch loss: 1.074692964553833
Total epoch: 55. epoch loss: 1.0533593893051147
Total epoch: 56. epoch loss: 1.0330647230148315
Total epoch: 57. epoch loss: 1.013724684715271
Total epoch: 58. epoch loss: 0.9952784180641174
Total epoch: 59. epoch loss: 0.9776604175567627
Total epoch: 60. epoch loss: 0.960822343826294
Total epoch: 61. epoch loss: 0.9447100758552551
Total epoch: 62. epoch loss: 0.9292834401130676
Total epoch: 63. epoch loss: 0.9144949316978455
Total epoch: 64. epoch loss: 0.9003090262413025
Total epoch: 65. epoch loss: 0.8866850137710571
Total epoch: 66. epoch loss: 0.8735935091972351
Total epoch: 67. epoch loss: 0.8609976172447205
Total epoch: 68. epoch loss: 0.8488661050796509
Total epoch: 69. epoch loss: 0.8371755480766296
Total epoch: 70. epoch loss: 0.8258972764015198
Total epoch: 71. epoch loss: 0.8150168061256409
Total epoch: 72. epoch loss: 0.8045017123222351
Total epoch: 73. epoch loss: 0.7943375706672668
Total epoch: 74. epoch loss: 0.7845045328140259
Total epoch: 75. epoch loss: 0.7749889492988586
Total epoch: 76. epoch loss: 0.7657647728919983
Total epoch: 77. epoch loss: 0.7568231225013733
Total epoch: 78. epoch loss: 0.748150646686554
Total epoch: 79. epoch loss: 0.7397368550300598
Total epoch: 80. epoch loss: 0.7315590381622314
Total epoch: 81. epoch loss: 0.7236166000366211
Total epoch: 82. epoch loss: 0.7158918380737305
Total epoch: 83. epoch loss: 0.7083771824836731
Total epoch: 84. epoch loss: 0.7010597586631775
Total epoch: 85. epoch loss: 0.6939359903335571
Total epoch: 86. epoch loss: 0.6869917511940002
Total epoch: 87. epoch loss: 0.6802237033843994
Total epoch: 88. epoch loss: 0.6736214756965637
Total epoch: 89. epoch loss: 0.6671802997589111
Total epoch: 90. epoch loss: 0.6608970165252686
Total epoch: 91. epoch loss: 0.6547564268112183
Total epoch: 92. epoch loss: 0.6487621665000916
Total epoch: 93. epoch loss: 0.6428999900817871
Total epoch: 94. epoch loss: 0.6371724605560303
Total epoch: 95. epoch loss: 0.6315712332725525
Total epoch: 96. epoch loss: 0.6260939836502075
Total epoch: 97. epoch loss: 0.620732843875885
Total epoch: 98. epoch loss: 0.6154831051826477
Total epoch: 99. epoch loss: 0.6103476881980896
Total epoch: 99. DecT loss: 0.6103476881980896
Training time: 0.6804652214050293
APL_precision: 0.28627450980392155, APL_recall: 0.4294117647058823, APL_f1: 0.34352941176470586, APL_number: 170
CMT_precision: 0.3819444444444444, CMT_recall: 0.5641025641025641, CMT_f1: 0.45548654244306414, CMT_number: 195
DSC_precision: 0.44804088586030666, DSC_recall: 0.6018306636155606, DSC_f1: 0.513671875, DSC_number: 437
MAT_precision: 0.5667107001321003, MAT_recall: 0.6290322580645161, MAT_f1: 0.5962473940236275, MAT_number: 682
PRO_precision: 0.41802252816020025, PRO_recall: 0.43320363164721143, PRO_f1: 0.4254777070063694, PRO_number: 771
SMT_precision: 0.3377049180327869, SMT_recall: 0.6023391812865497, SMT_f1: 0.4327731092436975, SMT_number: 171
SPL_precision: 0.3974358974358974, SPL_recall: 0.41333333333333333, SPL_f1: 0.40522875816993464, SPL_number: 75
overall_precision: 0.4376018246985989, overall_recall: 0.536985205917633, overall_f1: 0.4822262118491921, overall_accuracy: 0.8312486598527625
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:33:27 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:33:29 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1073.54it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
05/31/2023 13:33:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ce3c4f0d7bc29425.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4569.71 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:33:35 - INFO - __main__ - ***** Running training *****
05/31/2023 13:33:35 - INFO - __main__ -   Num examples = 64
05/31/2023 13:33:35 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:33:35 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:33:35 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:33:35 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:33:35 - INFO - __main__ -   Total optimization steps = 200
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.812216758728027
Total epoch: 1. epoch loss: 14.89633846282959
Total epoch: 2. epoch loss: 14.029269218444824
Total epoch: 3. epoch loss: 13.207959175109863
Total epoch: 4. epoch loss: 12.430338859558105
Total epoch: 5. epoch loss: 11.694823265075684
Total epoch: 6. epoch loss: 11.000165939331055
Total epoch: 7. epoch loss: 10.345174789428711
Total epoch: 8. epoch loss: 9.728507995605469
Total epoch: 9. epoch loss: 9.148632049560547
Total epoch: 10. epoch loss: 8.60379695892334
Total epoch: 11. epoch loss: 8.092180252075195
Total epoch: 12. epoch loss: 7.611810684204102
Total epoch: 13. epoch loss: 7.16072940826416
Total epoch: 14. epoch loss: 6.737067222595215
Total epoch: 15. epoch loss: 6.339112758636475
Total epoch: 16. epoch loss: 5.9653143882751465
Total epoch: 17. epoch loss: 5.614284992218018
Total epoch: 18. epoch loss: 5.2846479415893555
Total epoch: 19. epoch loss: 4.975735664367676
Total epoch: 20. epoch loss: 4.687107563018799
Total epoch: 21. epoch loss: 4.417985439300537
Total epoch: 22. epoch loss: 4.16744327545166
Total epoch: 23. epoch loss: 3.934448480606079
Total epoch: 24. epoch loss: 3.7179367542266846
Total epoch: 25. epoch loss: 3.516850233078003
Total epoch: 26. epoch loss: 3.330122232437134
Total epoch: 27. epoch loss: 3.1567330360412598
Total epoch: 28. epoch loss: 2.995673179626465
Total epoch: 29. epoch loss: 2.8460018634796143
Total epoch: 30. epoch loss: 2.7068583965301514
Total epoch: 31. epoch loss: 2.577393054962158
Total epoch: 32. epoch loss: 2.4569101333618164
Total epoch: 33. epoch loss: 2.3446803092956543
Total epoch: 34. epoch loss: 2.240090847015381
Total epoch: 35. epoch loss: 2.142540693283081
Total epoch: 36. epoch loss: 2.0514981746673584
Total epoch: 37. epoch loss: 1.9664472341537476
Total epoch: 38. epoch loss: 1.8869420289993286
Total epoch: 39. epoch loss: 1.812544822692871
Total epoch: 40. epoch loss: 1.7428816556930542
Total epoch: 41. epoch loss: 1.6775871515274048
Total epoch: 42. epoch loss: 1.6163429021835327
Total epoch: 43. epoch loss: 1.5588468313217163
Total epoch: 44. epoch loss: 1.5048253536224365
Total epoch: 45. epoch loss: 1.4540321826934814
Total epoch: 46. epoch loss: 1.4062354564666748
Total epoch: 47. epoch loss: 1.3612174987792969
Total epoch: 48. epoch loss: 1.3187787532806396
Total epoch: 49. epoch loss: 1.2787450551986694
Total epoch: 50. epoch loss: 1.2409261465072632
Total epoch: 51. epoch loss: 1.2051852941513062
Total epoch: 52. epoch loss: 1.1713602542877197
Total epoch: 53. epoch loss: 1.1393250226974487
Total epoch: 54. epoch loss: 1.108941912651062
Total epoch: 55. epoch loss: 1.0801043510437012
Total epoch: 56. epoch loss: 1.0527056455612183
Total epoch: 57. epoch loss: 1.0266469717025757
Total epoch: 58. epoch loss: 1.0018367767333984
Total epoch: 59. epoch loss: 0.9781945943832397
Total epoch: 60. epoch loss: 0.955647349357605
Total epoch: 61. epoch loss: 0.9341171979904175
Total epoch: 62. epoch loss: 0.9135446548461914
Total epoch: 63. epoch loss: 0.8938690423965454
Total epoch: 64. epoch loss: 0.8750393986701965
Total epoch: 65. epoch loss: 0.856995701789856
Total epoch: 66. epoch loss: 0.8396974802017212
Total epoch: 67. epoch loss: 0.8231028318405151
Total epoch: 68. epoch loss: 0.807161808013916
Total epoch: 69. epoch loss: 0.791841983795166
Total epoch: 70. epoch loss: 0.7771067023277283
Total epoch: 71. epoch loss: 0.7629317045211792
Total epoch: 72. epoch loss: 0.7492719292640686
Total epoch: 73. epoch loss: 0.736104428768158
Total epoch: 74. epoch loss: 0.7234106063842773
Total epoch: 75. epoch loss: 0.7111532092094421
Total epoch: 76. epoch loss: 0.6993106603622437
Total epoch: 77. epoch loss: 0.6878697872161865
Total epoch: 78. epoch loss: 0.676805853843689
Total epoch: 79. epoch loss: 0.6660944223403931
Total epoch: 80. epoch loss: 0.6557258367538452
Total epoch: 81. epoch loss: 0.6456707715988159
Total epoch: 82. epoch loss: 0.6359273791313171
Total epoch: 83. epoch loss: 0.6264716982841492
Total epoch: 84. epoch loss: 0.617296576499939
Total epoch: 85. epoch loss: 0.6083800792694092
Total epoch: 86. epoch loss: 0.599722683429718
Total epoch: 87. epoch loss: 0.5913029909133911
Total epoch: 88. epoch loss: 0.5831102132797241
Total epoch: 89. epoch loss: 0.5751404166221619
Total epoch: 90. epoch loss: 0.5673786997795105
Total epoch: 91. epoch loss: 0.5598205327987671
Total epoch: 92. epoch loss: 0.5524566769599915
Total epoch: 93. epoch loss: 0.5452752709388733
Total epoch: 94. epoch loss: 0.5382742285728455
Total epoch: 95. epoch loss: 0.5314435362815857
Total epoch: 96. epoch loss: 0.5247741937637329
Total epoch: 97. epoch loss: 0.5182669162750244
Total epoch: 98. epoch loss: 0.5119150876998901
Total epoch: 99. epoch loss: 0.5057039856910706
Total epoch: 99. DecT loss: 0.5057039856910706
Training time: 0.6629345417022705
APL_precision: 0.30434782608695654, APL_recall: 0.4117647058823529, APL_f1: 0.35, APL_number: 170
CMT_precision: 0.3938223938223938, CMT_recall: 0.5230769230769231, CMT_f1: 0.44933920704845814, CMT_number: 195
DSC_precision: 0.46503496503496505, DSC_recall: 0.6086956521739131, DSC_f1: 0.5272547076313182, DSC_number: 437
MAT_precision: 0.5485933503836317, MAT_recall: 0.6290322580645161, MAT_f1: 0.5860655737704917, MAT_number: 682
PRO_precision: 0.4154057771664374, PRO_recall: 0.39169909208819714, PRO_f1: 0.4032042723631509, PRO_number: 771
SMT_precision: 0.3221476510067114, SMT_recall: 0.5614035087719298, SMT_f1: 0.40938166311300633, SMT_number: 171
SPL_precision: 0.38235294117647056, SPL_recall: 0.3466666666666667, SPL_f1: 0.3636363636363636, SPL_number: 75
overall_precision: 0.43971389645776565, overall_recall: 0.5161935225909636, overall_f1: 0.47489424314879525, overall_accuracy: 0.8312486598527625
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:35:15 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:35:16 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 849.65it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
05/31/2023 13:35:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ce3c4f0d7bc29425.arrow
05/31/2023 13:35:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-1ca8e1966778dcf2/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0d27c911173ae058.arrow
/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:35:22 - INFO - __main__ - ***** Running training *****
05/31/2023 13:35:22 - INFO - __main__ -   Num examples = 64
05/31/2023 13:35:22 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:35:22 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:35:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:35:22 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:35:22 - INFO - __main__ -   Total optimization steps = 200
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.679176330566406
Total epoch: 1. epoch loss: 14.75802993774414
Total epoch: 2. epoch loss: 13.867758750915527
Total epoch: 3. epoch loss: 13.009511947631836
Total epoch: 4. epoch loss: 12.185879707336426
Total epoch: 5. epoch loss: 11.399680137634277
Total epoch: 6. epoch loss: 10.652884483337402
Total epoch: 7. epoch loss: 9.94605541229248
Total epoch: 8. epoch loss: 9.27882194519043
Total epoch: 9. epoch loss: 8.650627136230469
Total epoch: 10. epoch loss: 8.061251640319824
Total epoch: 11. epoch loss: 7.510704040527344
Total epoch: 12. epoch loss: 6.998834609985352
Total epoch: 13. epoch loss: 6.524846076965332
Total epoch: 14. epoch loss: 6.087264060974121
Total epoch: 15. epoch loss: 5.684077262878418
Total epoch: 16. epoch loss: 5.312854766845703
Total epoch: 17. epoch loss: 4.971014022827148
Total epoch: 18. epoch loss: 4.65579080581665
Total epoch: 19. epoch loss: 4.3648362159729
Total epoch: 20. epoch loss: 4.096561431884766
Total epoch: 21. epoch loss: 3.849141836166382
Total epoch: 22. epoch loss: 3.6208810806274414
Total epoch: 23. epoch loss: 3.410257577896118
Total epoch: 24. epoch loss: 3.21591854095459
Total epoch: 25. epoch loss: 3.036700963973999
Total epoch: 26. epoch loss: 2.8715107440948486
Total epoch: 27. epoch loss: 2.7193760871887207
Total epoch: 28. epoch loss: 2.5793447494506836
Total epoch: 29. epoch loss: 2.4505348205566406
Total epoch: 30. epoch loss: 2.332101821899414
Total epoch: 31. epoch loss: 2.2232093811035156
Total epoch: 32. epoch loss: 2.1230385303497314
Total epoch: 33. epoch loss: 2.0307836532592773
Total epoch: 34. epoch loss: 1.9456946849822998
Total epoch: 35. epoch loss: 1.867047667503357
Total epoch: 36. epoch loss: 1.7942235469818115
Total epoch: 37. epoch loss: 1.726641297340393
Total epoch: 38. epoch loss: 1.6638526916503906
Total epoch: 39. epoch loss: 1.6054335832595825
Total epoch: 40. epoch loss: 1.551047921180725
Total epoch: 41. epoch loss: 1.500382661819458
Total epoch: 42. epoch loss: 1.4531735181808472
Total epoch: 43. epoch loss: 1.4091465473175049
Total epoch: 44. epoch loss: 1.3680644035339355
Total epoch: 45. epoch loss: 1.3296877145767212
Total epoch: 46. epoch loss: 1.2937922477722168
Total epoch: 47. epoch loss: 1.2601702213287354
Total epoch: 48. epoch loss: 1.2286252975463867
Total epoch: 49. epoch loss: 1.1989765167236328
Total epoch: 50. epoch loss: 1.1710609197616577
Total epoch: 51. epoch loss: 1.1447213888168335
Total epoch: 52. epoch loss: 1.119829535484314
Total epoch: 53. epoch loss: 1.0962531566619873
Total epoch: 54. epoch loss: 1.0738894939422607
Total epoch: 55. epoch loss: 1.0526368618011475
Total epoch: 56. epoch loss: 1.0324151515960693
Total epoch: 57. epoch loss: 1.013140082359314
Total epoch: 58. epoch loss: 0.9947502613067627
Total epoch: 59. epoch loss: 0.9771814346313477
Total epoch: 60. epoch loss: 0.9603852033615112
Total epoch: 61. epoch loss: 0.9443144798278809
Total epoch: 62. epoch loss: 0.9289184808731079
Total epoch: 63. epoch loss: 0.9141603112220764
Total epoch: 64. epoch loss: 0.899999737739563
Total epoch: 65. epoch loss: 0.8863970637321472
Total epoch: 66. epoch loss: 0.8733217120170593
Total epoch: 67. epoch loss: 0.8607427477836609
Total epoch: 68. epoch loss: 0.8486250638961792
Total epoch: 69. epoch loss: 0.8369461894035339
Total epoch: 70. epoch loss: 0.8256814479827881
Total epoch: 71. epoch loss: 0.8148060441017151
Total epoch: 72. epoch loss: 0.8042967319488525
Total epoch: 73. epoch loss: 0.7941394448280334
Total epoch: 74. epoch loss: 0.7843090891838074
Total epoch: 75. epoch loss: 0.7747927308082581
Total epoch: 76. epoch loss: 0.7655733227729797
Total epoch: 77. epoch loss: 0.7566304802894592
Total epoch: 78. epoch loss: 0.74796062707901
Total epoch: 79. epoch loss: 0.7395374178886414
Total epoch: 80. epoch loss: 0.7313594222068787
Total epoch: 81. epoch loss: 0.7234107255935669
Total epoch: 82. epoch loss: 0.7156796455383301
Total epoch: 83. epoch loss: 0.7081584930419922
Total epoch: 84. epoch loss: 0.7008333802223206
Total epoch: 85. epoch loss: 0.693699061870575
Total epoch: 86. epoch loss: 0.6867469549179077
Total epoch: 87. epoch loss: 0.6799703240394592
Total epoch: 88. epoch loss: 0.6733623743057251
Total epoch: 89. epoch loss: 0.6669113039970398
Total epoch: 90. epoch loss: 0.6606147885322571
Total epoch: 91. epoch loss: 0.6544646620750427
Total epoch: 92. epoch loss: 0.6484563946723938
Total epoch: 93. epoch loss: 0.6425877809524536
Total epoch: 94. epoch loss: 0.6368483901023865
Total epoch: 95. epoch loss: 0.6312353014945984
Total epoch: 96. epoch loss: 0.6257407069206238
Total epoch: 97. epoch loss: 0.6203724145889282
Total epoch: 98. epoch loss: 0.6151103377342224
Total epoch: 99. epoch loss: 0.6099581122398376
Total epoch: 99. DecT loss: 0.6099581122398376
Training time: 0.7011101245880127
APL_precision: 0.2868217054263566, APL_recall: 0.43529411764705883, APL_f1: 0.34579439252336447, APL_number: 170
CMT_precision: 0.3711340206185567, CMT_recall: 0.5538461538461539, CMT_f1: 0.4444444444444445, CMT_number: 195
DSC_precision: 0.4511784511784512, DSC_recall: 0.6132723112128147, DSC_f1: 0.5198836081474297, DSC_number: 437
MAT_precision: 0.5655629139072847, MAT_recall: 0.626099706744868, MAT_f1: 0.594293667362561, MAT_number: 682
PRO_precision: 0.41895261845386533, PRO_recall: 0.4357976653696498, PRO_f1: 0.42720915448188174, PRO_number: 771
SMT_precision: 0.33980582524271846, SMT_recall: 0.6140350877192983, SMT_f1: 0.43750000000000006, SMT_number: 171
SPL_precision: 0.4025974025974026, SPL_recall: 0.41333333333333333, SPL_f1: 0.4078947368421053, SPL_number: 75
overall_precision: 0.4371354504212573, overall_recall: 0.5393842463014794, overall_f1: 0.48290674780741005, overall_accuracy: 0.830962761775427
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]