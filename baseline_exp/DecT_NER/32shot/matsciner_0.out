/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:34:28 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:34:29 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:64, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1186.84it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:34:47 - INFO - __main__ - ***** Running training *****
05/30/2023 12:34:47 - INFO - __main__ -   Num examples = 77
05/30/2023 12:34:47 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:34:47 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:34:47 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:34:47 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:34:47 - INFO - __main__ -   Total optimization steps = 105
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/105 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.982390403747559
Total epoch: 1. epoch loss: 15.416291236877441
Total epoch: 2. epoch loss: 14.862736701965332
Total epoch: 3. epoch loss: 14.32103443145752
Total epoch: 4. epoch loss: 13.790369033813477
Total epoch: 5. epoch loss: 13.270076751708984
Total epoch: 6. epoch loss: 12.759822845458984
Total epoch: 7. epoch loss: 12.259554862976074
Total epoch: 8. epoch loss: 11.769464492797852
Total epoch: 9. epoch loss: 11.290038108825684
Total epoch: 10. epoch loss: 10.821813583374023
Total epoch: 11. epoch loss: 10.365509033203125
Total epoch: 12. epoch loss: 9.921859741210938
Total epoch: 13. epoch loss: 9.49151611328125
Total epoch: 14. epoch loss: 9.075139999389648
Total epoch: 15. epoch loss: 8.673186302185059
Total epoch: 16. epoch loss: 8.286077499389648
Total epoch: 17. epoch loss: 7.914104461669922
Total epoch: 18. epoch loss: 7.5574235916137695
Total epoch: 19. epoch loss: 7.216114044189453
Total epoch: 20. epoch loss: 6.89009952545166
Total epoch: 21. epoch loss: 6.579243183135986
Total epoch: 22. epoch loss: 6.283174514770508
Total epoch: 23. epoch loss: 6.001481533050537
Total epoch: 24. epoch loss: 5.733555316925049
Total epoch: 25. epoch loss: 5.478725433349609
Total epoch: 26. epoch loss: 5.2363152503967285
Total epoch: 27. epoch loss: 5.005539894104004
Total epoch: 28. epoch loss: 4.785775184631348
Total epoch: 29. epoch loss: 4.5763044357299805
Total epoch: 30. epoch loss: 4.3768696784973145
Total epoch: 31. epoch loss: 4.187488079071045
Total epoch: 32. epoch loss: 4.008035182952881
Total epoch: 33. epoch loss: 3.838312864303589
Total epoch: 34. epoch loss: 3.678053379058838
Total epoch: 34. DecT loss: 3.678053379058838
Training time: 0.2887876033782959
APL_precision: 0.18493150684931506, APL_recall: 0.3176470588235294, APL_f1: 0.23376623376623376, APL_number: 170
CMT_precision: 0.15167095115681234, CMT_recall: 0.30256410256410254, CMT_f1: 0.20205479452054795, CMT_number: 195
DSC_precision: 0.3624382207578254, DSC_recall: 0.5034324942791762, DSC_f1: 0.421455938697318, DSC_number: 437
MAT_precision: 0.4383408071748879, MAT_recall: 0.5733137829912024, MAT_f1: 0.49682337992376113, MAT_number: 682
PRO_precision: 0.33699059561128525, PRO_recall: 0.5577172503242542, PRO_f1: 0.4201270151441133, PRO_number: 771
SMT_precision: 0.30578512396694213, SMT_recall: 0.6491228070175439, SMT_f1: 0.41573033707865165, SMT_number: 171
SPL_precision: 0.28205128205128205, SPL_recall: 0.44, SPL_f1: 0.34375, SPL_number: 75
overall_precision: 0.32977642276422764, overall_recall: 0.5189924030387845, overall_f1: 0.4032934596861892, overall_accuracy: 0.7802873275677221
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/105 [00:01<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:37:30 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:37:31 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:128, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1096.26it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:37:39 - INFO - __main__ - ***** Running training *****
05/30/2023 12:37:39 - INFO - __main__ -   Num examples = 77
05/30/2023 12:37:39 - INFO - __main__ -   Num Epochs = 100
05/30/2023 12:37:39 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:37:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:37:39 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:37:39 - INFO - __main__ -   Total optimization steps = 300
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.975959777832031
Total epoch: 1. epoch loss: 15.183961868286133
Total epoch: 2. epoch loss: 14.417271614074707
Total epoch: 3. epoch loss: 13.673482894897461
Total epoch: 4. epoch loss: 12.950556755065918
Total epoch: 5. epoch loss: 12.247699737548828
Total epoch: 6. epoch loss: 11.565361976623535
Total epoch: 7. epoch loss: 10.904932975769043
Total epoch: 8. epoch loss: 10.268260955810547
Total epoch: 9. epoch loss: 9.65732479095459
Total epoch: 10. epoch loss: 9.073999404907227
Total epoch: 11. epoch loss: 8.51983642578125
Total epoch: 12. epoch loss: 7.995975017547607
Total epoch: 13. epoch loss: 7.502962589263916
Total epoch: 14. epoch loss: 7.04082727432251
Total epoch: 15. epoch loss: 6.60901403427124
Total epoch: 16. epoch loss: 6.206570148468018
Total epoch: 17. epoch loss: 5.832021713256836
Total epoch: 18. epoch loss: 5.483608245849609
Total epoch: 19. epoch loss: 5.15931510925293
Total epoch: 20. epoch loss: 4.856998443603516
Total epoch: 21. epoch loss: 4.574618339538574
Total epoch: 22. epoch loss: 4.310876369476318
Total epoch: 23. epoch loss: 4.065097332000732
Total epoch: 24. epoch loss: 3.836543321609497
Total epoch: 25. epoch loss: 3.6243839263916016
Total epoch: 26. epoch loss: 3.427783727645874
Total epoch: 27. epoch loss: 3.245832681655884
Total epoch: 28. epoch loss: 3.077629327774048
Total epoch: 29. epoch loss: 2.9222922325134277
Total epoch: 30. epoch loss: 2.778932571411133
Total epoch: 31. epoch loss: 2.646681070327759
Total epoch: 32. epoch loss: 2.5246529579162598
Total epoch: 33. epoch loss: 2.4119932651519775
Total epoch: 34. epoch loss: 2.3078691959381104
Total epoch: 35. epoch loss: 2.211507558822632
Total epoch: 36. epoch loss: 2.1222116947174072
Total epoch: 37. epoch loss: 2.0393452644348145
Total epoch: 38. epoch loss: 1.9623756408691406
Total epoch: 39. epoch loss: 1.8908041715621948
Total epoch: 40. epoch loss: 1.8242051601409912
Total epoch: 41. epoch loss: 1.762187123298645
Total epoch: 42. epoch loss: 1.704378366470337
Total epoch: 43. epoch loss: 1.6504487991333008
Total epoch: 44. epoch loss: 1.6001036167144775
Total epoch: 45. epoch loss: 1.5530555248260498
Total epoch: 46. epoch loss: 1.5090417861938477
Total epoch: 47. epoch loss: 1.467819333076477
Total epoch: 48. epoch loss: 1.429173469543457
Total epoch: 49. epoch loss: 1.3928850889205933
Total epoch: 50. epoch loss: 1.3587777614593506
Total epoch: 51. epoch loss: 1.3266587257385254
Total epoch: 52. epoch loss: 1.2963789701461792
Total epoch: 53. epoch loss: 1.2677794694900513
Total epoch: 54. epoch loss: 1.2407408952713013
Total epoch: 55. epoch loss: 1.2151329517364502
Total epoch: 56. epoch loss: 1.1908515691757202
Total epoch: 57. epoch loss: 1.1677989959716797
Total epoch: 58. epoch loss: 1.1458865404129028
Total epoch: 59. epoch loss: 1.125031590461731
Total epoch: 60. epoch loss: 1.1051533222198486
Total epoch: 61. epoch loss: 1.0861889123916626
Total epoch: 62. epoch loss: 1.068073034286499
Total epoch: 63. epoch loss: 1.0507344007492065
Total epoch: 64. epoch loss: 1.0341415405273438
Total epoch: 65. epoch loss: 1.0182278156280518
Total epoch: 66. epoch loss: 1.0029466152191162
Total epoch: 67. epoch loss: 0.9882709383964539
Total epoch: 68. epoch loss: 0.9741513133049011
Total epoch: 69. epoch loss: 0.9605586528778076
Total epoch: 70. epoch loss: 0.9474635124206543
Total epoch: 71. epoch loss: 0.9348306655883789
Total epoch: 72. epoch loss: 0.9226433038711548
Total epoch: 73. epoch loss: 0.9108641743659973
Total epoch: 74. epoch loss: 0.8994776606559753
Total epoch: 75. epoch loss: 0.8884633779525757
Total epoch: 76. epoch loss: 0.87779700756073
Total epoch: 77. epoch loss: 0.8674602508544922
Total epoch: 78. epoch loss: 0.8574395775794983
Total epoch: 79. epoch loss: 0.8477140069007874
Total epoch: 80. epoch loss: 0.8382723331451416
Total epoch: 81. epoch loss: 0.8290983438491821
Total epoch: 82. epoch loss: 0.8201808929443359
Total epoch: 83. epoch loss: 0.8115037679672241
Total epoch: 84. epoch loss: 0.8030614852905273
Total epoch: 85. epoch loss: 0.7948421835899353
Total epoch: 86. epoch loss: 0.7868334054946899
Total epoch: 87. epoch loss: 0.779028058052063
Total epoch: 88. epoch loss: 0.771418571472168
Total epoch: 89. epoch loss: 0.7639920711517334
Total epoch: 90. epoch loss: 0.7567474246025085
Total epoch: 91. epoch loss: 0.7496675252914429
Total epoch: 92. epoch loss: 0.7427549958229065
Total epoch: 93. epoch loss: 0.7359986305236816
Total epoch: 94. epoch loss: 0.7293936014175415
Total epoch: 95. epoch loss: 0.7229365706443787
Total epoch: 96. epoch loss: 0.7166186571121216
Total epoch: 97. epoch loss: 0.710435688495636
Total epoch: 98. epoch loss: 0.7043805718421936
Total epoch: 99. epoch loss: 0.6984507441520691
Total epoch: 99. DecT loss: 0.6984507441520691
Training time: 0.4709482192993164
APL_precision: 0.3543307086614173, APL_recall: 0.5294117647058824, APL_f1: 0.42452830188679247, APL_number: 170
CMT_precision: 0.3202614379084967, CMT_recall: 0.5025641025641026, CMT_f1: 0.3912175648702595, CMT_number: 195
DSC_precision: 0.4876543209876543, DSC_recall: 0.5423340961098398, DSC_f1: 0.513542795232936, DSC_number: 437
MAT_precision: 0.5607940446650124, MAT_recall: 0.6627565982404692, MAT_f1: 0.60752688172043, MAT_number: 682
PRO_precision: 0.3834254143646409, PRO_recall: 0.45006485084306097, PRO_f1: 0.4140811455847256, PRO_number: 771
SMT_precision: 0.34965034965034963, SMT_recall: 0.5847953216374269, SMT_f1: 0.437636761487965, SMT_number: 171
SPL_precision: 0.3333333333333333, SPL_recall: 0.30666666666666664, SPL_f1: 0.3194444444444445, SPL_number: 75
overall_precision: 0.432840616966581, overall_recall: 0.5385845661735306, overall_f1: 0.47995724211651525, overall_accuracy: 0.830962761775427
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:41:02 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:41:03 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1215.74it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:41:09 - INFO - __main__ - ***** Running training *****
05/30/2023 12:41:09 - INFO - __main__ -   Num examples = 77
05/30/2023 12:41:09 - INFO - __main__ -   Num Epochs = 150
05/30/2023 12:41:09 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:41:09 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:41:09 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:41:09 - INFO - __main__ -   Total optimization steps = 450
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/450 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 15.94201946258545
Total epoch: 1. epoch loss: 15.058692932128906
Total epoch: 2. epoch loss: 14.206326484680176
Total epoch: 3. epoch loss: 13.38263988494873
Total epoch: 4. epoch loss: 12.585264205932617
Total epoch: 5. epoch loss: 11.813673973083496
Total epoch: 6. epoch loss: 11.069045066833496
Total epoch: 7. epoch loss: 10.3535795211792
Total epoch: 8. epoch loss: 9.66977310180664
Total epoch: 9. epoch loss: 9.020106315612793
Total epoch: 10. epoch loss: 8.406704902648926
Total epoch: 11. epoch loss: 7.831148147583008
Total epoch: 12. epoch loss: 7.294233322143555
Total epoch: 13. epoch loss: 6.795894145965576
Total epoch: 14. epoch loss: 6.335137367248535
Total epoch: 15. epoch loss: 5.910149097442627
Total epoch: 16. epoch loss: 5.518552780151367
Total epoch: 17. epoch loss: 5.157576084136963
Total epoch: 18. epoch loss: 4.824322700500488
Total epoch: 19. epoch loss: 4.515989303588867
Total epoch: 20. epoch loss: 4.2307329177856445
Total epoch: 21. epoch loss: 3.967229127883911
Total epoch: 22. epoch loss: 3.724100351333618
Total epoch: 23. epoch loss: 3.5000293254852295
Total epoch: 24. epoch loss: 3.2937722206115723
Total epoch: 25. epoch loss: 3.1041460037231445
Total epoch: 26. epoch loss: 2.9300129413604736
Total epoch: 27. epoch loss: 2.770263671875
Total epoch: 28. epoch loss: 2.623828411102295
Total epoch: 29. epoch loss: 2.4896485805511475
Total epoch: 30. epoch loss: 2.3666794300079346
Total epoch: 31. epoch loss: 2.253924608230591
Total epoch: 32. epoch loss: 2.150385856628418
Total epoch: 33. epoch loss: 2.055192470550537
Total epoch: 34. epoch loss: 1.9675220251083374
Total epoch: 35. epoch loss: 1.8866475820541382
Total epoch: 36. epoch loss: 1.8119399547576904
Total epoch: 37. epoch loss: 1.7428388595581055
Total epoch: 38. epoch loss: 1.6788653135299683
Total epoch: 39. epoch loss: 1.6195772886276245
Total epoch: 40. epoch loss: 1.5645928382873535
Total epoch: 41. epoch loss: 1.5135443210601807
Total epoch: 42. epoch loss: 1.4660966396331787
Total epoch: 43. epoch loss: 1.4219505786895752
Total epoch: 44. epoch loss: 1.380802035331726
Total epoch: 45. epoch loss: 1.3424112796783447
Total epoch: 46. epoch loss: 1.3065232038497925
Total epoch: 47. epoch loss: 1.272935390472412
Total epoch: 48. epoch loss: 1.2414404153823853
Total epoch: 49. epoch loss: 1.2118656635284424
Total epoch: 50. epoch loss: 1.1840413808822632
Total epoch: 51. epoch loss: 1.1578296422958374
Total epoch: 52. epoch loss: 1.1330797672271729
Total epoch: 53. epoch loss: 1.1096900701522827
Total epoch: 54. epoch loss: 1.0875388383865356
Total epoch: 55. epoch loss: 1.0665253400802612
Total epoch: 56. epoch loss: 1.0465744733810425
Total epoch: 57. epoch loss: 1.0275945663452148
Total epoch: 58. epoch loss: 1.0095171928405762
Total epoch: 59. epoch loss: 0.9922764897346497
Total epoch: 60. epoch loss: 0.9758180975914001
Total epoch: 61. epoch loss: 0.9600791931152344
Total epoch: 62. epoch loss: 0.9450104236602783
Total epoch: 63. epoch loss: 0.9305689334869385
Total epoch: 64. epoch loss: 0.916713297367096
Total epoch: 65. epoch loss: 0.9034094214439392
Total epoch: 66. epoch loss: 0.8906124830245972
Total epoch: 67. epoch loss: 0.8783004283905029
Total epoch: 68. epoch loss: 0.8664381504058838
Total epoch: 69. epoch loss: 0.8550012111663818
Total epoch: 70. epoch loss: 0.8439653515815735
Total epoch: 71. epoch loss: 0.8333044648170471
Total epoch: 72. epoch loss: 0.8229984045028687
Total epoch: 73. epoch loss: 0.8130278587341309
Total epoch: 74. epoch loss: 0.8033715486526489
Total epoch: 75. epoch loss: 0.7940188646316528
Total epoch: 76. epoch loss: 0.784948468208313
Total epoch: 77. epoch loss: 0.7761486172676086
Total epoch: 78. epoch loss: 0.7675994038581848
Total epoch: 79. epoch loss: 0.7592964768409729
Total epoch: 80. epoch loss: 0.7512266039848328
Total epoch: 81. epoch loss: 0.7433751225471497
Total epoch: 82. epoch loss: 0.7357340455055237
Total epoch: 83. epoch loss: 0.7282949686050415
Total epoch: 84. epoch loss: 0.7210444211959839
Total epoch: 85. epoch loss: 0.713979959487915
Total epoch: 86. epoch loss: 0.7070902585983276
Total epoch: 87. epoch loss: 0.7003688812255859
Total epoch: 88. epoch loss: 0.6938072443008423
Total epoch: 89. epoch loss: 0.6873990893363953
Total epoch: 90. epoch loss: 0.6811399459838867
Total epoch: 91. epoch loss: 0.6750211715698242
Total epoch: 92. epoch loss: 0.6690404415130615
Total epoch: 93. epoch loss: 0.6631902456283569
Total epoch: 94. epoch loss: 0.6574658155441284
Total epoch: 95. epoch loss: 0.6518604755401611
Total epoch: 96. epoch loss: 0.6463755369186401
Total epoch: 97. epoch loss: 0.6410011053085327
Total epoch: 98. epoch loss: 0.6357342004776001
Total epoch: 99. epoch loss: 0.6305755972862244
Total epoch: 100. epoch loss: 0.6255164742469788
Total epoch: 101. epoch loss: 0.6205552220344543
Total epoch: 102. epoch loss: 0.6156890988349915
Total epoch: 103. epoch loss: 0.6109152436256409
Total epoch: 104. epoch loss: 0.6062287092208862
Total epoch: 105. epoch loss: 0.6016293168067932
Total epoch: 106. epoch loss: 0.5971139073371887
Total epoch: 107. epoch loss: 0.5926791429519653
Total epoch: 108. epoch loss: 0.5883209705352783
Total epoch: 109. epoch loss: 0.5840418934822083
Total epoch: 110. epoch loss: 0.5798349380493164
Total epoch: 111. epoch loss: 0.5756989121437073
Total epoch: 112. epoch loss: 0.5716336369514465
Total epoch: 113. epoch loss: 0.5676367282867432
Total epoch: 114. epoch loss: 0.5637036561965942
Total epoch: 115. epoch loss: 0.5598394870758057
Total epoch: 116. epoch loss: 0.5560363531112671
Total epoch: 117. epoch loss: 0.5522933602333069
Total epoch: 118. epoch loss: 0.5486074090003967
Total epoch: 119. epoch loss: 0.5449822545051575
Total epoch: 120. epoch loss: 0.5414146184921265
Total epoch: 121. epoch loss: 0.5379003286361694
Total epoch: 122. epoch loss: 0.534437894821167
Total epoch: 123. epoch loss: 0.5310297608375549
Total epoch: 124. epoch loss: 0.5276727676391602
Total epoch: 125. epoch loss: 0.5243659615516663
Total epoch: 126. epoch loss: 0.5211067795753479
Total epoch: 127. epoch loss: 0.5178946852684021
Total epoch: 128. epoch loss: 0.5147308707237244
Total epoch: 129. epoch loss: 0.5116139054298401
Total epoch: 130. epoch loss: 0.5085387825965881
Total epoch: 131. epoch loss: 0.5055071711540222
Total epoch: 132. epoch loss: 0.5025203824043274
Total epoch: 133. epoch loss: 0.49957141280174255
Total epoch: 134. epoch loss: 0.4966663718223572
Total epoch: 135. epoch loss: 0.4937983751296997
Total epoch: 136. epoch loss: 0.49096983671188354
Total epoch: 137. epoch loss: 0.48818057775497437
Total epoch: 138. epoch loss: 0.4854280650615692
Total epoch: 139. epoch loss: 0.4827131927013397
Total epoch: 140. epoch loss: 0.4800330102443695
Total epoch: 141. epoch loss: 0.4773881733417511
Total epoch: 142. epoch loss: 0.4747774004936218
Total epoch: 143. epoch loss: 0.472200483083725
Total epoch: 144. epoch loss: 0.4696570038795471
Total epoch: 145. epoch loss: 0.4671440124511719
Total epoch: 146. epoch loss: 0.4646664559841156
Total epoch: 147. epoch loss: 0.4622165858745575
Total epoch: 148. epoch loss: 0.4597981572151184
Total epoch: 149. epoch loss: 0.4574105143547058
Total epoch: 149. DecT loss: 0.4574105143547058
Training time: 0.7610251903533936
APL_precision: 0.36213991769547327, APL_recall: 0.5176470588235295, APL_f1: 0.4261501210653753, APL_number: 170
CMT_precision: 0.3848797250859107, CMT_recall: 0.5743589743589743, CMT_f1: 0.4609053497942387, CMT_number: 195
DSC_precision: 0.5230769230769231, DSC_recall: 0.5446224256292906, DSC_f1: 0.5336322869955157, DSC_number: 437
MAT_precision: 0.5935897435897436, MAT_recall: 0.6788856304985337, MAT_f1: 0.6333789329685362, MAT_number: 682
PRO_precision: 0.3811764705882353, PRO_recall: 0.42023346303501946, PRO_f1: 0.3997532387415176, PRO_number: 771
SMT_precision: 0.36964980544747084, SMT_recall: 0.5555555555555556, SMT_f1: 0.44392523364485986, SMT_number: 171
SPL_precision: 0.3448275862068966, SPL_recall: 0.26666666666666666, SPL_f1: 0.30075187969924816, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.4567143830947512, overall_recall: 0.5357856857257097, overall_f1: 0.49310027598896045, overall_accuracy: 0.8355371310127939
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/450 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:14:51 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:14:53 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:150,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:2.0, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1141.00it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5023.43 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:14:58 - INFO - __main__ - ***** Running training *****
05/31/2023 13:14:58 - INFO - __main__ -   Num examples = 77
05/31/2023 13:14:58 - INFO - __main__ -   Num Epochs = 150
05/31/2023 13:14:58 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:14:58 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:14:58 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:14:58 - INFO - __main__ -   Total optimization steps = 450
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/450 [00:00<?, ?it/s]Total epoch: 0. epoch loss: 16.436546325683594
Total epoch: 1. epoch loss: 15.535133361816406
Total epoch: 2. epoch loss: 14.651430130004883
Total epoch: 3. epoch loss: 13.787853240966797
Total epoch: 4. epoch loss: 12.949188232421875
Total epoch: 5. epoch loss: 12.139789581298828
Total epoch: 6. epoch loss: 11.362425804138184
Total epoch: 7. epoch loss: 10.618741035461426
Total epoch: 8. epoch loss: 9.91025161743164
Total epoch: 9. epoch loss: 9.238621711730957
Total epoch: 10. epoch loss: 8.605589866638184
Total epoch: 11. epoch loss: 8.012556076049805
Total epoch: 12. epoch loss: 7.460178375244141
Total epoch: 13. epoch loss: 6.948209285736084
Total epoch: 14. epoch loss: 6.475415229797363
Total epoch: 15. epoch loss: 6.039700984954834
Total epoch: 16. epoch loss: 5.638332366943359
Total epoch: 17. epoch loss: 5.268219947814941
Total epoch: 18. epoch loss: 4.926290035247803
Total epoch: 19. epoch loss: 4.60966157913208
Total epoch: 20. epoch loss: 4.3161821365356445
Total epoch: 21. epoch loss: 4.0447998046875
Total epoch: 22. epoch loss: 3.7942824363708496
Total epoch: 23. epoch loss: 3.5633857250213623
Total epoch: 24. epoch loss: 3.350942373275757
Total epoch: 25. epoch loss: 3.155797243118286
Total epoch: 26. epoch loss: 2.9768006801605225
Total epoch: 27. epoch loss: 2.812844753265381
Total epoch: 28. epoch loss: 2.662785053253174
Total epoch: 29. epoch loss: 2.525470733642578
Total epoch: 30. epoch loss: 2.399750232696533
Total epoch: 31. epoch loss: 2.2845284938812256
Total epoch: 32. epoch loss: 2.178760051727295
Total epoch: 33. epoch loss: 2.0815038681030273
Total epoch: 34. epoch loss: 1.9919297695159912
Total epoch: 35. epoch loss: 1.9092682600021362
Total epoch: 36. epoch loss: 1.8329156637191772
Total epoch: 37. epoch loss: 1.7623207569122314
Total epoch: 38. epoch loss: 1.6969918012619019
Total epoch: 39. epoch loss: 1.6365151405334473
Total epoch: 40. epoch loss: 1.5804826021194458
Total epoch: 41. epoch loss: 1.5285286903381348
Total epoch: 42. epoch loss: 1.4802929162979126
Total epoch: 43. epoch loss: 1.4354532957077026
Total epoch: 44. epoch loss: 1.3937184810638428
Total epoch: 45. epoch loss: 1.3548030853271484
Total epoch: 46. epoch loss: 1.318447470664978
Total epoch: 47. epoch loss: 1.284443974494934
Total epoch: 48. epoch loss: 1.252564787864685
Total epoch: 49. epoch loss: 1.222629189491272
Total epoch: 50. epoch loss: 1.1944804191589355
Total epoch: 51. epoch loss: 1.1679612398147583
Total epoch: 52. epoch loss: 1.142938494682312
Total epoch: 53. epoch loss: 1.1192891597747803
Total epoch: 54. epoch loss: 1.0969158411026
Total epoch: 55. epoch loss: 1.0757007598876953
Total epoch: 56. epoch loss: 1.055568814277649
Total epoch: 57. epoch loss: 1.0364298820495605
Total epoch: 58. epoch loss: 1.018216848373413
Total epoch: 59. epoch loss: 1.0008540153503418
Total epoch: 60. epoch loss: 0.9842866659164429
Total epoch: 61. epoch loss: 0.9684550166130066
Total epoch: 62. epoch loss: 0.9532984495162964
Total epoch: 63. epoch loss: 0.9387862682342529
Total epoch: 64. epoch loss: 0.9248661398887634
Total epoch: 65. epoch loss: 0.911491334438324
Total epoch: 66. epoch loss: 0.8986402153968811
Total epoch: 67. epoch loss: 0.8862690329551697
Total epoch: 68. epoch loss: 0.8743516802787781
Total epoch: 69. epoch loss: 0.8628632426261902
Total epoch: 70. epoch loss: 0.8517780900001526
Total epoch: 71. epoch loss: 0.8410739302635193
Total epoch: 72. epoch loss: 0.8307240605354309
Total epoch: 73. epoch loss: 0.8207134008407593
Total epoch: 74. epoch loss: 0.8110238909721375
Total epoch: 75. epoch loss: 0.8016330003738403
Total epoch: 76. epoch loss: 0.7925335168838501
Total epoch: 77. epoch loss: 0.7837056517601013
Total epoch: 78. epoch loss: 0.7751341462135315
Total epoch: 79. epoch loss: 0.7668077349662781
Total epoch: 80. epoch loss: 0.7587178945541382
Total epoch: 81. epoch loss: 0.7508476972579956
Total epoch: 82. epoch loss: 0.7431871891021729
Total epoch: 83. epoch loss: 0.7357305288314819
Total epoch: 84. epoch loss: 0.7284651398658752
Total epoch: 85. epoch loss: 0.7213830947875977
Total epoch: 86. epoch loss: 0.7144812345504761
Total epoch: 87. epoch loss: 0.7077448964118958
Total epoch: 88. epoch loss: 0.7011696696281433
Total epoch: 89. epoch loss: 0.6947489380836487
Total epoch: 90. epoch loss: 0.6884768605232239
Total epoch: 91. epoch loss: 0.682349443435669
Total epoch: 92. epoch loss: 0.6763588190078735
Total epoch: 93. epoch loss: 0.6704961657524109
Total epoch: 94. epoch loss: 0.6647655367851257
Total epoch: 95. epoch loss: 0.6591539978981018
Total epoch: 96. epoch loss: 0.6536608338356018
Total epoch: 97. epoch loss: 0.6482794284820557
Total epoch: 98. epoch loss: 0.6430084109306335
Total epoch: 99. epoch loss: 0.6378421187400818
Total epoch: 100. epoch loss: 0.6327763199806213
Total epoch: 101. epoch loss: 0.6278114318847656
Total epoch: 102. epoch loss: 0.6229382753372192
Total epoch: 103. epoch loss: 0.6181603670120239
Total epoch: 104. epoch loss: 0.6134687066078186
Total epoch: 105. epoch loss: 0.6088644862174988
Total epoch: 106. epoch loss: 0.6043421626091003
Total epoch: 107. epoch loss: 0.5999042391777039
Total epoch: 108. epoch loss: 0.5955419540405273
Total epoch: 109. epoch loss: 0.5912573933601379
Total epoch: 110. epoch loss: 0.587047815322876
Total epoch: 111. epoch loss: 0.5829094648361206
Total epoch: 112. epoch loss: 0.5788400173187256
Total epoch: 113. epoch loss: 0.5748398900032043
Total epoch: 114. epoch loss: 0.5709053874015808
Total epoch: 115. epoch loss: 0.567034125328064
Total epoch: 116. epoch loss: 0.5632288455963135
Total epoch: 117. epoch loss: 0.5594813227653503
Total epoch: 118. epoch loss: 0.5557945370674133
Total epoch: 119. epoch loss: 0.5521669387817383
Total epoch: 120. epoch loss: 0.5485938191413879
Total epoch: 121. epoch loss: 0.5450754761695862
Total epoch: 122. epoch loss: 0.5416131019592285
Total epoch: 123. epoch loss: 0.5382020473480225
Total epoch: 124. epoch loss: 0.534841775894165
Total epoch: 125. epoch loss: 0.5315325856208801
Total epoch: 126. epoch loss: 0.5282695889472961
Total epoch: 127. epoch loss: 0.5250563025474548
Total epoch: 128. epoch loss: 0.5218886137008667
Total epoch: 129. epoch loss: 0.5187655091285706
Total epoch: 130. epoch loss: 0.5156888365745544
Total epoch: 131. epoch loss: 0.5126552581787109
Total epoch: 132. epoch loss: 0.5096638798713684
Total epoch: 133. epoch loss: 0.5067143440246582
Total epoch: 134. epoch loss: 0.5038056969642639
Total epoch: 135. epoch loss: 0.5009365677833557
Total epoch: 136. epoch loss: 0.4981034994125366
Total epoch: 137. epoch loss: 0.49531155824661255
Total epoch: 138. epoch loss: 0.4925571084022522
Total epoch: 139. epoch loss: 0.4898377060890198
Total epoch: 140. epoch loss: 0.487155944108963
Total epoch: 141. epoch loss: 0.48450785875320435
Total epoch: 142. epoch loss: 0.48189541697502136
Total epoch: 143. epoch loss: 0.4793155789375305
Total epoch: 144. epoch loss: 0.4767700433731079
Total epoch: 145. epoch loss: 0.47425368428230286
Total epoch: 146. epoch loss: 0.4717714488506317
Total epoch: 147. epoch loss: 0.4693197011947632
Total epoch: 148. epoch loss: 0.4669007658958435
Total epoch: 149. epoch loss: 0.464508593082428
Total epoch: 149. DecT loss: 0.464508593082428
Training time: 0.9533798694610596
APL_precision: 0.34509803921568627, APL_recall: 0.5176470588235295, APL_f1: 0.41411764705882353, APL_number: 170
CMT_precision: 0.36303630363036304, CMT_recall: 0.5641025641025641, CMT_f1: 0.4417670682730924, CMT_number: 195
DSC_precision: 0.5117773019271948, DSC_recall: 0.5469107551487414, DSC_f1: 0.5287610619469026, DSC_number: 437
MAT_precision: 0.6007802340702211, MAT_recall: 0.6774193548387096, MAT_f1: 0.636802205375603, MAT_number: 682
PRO_precision: 0.39281437125748503, PRO_recall: 0.4254215304798962, PRO_f1: 0.4084682440846824, PRO_number: 771
SMT_precision: 0.34767025089605735, SMT_recall: 0.5672514619883041, SMT_f1: 0.43111111111111117, SMT_number: 171
SPL_precision: 0.3770491803278688, SPL_recall: 0.30666666666666664, SPL_f1: 0.338235294117647, SPL_number: 75
/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
overall_precision: 0.45368811047490737, overall_recall: 0.5385845661735306, overall_f1: 0.4925045703839122, overall_accuracy: 0.8364662997641341
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/450 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:25:02 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:25:03 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1034.99it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4988.66 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:25:39 - INFO - __main__ - ***** Running training *****
05/31/2023 13:25:39 - INFO - __main__ -   Num examples = 77
05/31/2023 13:25:39 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:25:39 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:25:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:25:39 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:25:39 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.96668529510498
Total epoch: 1. epoch loss: 15.343281745910645
Total epoch: 2. epoch loss: 14.48888111114502
Total epoch: 3. epoch loss: 13.301482200622559
Total epoch: 4. epoch loss: 11.861408233642578
Total epoch: 5. epoch loss: 10.323192596435547
Total epoch: 6. epoch loss: 8.828814506530762
Total epoch: 7. epoch loss: 7.459644317626953
Total epoch: 8. epoch loss: 6.255162239074707
Total epoch: 9. epoch loss: 5.236095428466797
Total epoch: 10. epoch loss: 4.387399673461914
Total epoch: 11. epoch loss: 3.6693012714385986
Total epoch: 12. epoch loss: 3.0554728507995605
Total epoch: 13. epoch loss: 2.54232120513916
Total epoch: 14. epoch loss: 2.13339900970459
Total epoch: 15. epoch loss: 1.8144317865371704
Total epoch: 16. epoch loss: 1.5614521503448486
Total epoch: 17. epoch loss: 1.3580597639083862
Total epoch: 18. epoch loss: 1.1956194639205933
Total epoch: 19. epoch loss: 1.0675522089004517
Total epoch: 20. epoch loss: 0.9667906761169434
Total epoch: 21. epoch loss: 0.8853102326393127
Total epoch: 22. epoch loss: 0.8173081874847412
Total epoch: 23. epoch loss: 0.7607133984565735
Total epoch: 24. epoch loss: 0.7142630219459534
Total epoch: 25. epoch loss: 0.6754262447357178
Total epoch: 26. epoch loss: 0.641385555267334
Total epoch: 27. epoch loss: 0.6104782223701477
Total epoch: 28. epoch loss: 0.582254946231842
Total epoch: 29. epoch loss: 0.5567018389701843
Total epoch: 30. epoch loss: 0.5336326360702515
Total epoch: 31. epoch loss: 0.5126199126243591
Total epoch: 32. epoch loss: 0.49329107999801636
Total epoch: 33. epoch loss: 0.47535452246665955
Total epoch: 34. epoch loss: 0.45860376954078674
Total epoch: 35. epoch loss: 0.44287538528442383
Total epoch: 36. epoch loss: 0.4280506670475006
Total epoch: 37. epoch loss: 0.4141029715538025
Total epoch: 38. epoch loss: 0.4010010361671448
Total epoch: 39. epoch loss: 0.3887025713920593
Total epoch: 40. epoch loss: 0.37715813517570496
Total epoch: 41. epoch loss: 0.3663136959075928
Total epoch: 42. epoch loss: 0.3561367690563202
Total epoch: 43. epoch loss: 0.3465978503227234
Total epoch: 44. epoch loss: 0.3376462757587433
Total epoch: 45. epoch loss: 0.32918161153793335
Total epoch: 46. epoch loss: 0.3211202323436737
Total epoch: 47. epoch loss: 0.3134070038795471
Total epoch: 48. epoch loss: 0.30598726868629456
Total epoch: 49. epoch loss: 0.2988302707672119
Total epoch: 50. epoch loss: 0.29192450642585754
Total epoch: 51. epoch loss: 0.28525519371032715
Total epoch: 52. epoch loss: 0.27883151173591614
Total epoch: 53. epoch loss: 0.2726716995239258
Total epoch: 54. epoch loss: 0.2667931020259857
Total epoch: 55. epoch loss: 0.26119130849838257
Total epoch: 56. epoch loss: 0.2558501660823822
Total epoch: 57. epoch loss: 0.25074130296707153
Total epoch: 58. epoch loss: 0.245840385556221
Total epoch: 59. epoch loss: 0.24111182987689972
Total epoch: 60. epoch loss: 0.23653629422187805
Total epoch: 61. epoch loss: 0.23210085928440094
Total epoch: 62. epoch loss: 0.22779159247875214
Total epoch: 63. epoch loss: 0.22360827028751373
Total epoch: 64. epoch loss: 0.21955575048923492
Total epoch: 65. epoch loss: 0.2156219780445099
Total epoch: 66. epoch loss: 0.21181413531303406
Total epoch: 67. epoch loss: 0.20813128352165222
Total epoch: 68. epoch loss: 0.20457573235034943
Total epoch: 69. epoch loss: 0.20115616917610168
Total epoch: 70. epoch loss: 0.19787827134132385
Total epoch: 71. epoch loss: 0.1947304606437683
Total epoch: 72. epoch loss: 0.19170436263084412
Total epoch: 73. epoch loss: 0.18878579139709473
Total epoch: 74. epoch loss: 0.18596751987934113
Total epoch: 75. epoch loss: 0.1832401603460312
Total epoch: 76. epoch loss: 0.1806049644947052
Total epoch: 77. epoch loss: 0.17805276811122894
Total epoch: 78. epoch loss: 0.17558631300926208
Total epoch: 79. epoch loss: 0.17319922149181366
Total epoch: 80. epoch loss: 0.17088967561721802
Total epoch: 81. epoch loss: 0.16865333914756775
Total epoch: 82. epoch loss: 0.1664867103099823
Total epoch: 83. epoch loss: 0.1643899381160736
Total epoch: 84. epoch loss: 0.16235385835170746
Total epoch: 85. epoch loss: 0.16038110852241516
Total epoch: 86. epoch loss: 0.15846844017505646
Total epoch: 87. epoch loss: 0.15661746263504028
Total epoch: 88. epoch loss: 0.15482769906520844
Total epoch: 89. epoch loss: 0.1530943661928177
Total epoch: 90. epoch loss: 0.15141561627388
Total epoch: 91. epoch loss: 0.14979645609855652
Total epoch: 92. epoch loss: 0.14822642505168915
Total epoch: 93. epoch loss: 0.14670751988887787
Total epoch: 94. epoch loss: 0.1452362984418869
Total epoch: 95. epoch loss: 0.1438106894493103
Total epoch: 96. epoch loss: 0.14243237674236298
Total epoch: 97. epoch loss: 0.14109393954277039
Total epoch: 98. epoch loss: 0.13979683816432953
Total epoch: 99. epoch loss: 0.13854028284549713
Total epoch: 99. DecT loss: 0.13854028284549713
Training time: 0.794151782989502
APL_precision: 0.33454545454545453, APL_recall: 0.5411764705882353, APL_f1: 0.4134831460674157, APL_number: 170
CMT_precision: 0.3856655290102389, CMT_recall: 0.5794871794871795, CMT_f1: 0.4631147540983606, CMT_number: 195
DSC_precision: 0.5802816901408451, DSC_recall: 0.47139588100686497, DSC_f1: 0.5202020202020202, DSC_number: 437
MAT_precision: 0.6, MAT_recall: 0.5850439882697948, MAT_f1: 0.5924276169265034, MAT_number: 682
PRO_precision: 0.3424460431654676, PRO_recall: 0.3086900129701686, PRO_f1: 0.32469304229195084, PRO_number: 771
SMT_precision: 0.3212669683257919, SMT_recall: 0.4152046783625731, SMT_f1: 0.3622448979591837, SMT_number: 171
SPL_precision: 0.41935483870967744, SPL_recall: 0.17333333333333334, SPL_f1: 0.2452830188679245, SPL_number: 75
overall_precision: 0.4465483234714004, overall_recall: 0.4526189524190324, overall_f1: 0.4495631453534551, overall_accuracy: 0.8216710742620256
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 489, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:28:17 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:28:21 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:0 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1112.10it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4891.15 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:28:27 - INFO - __main__ - ***** Running training *****
05/31/2023 13:28:27 - INFO - __main__ -   Num examples = 77
05/31/2023 13:28:27 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:28:27 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:28:27 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:28:27 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:28:27 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.9409818649292
Total epoch: 1. epoch loss: 15.053805351257324
Total epoch: 2. epoch loss: 14.200231552124023
Total epoch: 3. epoch loss: 13.375823974609375
Total epoch: 4. epoch loss: 12.57744026184082
Total epoch: 5. epoch loss: 11.804378509521484
Total epoch: 6. epoch loss: 11.05794906616211
Total epoch: 7. epoch loss: 10.340509414672852
Total epoch: 8. epoch loss: 9.654802322387695
Total epoch: 9. epoch loss: 9.003488540649414
Total epoch: 10. epoch loss: 8.388853073120117
Total epoch: 11. epoch loss: 7.812455177307129
Total epoch: 12. epoch loss: 7.275101661682129
Total epoch: 13. epoch loss: 6.776642799377441
Total epoch: 14. epoch loss: 6.316058158874512
Total epoch: 15. epoch loss: 5.891521453857422
Total epoch: 16. epoch loss: 5.5005974769592285
Total epoch: 17. epoch loss: 5.140488624572754
Total epoch: 18. epoch loss: 4.808183670043945
Total epoch: 19. epoch loss: 4.500732898712158
Total epoch: 20. epoch loss: 4.216327667236328
Total epoch: 21. epoch loss: 3.9535739421844482
Total epoch: 22. epoch loss: 3.711153030395508
Total epoch: 23. epoch loss: 3.4877421855926514
Total epoch: 24. epoch loss: 3.282092332839966
Total epoch: 25. epoch loss: 3.0930519104003906
Total epoch: 26. epoch loss: 2.919454336166382
Total epoch: 27. epoch loss: 2.7602195739746094
Total epoch: 28. epoch loss: 2.614267587661743
Total epoch: 29. epoch loss: 2.4805729389190674
Total epoch: 30. epoch loss: 2.3580827713012695
Total epoch: 31. epoch loss: 2.24580454826355
Total epoch: 32. epoch loss: 2.1427760124206543
Total epoch: 33. epoch loss: 2.048076868057251
Total epoch: 34. epoch loss: 1.9608900547027588
Total epoch: 35. epoch loss: 1.880480408668518
Total epoch: 36. epoch loss: 1.8061950206756592
Total epoch: 37. epoch loss: 1.7374897003173828
Total epoch: 38. epoch loss: 1.6738659143447876
Total epoch: 39. epoch loss: 1.614893913269043
Total epoch: 40. epoch loss: 1.5601825714111328
Total epoch: 41. epoch loss: 1.5093835592269897
Total epoch: 42. epoch loss: 1.4621646404266357
Total epoch: 43. epoch loss: 1.4182190895080566
Total epoch: 44. epoch loss: 1.3772739171981812
Total epoch: 45. epoch loss: 1.3390804529190063
Total epoch: 46. epoch loss: 1.3033866882324219
Total epoch: 47. epoch loss: 1.269993543624878
Total epoch: 48. epoch loss: 1.2386919260025024
Total epoch: 49. epoch loss: 1.2093048095703125
Total epoch: 50. epoch loss: 1.1816612482070923
Total epoch: 51. epoch loss: 1.1556096076965332
Total epoch: 52. epoch loss: 1.131007194519043
Total epoch: 53. epoch loss: 1.107742428779602
Total epoch: 54. epoch loss: 1.0856980085372925
Total epoch: 55. epoch loss: 1.064793586730957
Total epoch: 56. epoch loss: 1.0449286699295044
Total epoch: 57. epoch loss: 1.0260334014892578
Total epoch: 58. epoch loss: 1.008033275604248
Total epoch: 59. epoch loss: 0.9908732771873474
Total epoch: 60. epoch loss: 0.9744903445243835
Total epoch: 61. epoch loss: 0.9588353037834167
Total epoch: 62. epoch loss: 0.9438432455062866
Total epoch: 63. epoch loss: 0.9294850826263428
Total epoch: 64. epoch loss: 0.9157119393348694
Total epoch: 65. epoch loss: 0.902481198310852
Total epoch: 66. epoch loss: 0.8897647261619568
Total epoch: 67. epoch loss: 0.8775190711021423
Total epoch: 68. epoch loss: 0.8657181262969971
Total epoch: 69. epoch loss: 0.8543423414230347
Total epoch: 70. epoch loss: 0.8433588743209839
Total epoch: 71. epoch loss: 0.8327451944351196
Total epoch: 72. epoch loss: 0.8224880695343018
Total epoch: 73. epoch loss: 0.8125576972961426
Total epoch: 74. epoch loss: 0.8029448390007019
Total epoch: 75. epoch loss: 0.7936370372772217
Total epoch: 76. epoch loss: 0.7846043109893799
Total epoch: 77. epoch loss: 0.7758437991142273
Total epoch: 78. epoch loss: 0.7673393487930298
Total epoch: 79. epoch loss: 0.7590729594230652
Total epoch: 80. epoch loss: 0.7510398626327515
Total epoch: 81. epoch loss: 0.7432234287261963
Total epoch: 82. epoch loss: 0.7356188893318176
Total epoch: 83. epoch loss: 0.7282090187072754
Total epoch: 84. epoch loss: 0.7209933996200562
Total epoch: 85. epoch loss: 0.7139559984207153
Total epoch: 86. epoch loss: 0.7070945501327515
Total epoch: 87. epoch loss: 0.7003992199897766
Total epoch: 88. epoch loss: 0.6938603520393372
Total epoch: 89. epoch loss: 0.6874799132347107
Total epoch: 90. epoch loss: 0.6812424063682556
Total epoch: 91. epoch loss: 0.675146222114563
Total epoch: 92. epoch loss: 0.6691864728927612
Total epoch: 93. epoch loss: 0.6633540987968445
Total epoch: 94. epoch loss: 0.6576507091522217
Total epoch: 95. epoch loss: 0.6520665884017944
Total epoch: 96. epoch loss: 0.6465962529182434
Total epoch: 97. epoch loss: 0.6412421464920044
Total epoch: 98. epoch loss: 0.6359931230545044
Total epoch: 99. epoch loss: 0.6308474540710449
Total epoch: 99. DecT loss: 0.6308474540710449
Training time: 0.7381649017333984
APL_precision: 0.3699186991869919, APL_recall: 0.5352941176470588, APL_f1: 0.4375, APL_number: 170
CMT_precision: 0.32558139534883723, CMT_recall: 0.5025641025641026, CMT_f1: 0.3951612903225806, CMT_number: 195
DSC_precision: 0.5, DSC_recall: 0.5446224256292906, DSC_f1: 0.5213581599123768, DSC_number: 437
MAT_precision: 0.5643939393939394, MAT_recall: 0.655425219941349, MAT_f1: 0.6065128900949797, MAT_number: 682
PRO_precision: 0.3853932584269663, PRO_recall: 0.44487678339818415, PRO_f1: 0.4130042143287176, PRO_number: 771
SMT_precision: 0.3531468531468531, SMT_recall: 0.5906432748538012, SMT_f1: 0.4420131291028447, SMT_number: 171
SPL_precision: 0.359375, SPL_recall: 0.30666666666666664, SPL_f1: 0.33093525179856115, SPL_number: 75
overall_precision: 0.4389525368248772, overall_recall: 0.5361855257896841, overall_f1: 0.48272138228941686, overall_accuracy: 0.8328210992781073
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:30:35 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:30:37 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1179.00it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4702.79 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:31:12 - INFO - __main__ - ***** Running training *****
05/31/2023 13:31:12 - INFO - __main__ -   Num examples = 77
05/31/2023 13:31:12 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:31:12 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:31:12 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:31:12 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:31:12 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.9409818649292
Total epoch: 1. epoch loss: 15.053810119628906
Total epoch: 2. epoch loss: 14.200237274169922
Total epoch: 3. epoch loss: 13.375823974609375
Total epoch: 4. epoch loss: 12.577428817749023
Total epoch: 5. epoch loss: 11.80435562133789
Total epoch: 6. epoch loss: 11.057917594909668
Total epoch: 7. epoch loss: 10.340472221374512
Total epoch: 8. epoch loss: 9.654769897460938
Total epoch: 9. epoch loss: 9.003457069396973
Total epoch: 10. epoch loss: 8.388825416564941
Total epoch: 11. epoch loss: 7.812440872192383
Total epoch: 12. epoch loss: 7.275094032287598
Total epoch: 13. epoch loss: 6.776645660400391
Total epoch: 14. epoch loss: 6.316060543060303
Total epoch: 15. epoch loss: 5.8915276527404785
Total epoch: 16. epoch loss: 5.5006022453308105
Total epoch: 17. epoch loss: 5.140481948852539
Total epoch: 18. epoch loss: 4.808163166046143
Total epoch: 19. epoch loss: 4.500694274902344
Total epoch: 20. epoch loss: 4.2162675857543945
Total epoch: 21. epoch loss: 3.953488826751709
Total epoch: 22. epoch loss: 3.711043119430542
Total epoch: 23. epoch loss: 3.487621545791626
Total epoch: 24. epoch loss: 3.2819671630859375
Total epoch: 25. epoch loss: 3.0929293632507324
Total epoch: 26. epoch loss: 2.919341564178467
Total epoch: 27. epoch loss: 2.7601208686828613
Total epoch: 28. epoch loss: 2.614189624786377
Total epoch: 29. epoch loss: 2.480522394180298
Total epoch: 30. epoch loss: 2.358067035675049
Total epoch: 31. epoch loss: 2.2458243370056152
Total epoch: 32. epoch loss: 2.142832040786743
Total epoch: 33. epoch loss: 2.0481724739074707
Total epoch: 34. epoch loss: 1.9610260725021362
Total epoch: 35. epoch loss: 1.8806573152542114
Total epoch: 36. epoch loss: 1.8064148426055908
Total epoch: 37. epoch loss: 1.7377530336380005
Total epoch: 38. epoch loss: 1.674165964126587
Total epoch: 39. epoch loss: 1.6152430772781372
Total epoch: 40. epoch loss: 1.5605803728103638
Total epoch: 41. epoch loss: 1.5098249912261963
Total epoch: 42. epoch loss: 1.462654709815979
Total epoch: 43. epoch loss: 1.418760061264038
Total epoch: 44. epoch loss: 1.3778620958328247
Total epoch: 45. epoch loss: 1.3397091627120972
Total epoch: 46. epoch loss: 1.3040733337402344
Total epoch: 47. epoch loss: 1.2707279920578003
Total epoch: 48. epoch loss: 1.239480972290039
Total epoch: 49. epoch loss: 1.2101447582244873
Total epoch: 50. epoch loss: 1.1825469732284546
Total epoch: 51. epoch loss: 1.1565426588058472
Total epoch: 52. epoch loss: 1.1319935321807861
Total epoch: 53. epoch loss: 1.1087733507156372
Total epoch: 54. epoch loss: 1.086782693862915
Total epoch: 55. epoch loss: 1.0659191608428955
Total epoch: 56. epoch loss: 1.0461033582687378
Total epoch: 57. epoch loss: 1.02725088596344
Total epoch: 58. epoch loss: 1.0093019008636475
Total epoch: 59. epoch loss: 0.9921916723251343
Total epoch: 60. epoch loss: 0.9758549928665161
Total epoch: 61. epoch loss: 0.9602422118186951
Total epoch: 62. epoch loss: 0.9453052282333374
Total epoch: 63. epoch loss: 0.9309924244880676
Total epoch: 64. epoch loss: 0.9172652959823608
Total epoch: 65. epoch loss: 0.904083251953125
Total epoch: 66. epoch loss: 0.8914090991020203
Total epoch: 67. epoch loss: 0.8792087435722351
Total epoch: 68. epoch loss: 0.8674600720405579
Total epoch: 69. epoch loss: 0.8561279773712158
Total epoch: 70. epoch loss: 0.8451909422874451
Total epoch: 71. epoch loss: 0.8346253037452698
Total epoch: 72. epoch loss: 0.824410617351532
Total epoch: 73. epoch loss: 0.8145330548286438
Total epoch: 74. epoch loss: 0.8049657940864563
Total epoch: 75. epoch loss: 0.7957022786140442
Total epoch: 76. epoch loss: 0.786716878414154
Total epoch: 77. epoch loss: 0.7780026793479919
Total epoch: 78. epoch loss: 0.7695411443710327
Total epoch: 79. epoch loss: 0.7613245844841003
Total epoch: 80. epoch loss: 0.7533336281776428
Total epoch: 81. epoch loss: 0.7455661296844482
Total epoch: 82. epoch loss: 0.7380058169364929
Total epoch: 83. epoch loss: 0.7306404113769531
Total epoch: 84. epoch loss: 0.7234669327735901
Total epoch: 85. epoch loss: 0.7164771556854248
Total epoch: 86. epoch loss: 0.7096606492996216
Total epoch: 87. epoch loss: 0.703007161617279
Total epoch: 88. epoch loss: 0.6965155601501465
Total epoch: 89. epoch loss: 0.6901782155036926
Total epoch: 90. epoch loss: 0.6839823722839355
Total epoch: 91. epoch loss: 0.6779288649559021
Total epoch: 92. epoch loss: 0.6720125079154968
Total epoch: 93. epoch loss: 0.6662247776985168
Total epoch: 94. epoch loss: 0.6605636477470398
Total epoch: 95. epoch loss: 0.6550207138061523
Total epoch: 96. epoch loss: 0.6495919227600098
Total epoch: 97. epoch loss: 0.6442787051200867
Total epoch: 98. epoch loss: 0.6390694379806519
Total epoch: 99. epoch loss: 0.6339672803878784
Total epoch: 99. DecT loss: 0.6339672803878784
Training time: 0.7461528778076172
APL_precision: 0.37142857142857144, APL_recall: 0.5352941176470588, APL_f1: 0.43855421686746987, APL_number: 170
CMT_precision: 0.32558139534883723, CMT_recall: 0.5025641025641026, CMT_f1: 0.3951612903225806, CMT_number: 195
DSC_precision: 0.5, DSC_recall: 0.5446224256292906, DSC_f1: 0.5213581599123768, DSC_number: 437
MAT_precision: 0.5651074589127687, MAT_recall: 0.655425219941349, MAT_f1: 0.6069246435845214, MAT_number: 682
PRO_precision: 0.38496071829405165, PRO_recall: 0.44487678339818415, PRO_f1: 0.4127557160048135, PRO_number: 771
SMT_precision: 0.3519163763066202, SMT_recall: 0.5906432748538012, SMT_f1: 0.4410480349344978, SMT_number: 171
SPL_precision: 0.35384615384615387, SPL_recall: 0.30666666666666664, SPL_f1: 0.3285714285714285, SPL_number: 75
overall_precision: 0.4388089005235602, overall_recall: 0.5361855257896841, overall_f1: 0.48263451502609317, overall_accuracy: 0.8330355228361089
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:33:27 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:33:28 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1070.66it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
05/31/2023 13:33:29 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-bc4738df46cc5954.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4270.03 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:33:34 - INFO - __main__ - ***** Running training *****
05/31/2023 13:33:34 - INFO - __main__ -   Num examples = 77
05/31/2023 13:33:34 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:33:34 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:33:34 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:33:34 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:33:34 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.940923690795898
Total epoch: 1. epoch loss: 15.062766075134277
Total epoch: 2. epoch loss: 14.233013153076172
Total epoch: 3. epoch loss: 13.444703102111816
Total epoch: 4. epoch loss: 12.692696571350098
Total epoch: 5. epoch loss: 11.974089622497559
Total epoch: 6. epoch loss: 11.287765502929688
Total epoch: 7. epoch loss: 10.6336669921875
Total epoch: 8. epoch loss: 10.012063980102539
Total epoch: 9. epoch loss: 9.422987937927246
Total epoch: 10. epoch loss: 8.866063117980957
Total epoch: 11. epoch loss: 8.340375900268555
Total epoch: 12. epoch loss: 7.844663143157959
Total epoch: 13. epoch loss: 7.377508640289307
Total epoch: 14. epoch loss: 6.937492847442627
Total epoch: 15. epoch loss: 6.523291110992432
Total epoch: 16. epoch loss: 6.1337199211120605
Total epoch: 17. epoch loss: 5.767585277557373
Total epoch: 18. epoch loss: 5.423727512359619
Total epoch: 19. epoch loss: 5.100873947143555
Total epoch: 20. epoch loss: 4.7988057136535645
Total epoch: 21. epoch loss: 4.5169901847839355
Total epoch: 22. epoch loss: 4.2546844482421875
Total epoch: 23. epoch loss: 4.010992050170898
Total epoch: 24. epoch loss: 3.7848947048187256
Total epoch: 25. epoch loss: 3.5752859115600586
Total epoch: 26. epoch loss: 3.3810360431671143
Total epoch: 27. epoch loss: 3.200960397720337
Total epoch: 28. epoch loss: 3.0340027809143066
Total epoch: 29. epoch loss: 2.8791451454162598
Total epoch: 30. epoch loss: 2.73545241355896
Total epoch: 31. epoch loss: 2.6020379066467285
Total epoch: 32. epoch loss: 2.4781153202056885
Total epoch: 33. epoch loss: 2.362914800643921
Total epoch: 34. epoch loss: 2.25575852394104
Total epoch: 35. epoch loss: 2.1559877395629883
Total epoch: 36. epoch loss: 2.063033103942871
Total epoch: 37. epoch loss: 1.9763485193252563
Total epoch: 38. epoch loss: 1.8954639434814453
Total epoch: 39. epoch loss: 1.819926142692566
Total epoch: 40. epoch loss: 1.7493228912353516
Total epoch: 41. epoch loss: 1.6832855939865112
Total epoch: 42. epoch loss: 1.6214580535888672
Total epoch: 43. epoch loss: 1.5635223388671875
Total epoch: 44. epoch loss: 1.5091793537139893
Total epoch: 45. epoch loss: 1.4581762552261353
Total epoch: 46. epoch loss: 1.4102704524993896
Total epoch: 47. epoch loss: 1.3652122020721436
Total epoch: 48. epoch loss: 1.3228133916854858
Total epoch: 49. epoch loss: 1.2828760147094727
Total epoch: 50. epoch loss: 1.245215892791748
Total epoch: 51. epoch loss: 1.209676742553711
Total epoch: 52. epoch loss: 1.1760953664779663
Total epoch: 53. epoch loss: 1.1443448066711426
Total epoch: 54. epoch loss: 1.1142935752868652
Total epoch: 55. epoch loss: 1.0858114957809448
Total epoch: 56. epoch loss: 1.0587936639785767
Total epoch: 57. epoch loss: 1.0331391096115112
Total epoch: 58. epoch loss: 1.0087443590164185
Total epoch: 59. epoch loss: 0.9855257272720337
Total epoch: 60. epoch loss: 0.9634097218513489
Total epoch: 61. epoch loss: 0.9423173666000366
Total epoch: 62. epoch loss: 0.9221718311309814
Total epoch: 63. epoch loss: 0.902910590171814
Total epoch: 64. epoch loss: 0.8844864964485168
Total epoch: 65. epoch loss: 0.8668341636657715
Total epoch: 66. epoch loss: 0.8499120473861694
Total epoch: 67. epoch loss: 0.8336706161499023
Total epoch: 68. epoch loss: 0.8180689811706543
Total epoch: 69. epoch loss: 0.8030688762664795
Total epoch: 70. epoch loss: 0.7886335253715515
Total epoch: 71. epoch loss: 0.7747299671173096
Total epoch: 72. epoch loss: 0.7613286375999451
Total epoch: 73. epoch loss: 0.7483994364738464
Total epoch: 74. epoch loss: 0.7359210252761841
Total epoch: 75. epoch loss: 0.723862886428833
Total epoch: 76. epoch loss: 0.7122090458869934
Total epoch: 77. epoch loss: 0.7009298205375671
Total epoch: 78. epoch loss: 0.6900187134742737
Total epoch: 79. epoch loss: 0.6794483065605164
Total epoch: 80. epoch loss: 0.6691986918449402
Total epoch: 81. epoch loss: 0.6592609882354736
Total epoch: 82. epoch loss: 0.649615466594696
Total epoch: 83. epoch loss: 0.6402540802955627
Total epoch: 84. epoch loss: 0.6311556696891785
Total epoch: 85. epoch loss: 0.6223137378692627
Total epoch: 86. epoch loss: 0.6137166023254395
Total epoch: 87. epoch loss: 0.6053481101989746
Total epoch: 88. epoch loss: 0.5972073078155518
Total epoch: 89. epoch loss: 0.5892764925956726
Total epoch: 90. epoch loss: 0.5815495848655701
Total epoch: 91. epoch loss: 0.5740191340446472
Total epoch: 92. epoch loss: 0.5666747093200684
Total epoch: 93. epoch loss: 0.5595117211341858
Total epoch: 94. epoch loss: 0.5525228381156921
Total epoch: 95. epoch loss: 0.5456985831260681
Total epoch: 96. epoch loss: 0.5390311479568481
Total epoch: 97. epoch loss: 0.5325222015380859
Total epoch: 98. epoch loss: 0.5261614918708801
Total epoch: 99. epoch loss: 0.5199406743049622
Total epoch: 99. DecT loss: 0.5199406743049622
Training time: 0.7729072570800781
APL_precision: 0.35046728971962615, APL_recall: 0.4411764705882353, APL_f1: 0.390625, APL_number: 170
CMT_precision: 0.35611510791366907, CMT_recall: 0.5076923076923077, CMT_f1: 0.41860465116279066, CMT_number: 195
DSC_precision: 0.5295454545454545, DSC_recall: 0.5331807780320366, DSC_f1: 0.5313568985176739, DSC_number: 437
MAT_precision: 0.5656050955414013, MAT_recall: 0.6510263929618768, MAT_f1: 0.6053169734151329, MAT_number: 682
PRO_precision: 0.3793911007025761, PRO_recall: 0.42023346303501946, PRO_f1: 0.39876923076923076, PRO_number: 771
SMT_precision: 0.3310810810810811, SMT_recall: 0.5730994152046783, SMT_f1: 0.4197002141327623, SMT_number: 171
SPL_precision: 0.3582089552238806, SPL_recall: 0.32, SPL_f1: 0.3380281690140845, SPL_number: 75
overall_precision: 0.44205862304021815, overall_recall: 0.51859256297481, overall_f1: 0.47727690892364305, overall_accuracy: 0.8318204560074334
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:35:15 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:35:16 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 930.21it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
05/31/2023 13:35:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-bc4738df46cc5954.arrow
05/31/2023 13:35:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-5c35b278722cfb93.arrow
/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:550: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:35:22 - INFO - __main__ - ***** Running training *****
05/31/2023 13:35:22 - INFO - __main__ -   Num examples = 77
05/31/2023 13:35:22 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:35:22 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:35:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:35:22 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:35:22 - INFO - __main__ -   Total optimization steps = 300
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.786142349243164
Total epoch: 1. epoch loss: 14.904176712036133
Total epoch: 2. epoch loss: 14.056124687194824
Total epoch: 3. epoch loss: 13.236756324768066
Total epoch: 4. epoch loss: 12.442829132080078
Total epoch: 5. epoch loss: 11.673941612243652
Total epoch: 6. epoch loss: 10.931748390197754
Total epoch: 7. epoch loss: 10.218878746032715
Total epoch: 8. epoch loss: 9.538183212280273
Total epoch: 9. epoch loss: 8.89242172241211
Total epoch: 10. epoch loss: 8.28375244140625
Total epoch: 11. epoch loss: 7.713647365570068
Total epoch: 12. epoch loss: 7.18263578414917
Total epoch: 13. epoch loss: 6.690417289733887
Total epoch: 14. epoch loss: 6.235822677612305
Total epoch: 15. epoch loss: 5.816995620727539
Total epoch: 16. epoch loss: 5.4315080642700195
Total epoch: 17. epoch loss: 5.076541900634766
Total epoch: 18. epoch loss: 4.749089241027832
Total epoch: 19. epoch loss: 4.446192264556885
Total epoch: 20. epoch loss: 4.166289806365967
Total epoch: 21. epoch loss: 3.9079055786132812
Total epoch: 22. epoch loss: 3.669581890106201
Total epoch: 23. epoch loss: 3.4499778747558594
Total epoch: 24. epoch loss: 3.247858762741089
Total epoch: 25. epoch loss: 3.0620477199554443
Total epoch: 26. epoch loss: 2.8914291858673096
Total epoch: 27. epoch loss: 2.7349390983581543
Total epoch: 28. epoch loss: 2.5914835929870605
Total epoch: 29. epoch loss: 2.4600205421447754
Total epoch: 30. epoch loss: 2.339531421661377
Total epoch: 31. epoch loss: 2.2290303707122803
Total epoch: 32. epoch loss: 2.127586841583252
Total epoch: 33. epoch loss: 2.0343070030212402
Total epoch: 34. epoch loss: 1.9484246969223022
Total epoch: 35. epoch loss: 1.8691861629486084
Total epoch: 36. epoch loss: 1.7959675788879395
Total epoch: 37. epoch loss: 1.7282246351242065
Total epoch: 38. epoch loss: 1.6654736995697021
Total epoch: 39. epoch loss: 1.6072866916656494
Total epoch: 40. epoch loss: 1.5532889366149902
Total epoch: 41. epoch loss: 1.503137469291687
Total epoch: 42. epoch loss: 1.4565198421478271
Total epoch: 43. epoch loss: 1.413123369216919
Total epoch: 44. epoch loss: 1.3726730346679688
Total epoch: 45. epoch loss: 1.3349274396896362
Total epoch: 46. epoch loss: 1.2996528148651123
Total epoch: 47. epoch loss: 1.2666319608688354
Total epoch: 48. epoch loss: 1.2356754541397095
Total epoch: 49. epoch loss: 1.2066069841384888
Total epoch: 50. epoch loss: 1.1792560815811157
Total epoch: 51. epoch loss: 1.1534725427627563
Total epoch: 52. epoch loss: 1.1291241645812988
Total epoch: 53. epoch loss: 1.1060930490493774
Total epoch: 54. epoch loss: 1.0842663049697876
Total epoch: 55. epoch loss: 1.0635523796081543
Total epoch: 56. epoch loss: 1.0438740253448486
Total epoch: 57. epoch loss: 1.0251461267471313
Total epoch: 58. epoch loss: 1.0073078870773315
Total epoch: 59. epoch loss: 0.9902953505516052
Total epoch: 60. epoch loss: 0.9740510582923889
Total epoch: 61. epoch loss: 0.9585188031196594
Total epoch: 62. epoch loss: 0.9436511397361755
Total epoch: 63. epoch loss: 0.9294047355651855
Total epoch: 64. epoch loss: 0.9157322645187378
Total epoch: 65. epoch loss: 0.902606725692749
Total epoch: 66. epoch loss: 0.8899821639060974
Total epoch: 67. epoch loss: 0.8778301477432251
Total epoch: 68. epoch loss: 0.8661176562309265
Total epoch: 69. epoch loss: 0.8548203706741333
Total epoch: 70. epoch loss: 0.8439181447029114
Total epoch: 71. epoch loss: 0.8333810567855835
Total epoch: 72. epoch loss: 0.8231912851333618
Total epoch: 73. epoch loss: 0.813336968421936
Total epoch: 74. epoch loss: 0.8037905693054199
Total epoch: 75. epoch loss: 0.7945408225059509
Total epoch: 76. epoch loss: 0.7855715155601501
Total epoch: 77. epoch loss: 0.7768707871437073
Total epoch: 78. epoch loss: 0.7684243321418762
Total epoch: 79. epoch loss: 0.7602159976959229
Total epoch: 80. epoch loss: 0.7522336840629578
Total epoch: 81. epoch loss: 0.7444761395454407
Total epoch: 82. epoch loss: 0.7369215488433838
Total epoch: 83. epoch loss: 0.7295611500740051
Total epoch: 84. epoch loss: 0.7223949432373047
Total epoch: 85. epoch loss: 0.7154057025909424
Total epoch: 86. epoch loss: 0.7085937857627869
Total epoch: 87. epoch loss: 0.7019394636154175
Total epoch: 88. epoch loss: 0.6954482793807983
Total epoch: 89. epoch loss: 0.6891062259674072
Total epoch: 90. epoch loss: 0.6829119920730591
Total epoch: 91. epoch loss: 0.6768597960472107
Total epoch: 92. epoch loss: 0.6709403395652771
Total epoch: 93. epoch loss: 0.6651504039764404
Total epoch: 94. epoch loss: 0.6594818234443665
Total epoch: 95. epoch loss: 0.6539377570152283
Total epoch: 96. epoch loss: 0.6485056281089783
Total epoch: 97. epoch loss: 0.6431835889816284
Total epoch: 98. epoch loss: 0.6379719972610474
Total epoch: 99. epoch loss: 0.6328635215759277
Total epoch: 99. DecT loss: 0.6328635215759277
Training time: 0.7610056400299072
APL_precision: 0.3673469387755102, APL_recall: 0.5294117647058824, APL_f1: 0.4337349397590361, APL_number: 170
CMT_precision: 0.3333333333333333, CMT_recall: 0.5128205128205128, CMT_f1: 0.40404040404040403, CMT_number: 195
DSC_precision: 0.4927536231884058, DSC_recall: 0.5446224256292906, DSC_f1: 0.5173913043478262, DSC_number: 437
MAT_precision: 0.5642317380352645, MAT_recall: 0.656891495601173, MAT_f1: 0.6070460704607046, MAT_number: 682
PRO_precision: 0.37831858407079644, PRO_recall: 0.44357976653696496, PRO_f1: 0.40835820895522384, PRO_number: 771
SMT_precision: 0.3519163763066202, SMT_recall: 0.5906432748538012, SMT_f1: 0.4410480349344978, SMT_number: 171
SPL_precision: 0.3484848484848485, SPL_recall: 0.30666666666666664, SPL_f1: 0.326241134751773, SPL_number: 75
overall_precision: 0.43585579733679763, overall_recall: 0.5365853658536586, overall_f1: 0.4810035842293907, overall_accuracy: 0.8327496247587735
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 972, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 795, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]