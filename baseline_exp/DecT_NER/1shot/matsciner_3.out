/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:29:09 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:29:10 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-a5b11f1308f98d86/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:1, proto_dim:32, logits_weight:20, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 981.47it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
05/30/2023 12:29:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-a5b11f1308f98d86/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-535393c8706f5801.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:29:16 - INFO - __main__ - ***** Running training *****
05/30/2023 12:29:16 - INFO - __main__ -   Num examples = 6
05/30/2023 12:29:16 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:29:16 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:29:16 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:29:16 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:29:16 - INFO - __main__ -   Total optimization steps = 35
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 14.300142288208008
Total epoch: 1. epoch loss: 13.21240520477295
Total epoch: 2. epoch loss: 12.137462615966797
Total epoch: 3. epoch loss: 11.083553314208984
Total epoch: 4. epoch loss: 10.059946060180664
Total epoch: 5. epoch loss: 9.077072143554688
Total epoch: 6. epoch loss: 8.145646095275879
Total epoch: 7. epoch loss: 7.275612831115723
Total epoch: 8. epoch loss: 6.475198745727539
Total epoch: 9. epoch loss: 5.74924373626709
Total epoch: 10. epoch loss: 5.09922456741333
Total epoch: 11. epoch loss: 4.52297830581665
Total epoch: 12. epoch loss: 4.015554904937744
Total epoch: 13. epoch loss: 3.570218801498413
Total epoch: 14. epoch loss: 3.1794350147247314
Total epoch: 15. epoch loss: 2.8354568481445312
Total epoch: 16. epoch loss: 2.5310943126678467
Total epoch: 17. epoch loss: 2.259866714477539
Total epoch: 18. epoch loss: 2.016399621963501
Total epoch: 19. epoch loss: 1.7963680028915405
Total epoch: 20. epoch loss: 1.5967135429382324
Total epoch: 21. epoch loss: 1.4154001474380493
Total epoch: 22. epoch loss: 1.2513017654418945
Total epoch: 23. epoch loss: 1.1039016246795654
Total epoch: 24. epoch loss: 0.9728153944015503
Total epoch: 25. epoch loss: 0.8576300144195557
Total epoch: 26. epoch loss: 0.7581846714019775
Total epoch: 27. epoch loss: 0.6735183000564575
Total epoch: 28. epoch loss: 0.6018010377883911
Total epoch: 29. epoch loss: 0.5408307313919067
Total epoch: 30. epoch loss: 0.4884796440601349
Total epoch: 31. epoch loss: 0.44301849603652954
Total epoch: 32. epoch loss: 0.4031805396080017
Total epoch: 33. epoch loss: 0.3680858910083771
Total epoch: 34. epoch loss: 0.33715584874153137
Total epoch: 34. DecT loss: 0.33715584874153137
Training time: 0.1751856803894043
APL_precision: 0.0, APL_recall: 0.0, APL_f1: 0.0, APL_number: 170
CMT_precision: 0.2549019607843137, CMT_recall: 0.06666666666666667, CMT_f1: 0.10569105691056911, CMT_number: 195
DSC_precision: 0.47619047619047616, DSC_recall: 0.02288329519450801, DSC_f1: 0.043668122270742356, DSC_number: 437
MAT_precision: 0.6134453781512605, MAT_recall: 0.32111436950146627, MAT_f1: 0.4215591915303177, MAT_number: 682
PRO_precision: 0.07142857142857142, PRO_recall: 0.0012970168612191958, PRO_f1: 0.002547770700636943, PRO_number: 771
SMT_precision: 0.0, SMT_recall: 0.0, SMT_f1: 0.0, SMT_number: 171
SPL_precision: 0.5, SPL_recall: 0.05333333333333334, SPL_f1: 0.0963855421686747, SPL_number: 75
overall_precision: 0.5369565217391304, overall_recall: 0.09876049580167932, overall_f1: 0.16683552853765618, overall_accuracy: 0.7077406904438568
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:02<?, ?it/s]