/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/30/2023 12:29:09 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/30/2023 12:29:11 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-83b4942faa9b0e8c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
lr:0.005, batch_size:32, shot:1, proto_dim:32, logits_weight:20, weight_decay:1e-07 
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1153.39it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
05/30/2023 12:29:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-83b4942faa9b0e8c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7cb4c3bfd54ed3a9.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081]
I-MAT
[8605]
I-DSC
[21155]
I-PRO
[1784]
I-SMT
[12040]
I-APL
[9754]
I-SPL
[13879]
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:555: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/30/2023 12:29:17 - INFO - __main__ - ***** Running training *****
05/30/2023 12:29:17 - INFO - __main__ -   Num examples = 5
05/30/2023 12:29:17 - INFO - __main__ -   Num Epochs = 35
05/30/2023 12:29:17 - INFO - __main__ -   Instantaneous batch size per device = 32
05/30/2023 12:29:17 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/30/2023 12:29:17 - INFO - __main__ -   Gradient Accumulation steps = 1
05/30/2023 12:29:17 - INFO - __main__ -   Total optimization steps = 35
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.971551895141602
Total epoch: 1. epoch loss: 14.958385467529297
Total epoch: 2. epoch loss: 13.958702087402344
Total epoch: 3. epoch loss: 12.978029251098633
Total epoch: 4. epoch loss: 12.022247314453125
Total epoch: 5. epoch loss: 11.097537994384766
Total epoch: 6. epoch loss: 10.210058212280273
Total epoch: 7. epoch loss: 9.365169525146484
Total epoch: 8. epoch loss: 8.566970825195312
Total epoch: 9. epoch loss: 7.817954063415527
Total epoch: 10. epoch loss: 7.119119167327881
Total epoch: 11. epoch loss: 6.470132350921631
Total epoch: 12. epoch loss: 5.8700361251831055
Total epoch: 13. epoch loss: 5.317418098449707
Total epoch: 14. epoch loss: 4.810863494873047
Total epoch: 15. epoch loss: 4.3488311767578125
Total epoch: 16. epoch loss: 3.9294934272766113
Total epoch: 17. epoch loss: 3.55078125
Total epoch: 18. epoch loss: 3.2099077701568604
Total epoch: 19. epoch loss: 2.903630495071411
Total epoch: 20. epoch loss: 2.62839674949646
Total epoch: 21. epoch loss: 2.3803558349609375
Total epoch: 22. epoch loss: 2.1559653282165527
Total epoch: 23. epoch loss: 1.95216703414917
Total epoch: 24. epoch loss: 1.7665574550628662
Total epoch: 25. epoch loss: 1.5975393056869507
Total epoch: 26. epoch loss: 1.4440993070602417
Total epoch: 27. epoch loss: 1.3056209087371826
Total epoch: 28. epoch loss: 1.1816537380218506
Total epoch: 29. epoch loss: 1.0720511674880981
Total epoch: 30. epoch loss: 0.9762434959411621
Total epoch: 31. epoch loss: 0.8928317427635193
Total epoch: 32. epoch loss: 0.8199163675308228
Total epoch: 33. epoch loss: 0.7556222677230835
Total epoch: 34. epoch loss: 0.6983421444892883
Total epoch: 34. DecT loss: 0.6983421444892883
Training time: 0.15530920028686523
APL_precision: 0.06626506024096386, APL_recall: 0.06470588235294118, APL_f1: 0.0654761904761905, APL_number: 170
CMT_precision: 0.09230769230769231, CMT_recall: 0.03076923076923077, CMT_f1: 0.046153846153846156, CMT_number: 195
DSC_precision: 0.9230769230769231, DSC_recall: 0.02745995423340961, DSC_f1: 0.05333333333333332, DSC_number: 437
MAT_precision: 0.782608695652174, MAT_recall: 0.026392961876832845, MAT_f1: 0.05106382978723404, MAT_number: 682
PRO_precision: 0.0, PRO_recall: 0.0, PRO_f1: 0.0, PRO_number: 771
SMT_precision: 0.0, SMT_recall: 0.0, SMT_f1: 0.0, SMT_number: 171
SPL_precision: 0.2857142857142857, SPL_recall: 0.05333333333333334, SPL_f1: 0.0898876404494382, SPL_number: 75
overall_precision: 0.15223880597014924, overall_recall: 0.02039184326269492, overall_f1: 0.03596614950634697, overall_accuracy: 0.6938746336930884
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 1000, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 800, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:01<?, ?it/s]