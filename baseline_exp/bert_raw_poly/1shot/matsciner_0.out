Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
5 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.1161646842956543
loss:2.1184446811676025
training_time:0.044725656509399414
loss:1.1554127931594849
training_time:0.04230618476867676
loss:0.5920385122299194
training_time:0.040628910064697266
loss:0.3659433424472809
training_time:0.041300058364868164
loss:0.2362646609544754
training_time:0.040981292724609375
loss:0.14778931438922882
training_time:0.03978157043457031
loss:0.09859714657068253
training_time:0.04083418846130371
loss:0.06945867091417313
training_time:0.0408167839050293
loss:0.04570729285478592
training_time:0.046508073806762695
loss:0.02901037596166134
training_time:0.0419306755065918
loss:0.02046971768140793
training_time:0.0479581356048584
loss:0.016077136620879173
training_time:0.05219697952270508
loss:0.013267261907458305
training_time:0.04445075988769531
loss:0.008973249234259129
training_time:0.04426980018615723
loss:0.008583533577620983
training_time:0.046953439712524414
loss:0.007628474850207567
training_time:0.05068325996398926
loss:0.006079610902816057
training_time:0.04893922805786133
loss:0.005036659073084593
training_time:0.049213409423828125
loss:0.00488045159727335
training_time:0.04696774482727051
loss:0.0043129934929311275
training_time:0.052874088287353516
loss:0.00427849218249321
training_time:0.04049253463745117
loss:0.004025293048471212
training_time:0.04657316207885742
loss:0.003949481528252363
training_time:0.04701852798461914
loss:0.003796996548771858
training_time:0.043004512786865234
loss:0.003403642913326621
training_time:0.04913043975830078
loss:0.0035314515698701143
training_time:0.044080495834350586
loss:0.003127208910882473
training_time:0.053165435791015625
loss:0.0030250137206166983
training_time:0.04564547538757324
loss:0.003135974984616041
training_time:0.06155753135681152
loss:0.00302810943685472
{'APL': 9.045226130653267, 'CMT': 7.1428571428571415, 'DSC': 4.0358744394618835, 'MAT': 2.8943560057887123, 'PRO': 0.5115089514066495, 'SMT': 8.849557522123895, 'SPL': 33.663366336633665, 'macro_f1': 0.0944896378984646, 'micro_f1': 0.0491743119266055}
