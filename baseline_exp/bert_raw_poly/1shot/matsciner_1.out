Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
5 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.09506034851074219
loss:2.467041254043579
training_time:0.04840397834777832
loss:1.3298969268798828
training_time:0.03995084762573242
loss:0.5522772073745728
training_time:0.04660844802856445
loss:0.42429211735725403
training_time:0.05305337905883789
loss:0.2617485225200653
training_time:0.041916847229003906
loss:0.20391495525836945
training_time:0.03871321678161621
loss:0.14032591879367828
training_time:0.04833555221557617
loss:0.08456337451934814
training_time:0.03880906105041504
loss:0.059215936809778214
training_time:0.050722360610961914
loss:0.037189655005931854
training_time:0.05269169807434082
loss:0.02960442565381527
training_time:0.04201364517211914
loss:0.01980740763247013
training_time:0.04737257957458496
loss:0.016858825460076332
training_time:0.038890838623046875
loss:0.011726770550012589
training_time:0.04854083061218262
loss:0.010859564878046513
training_time:0.03864336013793945
loss:0.009030173532664776
training_time:0.04786372184753418
loss:0.007658638060092926
training_time:0.038724660873413086
loss:0.005996383260935545
training_time:0.04909467697143555
loss:0.005615685135126114
training_time:0.04011893272399902
loss:0.0049332622438669205
training_time:0.04909634590148926
loss:0.005065756384283304
training_time:0.04164767265319824
loss:0.0045479354448616505
training_time:0.050267696380615234
loss:0.005424207542091608
training_time:0.04167342185974121
loss:0.0038940943777561188
training_time:0.05259132385253906
loss:0.0037671090103685856
training_time:0.050145864486694336
loss:0.004027698654681444
training_time:0.05369853973388672
loss:0.003729085670784116
training_time:0.05623269081115723
loss:0.0033681518398225307
training_time:0.0437467098236084
loss:0.0037990231066942215
training_time:0.052866220474243164
loss:0.0034242230467498302
{'APL': 13.761467889908257, 'CMT': 1.7621145374449338, 'DSC': 0.0, 'MAT': 26.95035460992908, 'PRO': 1.7897091722595075, 'SMT': 5.785123966942148, 'SPL': 39.655172413793096, 'macro_f1': 0.12814848941468146, 'micro_f1': 0.11315701372614663}
