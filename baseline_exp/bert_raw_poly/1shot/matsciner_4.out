Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
4 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.09505820274353027
loss:2.1393425464630127
training_time:0.04425978660583496
loss:1.07710599899292
training_time:0.05067896842956543
loss:0.5651684999465942
training_time:0.04363107681274414
loss:0.3803921341896057
training_time:0.05145859718322754
loss:0.2555372714996338
training_time:0.04395318031311035
loss:0.1614266037940979
training_time:0.04386162757873535
loss:0.1101514995098114
training_time:0.05126214027404785
loss:0.0671594962477684
training_time:0.042842864990234375
loss:0.045565515756607056
training_time:0.05567789077758789
loss:0.030806228518486023
training_time:0.04241943359375
loss:0.022152725607156754
training_time:0.04897618293762207
loss:0.01736488752067089
training_time:0.04304385185241699
loss:0.012694209814071655
training_time:0.048224687576293945
loss:0.011367264203727245
training_time:0.04268646240234375
loss:0.008395151235163212
training_time:0.049112558364868164
loss:0.007674859836697578
training_time:0.05293130874633789
loss:0.007023671641945839
training_time:0.04239392280578613
loss:0.00673339981585741
training_time:0.04945254325866699
loss:0.005948788020759821
training_time:0.04148054122924805
loss:0.004346286877989769
training_time:0.04344534873962402
loss:0.0044325594790279865
training_time:0.048311710357666016
loss:0.0038472251035273075
training_time:0.04184556007385254
loss:0.003789992071688175
training_time:0.04899239540100098
loss:0.003946272190660238
training_time:0.04251861572265625
loss:0.0036067841574549675
training_time:0.04779934883117676
loss:0.0038361339829862118
training_time:0.041115760803222656
loss:0.0036566159687936306
training_time:0.04771780967712402
loss:0.0034665062557905912
training_time:0.039374351501464844
loss:0.003427918767556548
training_time:0.04900503158569336
loss:0.0034373782109469175
{'APL': 14.07035175879397, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 20.584498094027953, 'PRO': 1.2738853503184715, 'SMT': 3.4482758620689653, 'SPL': 4.938271604938272, 'macro_f1': 0.06330754667163949, 'micro_f1': 0.07847533632286996}
