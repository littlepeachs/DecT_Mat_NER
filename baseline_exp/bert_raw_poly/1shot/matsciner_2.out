Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
4 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10770344734191895
loss:1.8928391933441162
training_time:0.054807424545288086
loss:0.9477400183677673
training_time:0.0472865104675293
loss:0.4652598798274994
training_time:0.05175471305847168
loss:0.3298024535179138
training_time:0.04837989807128906
loss:0.22102321684360504
training_time:0.052727699279785156
loss:0.15874788165092468
training_time:0.046751976013183594
loss:0.10829126089811325
training_time:0.04560995101928711
loss:0.0743209719657898
training_time:0.0496363639831543
loss:0.050944361835718155
training_time:0.051506996154785156
loss:0.02961685322225094
training_time:0.04207873344421387
loss:0.022589100524783134
training_time:0.05530905723571777
loss:0.01886117272078991
training_time:0.04452347755432129
loss:0.014342965558171272
training_time:0.04857301712036133
loss:0.012329555116593838
training_time:0.05198359489440918
loss:0.009488847106695175
training_time:0.05466198921203613
loss:0.010771144181489944
training_time:0.04253816604614258
loss:0.007184441201388836
training_time:0.04852795600891113
loss:0.006707603577524424
training_time:0.04298543930053711
loss:0.005469110794365406
training_time:0.05393815040588379
loss:0.0054517341777682304
training_time:0.045300960540771484
loss:0.005000019911676645
training_time:0.050841569900512695
loss:0.004755082540214062
training_time:0.03973126411437988
loss:0.003694525919854641
training_time:0.04720449447631836
loss:0.0045305285602808
training_time:0.04812431335449219
loss:0.003536622039973736
training_time:0.04134964942932129
loss:0.003653409192338586
training_time:0.048568010330200195
loss:0.003542665857821703
training_time:0.041281938552856445
loss:0.003933729138225317
training_time:0.048090457916259766
loss:0.0030528875067830086
training_time:0.03975200653076172
loss:0.003300144337117672
{'APL': 5.347593582887701, 'CMT': 1.9230769230769231, 'DSC': 3.821656050955414, 'MAT': 0.0, 'PRO': 6.642066420664207, 'SMT': 1.1560693641618498, 'SPL': 30.158730158730158, 'macro_f1': 0.07007027500068036, 'micro_f1': 0.0474040632054176}
