Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
15 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.0898427963256836
loss:2.0981826782226562
training_time:0.04517674446105957
loss:1.242012858390808
training_time:0.04457688331604004
loss:0.7017673850059509
training_time:0.04258298873901367
loss:0.5821649432182312
training_time:0.04991579055786133
loss:0.45230814814567566
training_time:0.04226398468017578
loss:0.36661893129348755
training_time:0.04868936538696289
loss:0.28839296102523804
training_time:0.043515920639038086
loss:0.21591660380363464
training_time:0.04966402053833008
loss:0.15949755907058716
training_time:0.04071474075317383
loss:0.11571840941905975
training_time:0.04936385154724121
loss:0.08382086455821991
training_time:0.040673017501831055
loss:0.06400743871927261
training_time:0.05061960220336914
loss:0.05045907199382782
training_time:0.04041314125061035
loss:0.03803956136107445
training_time:0.048470497131347656
loss:0.037574946880340576
training_time:0.04065299034118652
loss:0.025770897045731544
training_time:0.04856729507446289
loss:0.024684365838766098
training_time:0.04026317596435547
loss:0.017721004784107208
training_time:0.04812192916870117
loss:0.03601700812578201
training_time:0.04067254066467285
loss:0.012821140699088573
training_time:0.04885745048522949
loss:0.02645559050142765
training_time:0.04162764549255371
loss:0.022785713896155357
training_time:0.04686284065246582
loss:0.020919637754559517
training_time:0.03934049606323242
loss:0.020961059257388115
training_time:0.04758000373840332
loss:0.02256358414888382
training_time:0.05057859420776367
loss:0.022001052275300026
training_time:0.039215087890625
loss:0.018779676407575607
training_time:0.04768204689025879
loss:0.018963675945997238
training_time:0.039250850677490234
loss:0.016236957162618637
training_time:0.045815229415893555
loss:0.015090221539139748
{'APL': 31.768953068592058, 'CMT': 4.733727810650888, 'DSC': 17.05426356589147, 'MAT': 58.326289095519854, 'PRO': 6.235565819861433, 'SMT': 7.692307692307692, 'SPL': 32.58426966292135, 'macro_f1': 0.22627910959392106, 'micro_f1': 0.2802653399668325}
