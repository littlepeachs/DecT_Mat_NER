Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
14 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.09302711486816406
loss:2.0622098445892334
training_time:0.048979997634887695
loss:1.2152653932571411
training_time:0.04294133186340332
loss:0.7176375389099121
training_time:0.04864239692687988
loss:0.5904785990715027
training_time:0.04628753662109375
loss:0.4577098786830902
training_time:0.04285788536071777
loss:0.36187079548835754
training_time:0.042394399642944336
loss:0.2789846360683441
training_time:0.0488743782043457
loss:0.21066692471504211
training_time:0.04120588302612305
loss:0.1538512259721756
training_time:0.04814338684082031
loss:0.10757384449243546
training_time:0.03951573371887207
loss:0.08026715368032455
training_time:0.048362016677856445
loss:0.057157158851623535
training_time:0.039490699768066406
loss:0.0451991930603981
training_time:0.047891855239868164
loss:0.03575989603996277
training_time:0.0404658317565918
loss:0.026254000142216682
training_time:0.048035383224487305
loss:0.019621895626187325
training_time:0.03952646255493164
loss:0.017707953229546547
training_time:0.04889106750488281
loss:0.015139327384531498
training_time:0.03944849967956543
loss:0.013356085866689682
training_time:0.04775691032409668
loss:0.010547719895839691
training_time:0.0392303466796875
loss:0.009501326829195023
training_time:0.048870086669921875
loss:0.009490611962974072
training_time:0.0415494441986084
loss:0.007890229113399982
training_time:0.04698586463928223
loss:0.007799254264682531
training_time:0.03953218460083008
loss:0.0070618693716824055
training_time:0.04883933067321777
loss:0.007151450961828232
training_time:0.04864358901977539
loss:0.0066804359667003155
training_time:0.04431629180908203
loss:0.0064089675433933735
training_time:0.039435386657714844
loss:0.005968610290437937
training_time:0.049617767333984375
loss:0.006210483144968748
{'APL': 30.046948356807512, 'CMT': 34.75783475783476, 'DSC': 2.678571428571428, 'MAT': 20.82262210796915, 'PRO': 4.920049200492005, 'SMT': 19.00452488687783, 'SPL': 44.827586206896555, 'macro_f1': 0.22436876706492748, 'micro_f1': 0.1680272108843537}
