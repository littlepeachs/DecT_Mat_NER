Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
45 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.15469837188720703
loss:1.2667397260665894
training_time:0.12163019180297852
loss:0.7591769099235535
training_time:0.1189429759979248
loss:0.5658550262451172
training_time:0.11627054214477539
loss:0.43522149324417114
training_time:0.11558866500854492
loss:0.19842982292175293
training_time:0.1199791431427002
loss:0.12284720689058304
training_time:0.11322832107543945
loss:0.08830943703651428
training_time:0.1156461238861084
loss:0.05558009445667267
training_time:0.11550259590148926
loss:0.03354017436504364
training_time:0.11335015296936035
loss:0.028188223019242287
training_time:0.11338639259338379
loss:0.01508166640996933
training_time:0.11717009544372559
loss:0.007880143821239471
training_time:0.12530946731567383
loss:0.006198844872415066
training_time:0.12312793731689453
loss:0.009442080743610859
training_time:0.12311506271362305
loss:0.004917643964290619
training_time:0.12259244918823242
loss:0.002623711945489049
training_time:0.12009716033935547
loss:0.002886529779061675
training_time:0.11995148658752441
loss:0.002734909299761057
training_time:0.11220693588256836
loss:0.002394190989434719
training_time:0.11310219764709473
loss:0.0024280669167637825
training_time:0.1132965087890625
loss:0.0028279756661504507
training_time:0.12554216384887695
loss:0.0020567120518535376
training_time:0.12223339080810547
loss:0.002008191542699933
training_time:0.12003278732299805
loss:0.0020887211430817842
training_time:0.11205458641052246
loss:0.0023104283027350903
training_time:0.11325192451477051
loss:0.0018069440266117454
training_time:0.12339234352111816
loss:0.0015648867702111602
training_time:0.11675405502319336
loss:0.0025715548545122147
training_time:0.11530685424804688
loss:0.0018806315492838621
training_time:0.1145329475402832
loss:0.0019896789453923702
{'APL': 58.666666666666664, 'CMT': 66.82464454976304, 'DSC': 51.58371040723981, 'MAT': 67.89772727272727, 'PRO': 44.35917114351496, 'SMT': 51.63043478260869, 'SPL': 51.96850393700788, 'macro_f1': 0.5613297982278975, 'micro_f1': 0.5619285120532004}
