Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
45 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.19538307189941406
loss:1.4425793886184692
training_time:0.13636445999145508
loss:0.7315231561660767
training_time:0.12746930122375488
loss:0.43848809599876404
training_time:0.12778401374816895
loss:0.355876624584198
training_time:0.12856602668762207
loss:0.14728324115276337
training_time:0.12868165969848633
loss:0.1387690305709839
training_time:0.12877416610717773
loss:0.06139695644378662
training_time:0.1273212432861328
loss:0.027124689891934395
training_time:0.12914800643920898
loss:0.018339235335588455
training_time:0.12877178192138672
loss:0.013757824897766113
training_time:0.12875032424926758
loss:0.008733187802135944
training_time:0.13007688522338867
loss:0.005259078927338123
training_time:0.1296381950378418
loss:0.0036639708559960127
training_time:0.13008737564086914
loss:0.004214090760797262
training_time:0.12880396842956543
loss:0.003084148745983839
training_time:0.12930893898010254
loss:0.0022747416514903307
training_time:0.12888145446777344
loss:0.0025999085046350956
training_time:0.1352689266204834
loss:0.0017337895696982741
training_time:0.13483715057373047
loss:0.0018513617105782032
training_time:0.13118577003479004
loss:0.002131496788933873
training_time:0.13107085227966309
loss:0.0018814612412825227
training_time:0.1309826374053955
loss:0.0017150442581623793
training_time:0.13158464431762695
loss:0.0019439911702647805
training_time:0.13130617141723633
loss:0.0022756082471460104
training_time:0.1327202320098877
loss:0.0018096627900376916
training_time:0.13203907012939453
loss:0.001878492534160614
training_time:0.13436031341552734
loss:0.0016198460943996906
training_time:0.1356189250946045
loss:0.001980115892365575
training_time:0.13645458221435547
loss:0.001401066780090332
training_time:0.13481593132019043
loss:0.0016880014445632696
{'APL': 55.30546623794211, 'CMT': 66.00000000000001, 'DSC': 55.3349875930521, 'MAT': 72.51114413075781, 'PRO': 39.10521955260978, 'SMT': 45.092838196286465, 'SPL': 57.861635220125784, 'macro_f1': 0.5588732727582487, 'micro_f1': 0.5627442466348241}
