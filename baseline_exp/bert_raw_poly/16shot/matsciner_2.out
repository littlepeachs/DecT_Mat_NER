Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
46 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.16350507736206055
loss:1.5405610799789429
training_time:0.12433481216430664
loss:0.9491651654243469
training_time:0.12363767623901367
loss:0.5904524326324463
training_time:0.12327957153320312
loss:0.32554274797439575
training_time:0.11791682243347168
loss:0.22409065067768097
training_time:0.12398815155029297
loss:0.1639532893896103
training_time:0.12316727638244629
loss:0.09929579496383667
training_time:0.12554430961608887
loss:0.02824234962463379
training_time:0.12395143508911133
loss:0.03531859442591667
training_time:0.12442922592163086
loss:0.02656291425228119
training_time:0.12569975852966309
loss:0.031229956075549126
training_time:0.12346673011779785
loss:0.015674160793423653
training_time:0.12273669242858887
loss:0.006598287727683783
training_time:0.1284031867980957
loss:0.004583187401294708
training_time:0.12581801414489746
loss:0.011554105207324028
training_time:0.11819720268249512
loss:0.004003234673291445
training_time:0.11741399765014648
loss:0.0035030427388846874
training_time:0.11787891387939453
loss:0.01144661195576191
training_time:0.11887526512145996
loss:0.0031627805437892675
training_time:0.118011474609375
loss:0.003094398882240057
training_time:0.11891889572143555
loss:0.003009289037436247
training_time:0.11792230606079102
loss:0.002330243354663253
training_time:0.11895036697387695
loss:0.002613598247990012
training_time:0.11780977249145508
loss:0.002037201775237918
training_time:0.11892485618591309
loss:0.0027120893355458975
training_time:0.11776518821716309
loss:0.0024812882766127586
training_time:0.12041711807250977
loss:0.0026948305312544107
training_time:0.11784982681274414
loss:0.0025872034020721912
training_time:0.12227797508239746
loss:0.002147616818547249
training_time:0.12337779998779297
loss:0.002441316144540906
{'APL': 42.95774647887324, 'CMT': 61.098901098901095, 'DSC': 47.355769230769226, 'MAT': 68.10284920083392, 'PRO': 52.69552012148823, 'SMT': 45.12195121951219, 'SPL': 54.21686746987952, 'macro_f1': 0.5307851497432249, 'micro_f1': 0.5612943372744245}
