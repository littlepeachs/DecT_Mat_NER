Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
24 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.0984504222869873
loss:2.0243141651153564
training_time:0.06015467643737793
loss:1.1691246032714844
training_time:0.051729440689086914
loss:0.7101988196372986
training_time:0.051630496978759766
loss:0.6361469626426697
training_time:0.05131697654724121
loss:0.5077855587005615
training_time:0.050649166107177734
loss:0.418504536151886
training_time:0.0476834774017334
loss:0.33628639578819275
training_time:0.051084280014038086
loss:0.25770941376686096
training_time:0.04785513877868652
loss:0.19389571249485016
training_time:0.0511167049407959
loss:0.14025384187698364
training_time:0.04780006408691406
loss:0.10870171338319778
training_time:0.0522463321685791
loss:0.08346110582351685
training_time:0.047875165939331055
loss:0.06320381164550781
training_time:0.05031251907348633
loss:0.04706612229347229
training_time:0.04779553413391113
loss:0.03483575955033302
training_time:0.05376935005187988
loss:0.028444819152355194
training_time:0.04776573181152344
loss:0.021788183599710464
training_time:0.05313611030578613
loss:0.01987289823591709
training_time:0.04775381088256836
loss:0.01543277408927679
training_time:0.051025390625
loss:0.012902223505079746
training_time:0.047806739807128906
loss:0.010765651240944862
training_time:0.05130791664123535
loss:0.010566233657300472
training_time:0.0527491569519043
loss:0.009249898605048656
training_time:0.04806208610534668
loss:0.0077514247968792915
training_time:0.053150177001953125
loss:0.008138382807374
training_time:0.05447745323181152
loss:0.006988064851611853
training_time:0.04837846755981445
loss:0.006385909393429756
training_time:0.05300307273864746
loss:0.006649037357419729
training_time:0.04829692840576172
loss:0.006366291083395481
training_time:0.053165435791015625
loss:0.006379522383213043
{'APL': 45.7002457002457, 'CMT': 60.33519553072626, 'DSC': 29.807692307692307, 'MAT': 42.18258132214061, 'PRO': 22.244094488188974, 'SMT': 45.04504504504504, 'SPL': 58.823529411764696, 'macro_f1': 0.43448340543686237, 'micro_f1': 0.377841651424092}
