Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
23 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.09470462799072266
loss:1.9601356983184814
training_time:0.060050249099731445
loss:1.1182767152786255
training_time:0.055715084075927734
loss:0.7384602427482605
training_time:0.05209207534790039
loss:0.5958288311958313
training_time:0.05143237113952637
loss:0.46187159419059753
training_time:0.05154752731323242
loss:0.3598867952823639
training_time:0.05147361755371094
loss:0.2694874107837677
training_time:0.05133414268493652
loss:0.20087966322898865
training_time:0.05133414268493652
loss:0.14685416221618652
training_time:0.05128312110900879
loss:0.11376676708459854
training_time:0.05139875411987305
loss:0.08137592673301697
training_time:0.05129742622375488
loss:0.06019677221775055
training_time:0.05150890350341797
loss:0.049227274954319
training_time:0.05125617980957031
loss:0.03630378097295761
training_time:0.05127263069152832
loss:0.028786785900592804
training_time:0.05128121376037598
loss:0.02318502590060234
training_time:0.051247596740722656
loss:0.019597958773374557
training_time:0.05143594741821289
loss:0.016595201566815376
training_time:0.05127072334289551
loss:0.016502685844898224
training_time:0.05130958557128906
loss:0.011712188832461834
training_time:0.05608248710632324
loss:0.009981938637793064
training_time:0.05167126655578613
loss:0.008425573818385601
training_time:0.051700592041015625
loss:0.008829988539218903
training_time:0.05185747146606445
loss:0.007388508412986994
training_time:0.05210614204406738
loss:0.007577444426715374
training_time:0.05199098587036133
loss:0.0066262464970350266
training_time:0.05319046974182129
loss:0.006476876325905323
training_time:0.05194544792175293
loss:0.006363861262798309
training_time:0.054839134216308594
loss:0.006003176793456078
training_time:0.05187535285949707
loss:0.005882119294255972
{'APL': 40.87591240875913, 'CMT': 53.39578454332553, 'DSC': 35.71428571428571, 'MAT': 62.48995983935743, 'PRO': 24.264705882352942, 'SMT': 33.42465753424657, 'SPL': 64.38356164383562, 'macro_f1': 0.4493555250945185, 'micro_f1': 0.43164835164835164}
