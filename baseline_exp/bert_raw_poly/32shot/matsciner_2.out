Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
72 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.2415764331817627
loss:0.6997385621070862
training_time:0.1957554817199707
loss:0.6439303159713745
training_time:0.1925663948059082
loss:0.37308380007743835
training_time:0.19288229942321777
loss:0.25553372502326965
training_time:0.18906283378601074
loss:0.07309680432081223
training_time:0.19810724258422852
loss:0.08442199230194092
training_time:0.1874065399169922
loss:0.024929458275437355
training_time:0.1967167854309082
loss:0.01317343208938837
training_time:0.18639135360717773
loss:0.01429956965148449
training_time:0.1979997158050537
loss:0.0033769721630960703
training_time:0.18689370155334473
loss:0.002700262237340212
training_time:0.19630956649780273
loss:0.003533668350428343
training_time:0.18722891807556152
loss:0.0018207416869699955
training_time:0.1956043243408203
loss:0.0017072833143174648
training_time:0.1856687068939209
loss:0.0014939578250050545
training_time:0.1971275806427002
loss:0.0018614019500091672
training_time:0.1870555877685547
loss:0.0016291617648676038
training_time:0.19608092308044434
loss:0.0013534867903217673
training_time:0.18731451034545898
loss:0.001371457357890904
training_time:0.19580626487731934
loss:0.0013025515945628285
training_time:0.1865689754486084
loss:0.0016034962609410286
training_time:0.19876861572265625
loss:0.0015569169772788882
training_time:0.18595528602600098
loss:0.010211847722530365
training_time:0.19803166389465332
loss:0.003903315868228674
training_time:0.1979982852935791
loss:0.0011988406768068671
training_time:0.1883707046508789
loss:0.0012372613418847322
training_time:0.19143009185791016
loss:0.0011698849266394973
training_time:0.1898033618927002
loss:0.0013176664942875504
training_time:0.18845558166503906
loss:0.001102483132854104
training_time:0.20236539840698242
loss:0.0012314587365835905
{'APL': 66.2379421221865, 'CMT': 66.1904761904762, 'DSC': 59.78784956605594, 'MAT': 71.27583749109053, 'PRO': 50.831353919239916, 'SMT': 63.953488372093034, 'SPL': 57.746478873239425, 'macro_f1': 0.6228906093348308, 'micro_f1': 0.6195121951219513}
