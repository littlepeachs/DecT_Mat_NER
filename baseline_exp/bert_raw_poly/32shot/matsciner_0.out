Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
77 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.23317599296569824
loss:1.2141644954681396
training_time:0.19634652137756348
loss:0.5850849151611328
training_time:0.19597625732421875
loss:0.333673357963562
training_time:0.19717073440551758
loss:0.20636576414108276
training_time:0.1970522403717041
loss:0.08402484655380249
training_time:0.19663095474243164
loss:0.04119233414530754
training_time:0.19589686393737793
loss:0.014744587242603302
training_time:0.19737815856933594
loss:0.01505791675299406
training_time:0.19789528846740723
loss:0.006748870946466923
training_time:0.1977710723876953
loss:0.00437719002366066
training_time:0.19725823402404785
loss:0.0025065727531909943
training_time:0.19741606712341309
loss:0.014620634727180004
training_time:0.19782686233520508
loss:0.00736768776550889
training_time:0.1978013515472412
loss:0.0024291821755468845
training_time:0.19684410095214844
loss:0.0019044000655412674
training_time:0.19731879234313965
loss:0.001627263962291181
training_time:0.19743561744689941
loss:0.0016368639189749956
training_time:0.19772958755493164
loss:0.0013761657755821943
training_time:0.19785451889038086
loss:0.001348965335637331
training_time:0.197249174118042
loss:0.001205427455715835
training_time:0.19753718376159668
loss:0.0012794716749340296
training_time:0.20527911186218262
loss:0.001202080282382667
training_time:0.19760584831237793
loss:0.001326457830145955
training_time:0.20388174057006836
loss:0.001275307615287602
training_time:0.19781851768493652
loss:0.0012454178649932146
training_time:0.20443224906921387
loss:0.0011011887108907104
training_time:0.20473480224609375
loss:0.001141650602221489
training_time:0.20898008346557617
loss:0.0011509930482134223
training_time:0.19869589805603027
loss:0.0010759218130260706
training_time:0.1972503662109375
loss:0.0011388321872800589
{'APL': 64.797507788162, 'CMT': 72.3514211886305, 'DSC': 56.34824667472793, 'MAT': 77.16643741403026, 'PRO': 59.20223932820154, 'SMT': 63.02083333333333, 'SPL': 55.38461538461539, 'macro_f1': 0.6403875730167157, 'micro_f1': 0.6561232765612328}
