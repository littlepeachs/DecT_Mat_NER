Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
6 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.09740495681762695
loss:2.128114700317383
training_time:0.05339550971984863
loss:1.1953405141830444
training_time:0.04323291778564453
loss:0.677524745464325
training_time:0.05112791061401367
loss:0.5131272673606873
training_time:0.04137611389160156
loss:0.38528504967689514
training_time:0.04933524131774902
loss:0.2801181375980377
training_time:0.052880048751831055
loss:0.2002471536397934
training_time:0.0520319938659668
loss:0.13495765626430511
training_time:0.04130697250366211
loss:0.08842877298593521
training_time:0.04873061180114746
loss:0.06526941806077957
training_time:0.04994988441467285
loss:0.040346410125494
training_time:0.04412221908569336
loss:0.026400551199913025
training_time:0.047759056091308594
loss:0.020038321614265442
training_time:0.04315781593322754
loss:0.01447127666324377
training_time:0.048656463623046875
loss:0.013042275793850422
training_time:0.04133939743041992
loss:0.010837611742317677
training_time:0.05327892303466797
loss:0.009428993798792362
training_time:0.042189598083496094
loss:0.008289815858006477
training_time:0.04953718185424805
loss:0.007836020551621914
training_time:0.04395937919616699
loss:0.006754626985639334
training_time:0.0488433837890625
loss:0.005824686493724585
training_time:0.0413661003112793
loss:0.005219698883593082
training_time:0.049028635025024414
loss:0.0052860151045024395
training_time:0.04144096374511719
loss:0.004807443358004093
training_time:0.0494074821472168
loss:0.004410658963024616
training_time:0.04162192344665527
loss:0.004424315877258778
training_time:0.051610469818115234
loss:0.004525705240666866
training_time:0.041555166244506836
loss:0.004174825269728899
training_time:0.04448366165161133
loss:0.004313452169299126
training_time:0.0508573055267334
loss:0.003942257724702358
{'APL': 23.744292237442924, 'CMT': 2.790697674418605, 'DSC': 19.10569105691057, 'MAT': 36.84210526315789, 'PRO': 2.208588957055215, 'SMT': 17.692307692307693, 'SPL': 50.98039215686274, 'macro_f1': 0.2190915357687938, 'micro_f1': 0.2034346103038309}
