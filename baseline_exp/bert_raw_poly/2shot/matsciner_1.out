Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
9 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.12365484237670898
loss:2.3618884086608887
training_time:0.04188132286071777
loss:1.2258294820785522
training_time:0.048050642013549805
loss:0.5253753066062927
training_time:0.039160966873168945
loss:0.43145430088043213
training_time:0.052788496017456055
loss:0.3322176933288574
training_time:0.04256296157836914
loss:0.2547272741794586
training_time:0.04727530479431152
loss:0.20906640589237213
training_time:0.040369510650634766
loss:0.16055883467197418
training_time:0.047202348709106445
loss:0.12691423296928406
training_time:0.03919267654418945
loss:0.09625738859176636
training_time:0.04794669151306152
loss:0.07588149607181549
training_time:0.04104137420654297
loss:0.05168908089399338
training_time:0.04239773750305176
loss:0.0422060489654541
training_time:0.039076805114746094
loss:0.03375076502561569
training_time:0.04008126258850098
loss:0.027255503460764885
training_time:0.04020833969116211
loss:0.024388514459133148
training_time:0.04021954536437988
loss:0.017016295343637466
training_time:0.039284467697143555
loss:0.01401038933545351
training_time:0.04236912727355957
loss:0.012293808162212372
training_time:0.03898739814758301
loss:0.01050125528126955
training_time:0.04020977020263672
loss:0.01032014936208725
training_time:0.04023408889770508
loss:0.008938781917095184
training_time:0.0392148494720459
loss:0.008347369730472565
training_time:0.04176497459411621
loss:0.007772025652229786
training_time:0.04233193397521973
loss:0.006452158559113741
training_time:0.041857004165649414
loss:0.006727260071784258
training_time:0.04161524772644043
loss:0.006387622095644474
training_time:0.042884111404418945
loss:0.006129198241978884
training_time:0.0416865348815918
loss:0.00557385478168726
training_time:0.04102277755737305
loss:0.0054859877564013
{'APL': 5.2356020942408374, 'CMT': 0.0, 'DSC': 1.36986301369863, 'MAT': 38.958333333333336, 'PRO': 2.981366459627329, 'SMT': 2.247191011235955, 'SPL': 39.603960396039604, 'macro_f1': 0.12913759472596528, 'micro_f1': 0.15814917127071823}
