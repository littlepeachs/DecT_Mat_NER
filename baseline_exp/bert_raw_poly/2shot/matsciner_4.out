Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
6 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.1089925765991211
loss:2.3883140087127686
training_time:0.04228997230529785
loss:1.3181722164154053
training_time:0.046349525451660156
loss:0.7093885540962219
training_time:0.03932523727416992
loss:0.5092154741287231
training_time:0.04900670051574707
loss:0.3581102192401886
training_time:0.039270877838134766
loss:0.2453550547361374
training_time:0.047689199447631836
loss:0.18411190807819366
training_time:0.03939008712768555
loss:0.11650650203227997
training_time:0.04791975021362305
loss:0.08812590688467026
training_time:0.03920459747314453
loss:0.05651295930147171
training_time:0.04774117469787598
loss:0.04160445183515549
training_time:0.04056596755981445
loss:0.028508229181170464
training_time:0.059210777282714844
loss:0.024241292849183083
training_time:0.0430147647857666
loss:0.020386764779686928
training_time:0.04707026481628418
loss:0.022281473502516747
training_time:0.04067420959472656
loss:0.010922380723059177
training_time:0.04799008369445801
loss:0.010621770285069942
training_time:0.03956151008605957
loss:0.011655043810606003
training_time:0.04907798767089844
loss:0.008495938964188099
training_time:0.03950834274291992
loss:0.00678409356623888
training_time:0.04867839813232422
loss:0.00677943741902709
training_time:0.04965829849243164
loss:0.0061404029838740826
training_time:0.04061102867126465
loss:0.006298081949353218
training_time:0.04770994186401367
loss:0.005370522383600473
training_time:0.04044795036315918
loss:0.004910843912512064
training_time:0.0476992130279541
loss:0.00490444665774703
training_time:0.03954291343688965
loss:0.0047114756889641285
training_time:0.049117326736450195
loss:0.0047587184235453606
training_time:0.03979778289794922
loss:0.004373192321509123
training_time:0.052567243576049805
loss:0.004864242393523455
{'APL': 15.151515151515152, 'CMT': 11.715481171548117, 'DSC': 3.3333333333333335, 'MAT': 37.92325056433408, 'PRO': 0.24937655860349126, 'SMT': 11.981566820276498, 'SPL': 42.27642276422764, 'macro_f1': 0.17518706623405475, 'micro_f1': 0.16605778811026237}
