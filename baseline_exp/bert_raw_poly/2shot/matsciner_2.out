Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
7 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.09824705123901367
loss:1.8291504383087158
training_time:0.03933119773864746
loss:0.9650083184242249
training_time:0.04287385940551758
loss:0.5258964896202087
training_time:0.04560542106628418
loss:0.36835601925849915
training_time:0.04014420509338379
loss:0.26901212334632874
training_time:0.043283939361572266
loss:0.1928638219833374
training_time:0.03856039047241211
loss:0.138620987534523
training_time:0.045224905014038086
loss:0.1014586016535759
training_time:0.03817582130432129
loss:0.06700589507818222
training_time:0.04353618621826172
loss:0.042568616569042206
training_time:0.03766679763793945
loss:0.03323247656226158
training_time:0.042569637298583984
loss:0.023634223267436028
training_time:0.03744840621948242
loss:0.02581036277115345
training_time:0.044158220291137695
loss:0.014453643001616001
training_time:0.03702521324157715
loss:0.011675163172185421
training_time:0.043395042419433594
loss:0.009219268336892128
training_time:0.037798404693603516
loss:0.007563531864434481
training_time:0.04335641860961914
loss:0.007601302117109299
training_time:0.03663325309753418
loss:0.006252839230000973
training_time:0.04241013526916504
loss:0.00542329391464591
training_time:0.03598284721374512
loss:0.005754901561886072
training_time:0.04214620590209961
loss:0.004581142216920853
training_time:0.0358278751373291
loss:0.004621744155883789
training_time:0.03974771499633789
loss:0.004655392374843359
training_time:0.0499110221862793
loss:0.003990502096712589
training_time:0.039043426513671875
loss:0.004194935318082571
training_time:0.04844355583190918
loss:0.00468532694503665
training_time:0.03814053535461426
loss:0.0037027322687208652
training_time:0.04698920249938965
loss:0.0039306459948420525
training_time:0.03921937942504883
loss:0.0037136883474886417
{'APL': 25.751072961373396, 'CMT': 22.36842105263158, 'DSC': 31.724137931034484, 'MAT': 31.294117647058826, 'PRO': 0.7453416149068323, 'SMT': 14.184397163120568, 'SPL': 26.31578947368421, 'macro_f1': 0.21769039691972839, 'micro_f1': 0.20643939393939395}
