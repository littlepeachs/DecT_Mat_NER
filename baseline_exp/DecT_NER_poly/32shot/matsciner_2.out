/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:31:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:31:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-145490b5457c9ad0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 852.24it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:46 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:46 - INFO - __main__ -   Num examples = 72
06/01/2023 02:31:46 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:46 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:46 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:46 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:46 - INFO - __main__ -   Total optimization steps = 300
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.95658302307129
Total epoch: 1. epoch loss: 16.105379104614258
Total epoch: 2. epoch loss: 15.31206226348877
Total epoch: 3. epoch loss: 14.5784912109375
Total epoch: 4. epoch loss: 13.903925895690918
Total epoch: 5. epoch loss: 13.284821510314941
Total epoch: 6. epoch loss: 12.715221405029297
Total epoch: 7. epoch loss: 12.187942504882812
Total epoch: 8. epoch loss: 11.695907592773438
Total epoch: 9. epoch loss: 11.233004570007324
Total epoch: 10. epoch loss: 10.794343948364258
Total epoch: 11. epoch loss: 10.37598705291748
Total epoch: 12. epoch loss: 9.974835395812988
Total epoch: 13. epoch loss: 9.588432312011719
Total epoch: 14. epoch loss: 9.21483325958252
Total epoch: 15. epoch loss: 8.852622985839844
Total epoch: 16. epoch loss: 8.500788688659668
Total epoch: 17. epoch loss: 8.158738136291504
Total epoch: 18. epoch loss: 7.826229572296143
Total epoch: 19. epoch loss: 7.503298282623291
Total epoch: 20. epoch loss: 7.190229415893555
Total epoch: 21. epoch loss: 6.887485980987549
Total epoch: 22. epoch loss: 6.595618724822998
Total epoch: 23. epoch loss: 6.315147876739502
Total epoch: 24. epoch loss: 6.046431064605713
Total epoch: 25. epoch loss: 5.789673328399658
Total epoch: 26. epoch loss: 5.544739246368408
Total epoch: 27. epoch loss: 5.311217784881592
Total epoch: 28. epoch loss: 5.0895094871521
Total epoch: 29. epoch loss: 4.879586696624756
Total epoch: 30. epoch loss: 4.681118488311768
Total epoch: 31. epoch loss: 4.493587493896484
Total epoch: 32. epoch loss: 4.316478252410889
Total epoch: 33. epoch loss: 4.1492533683776855
Total epoch: 34. epoch loss: 3.9913923740386963
Total epoch: 35. epoch loss: 3.842351198196411
Total epoch: 36. epoch loss: 3.7015955448150635
Total epoch: 37. epoch loss: 3.5686428546905518
Total epoch: 38. epoch loss: 3.443047523498535
Total epoch: 39. epoch loss: 3.3243889808654785
Total epoch: 40. epoch loss: 3.2123379707336426
Total epoch: 41. epoch loss: 3.106548547744751
Total epoch: 42. epoch loss: 3.0067028999328613
Total epoch: 43. epoch loss: 2.9124374389648438
Total epoch: 44. epoch loss: 2.82340669631958
Total epoch: 45. epoch loss: 2.739267110824585
Total epoch: 46. epoch loss: 2.659684896469116
Total epoch: 47. epoch loss: 2.5843658447265625
Total epoch: 48. epoch loss: 2.513012409210205
Total epoch: 49. epoch loss: 2.4454193115234375
Total epoch: 50. epoch loss: 2.3813209533691406
Total epoch: 51. epoch loss: 2.3205130100250244
Total epoch: 52. epoch loss: 2.26279616355896
Total epoch: 53. epoch loss: 2.207976818084717
Total epoch: 54. epoch loss: 2.155851125717163
Total epoch: 55. epoch loss: 2.1062514781951904
Total epoch: 56. epoch loss: 2.059034824371338
Total epoch: 57. epoch loss: 2.0140414237976074
Total epoch: 58. epoch loss: 1.9711620807647705
Total epoch: 59. epoch loss: 1.9302541017532349
Total epoch: 60. epoch loss: 1.8912056684494019
Total epoch: 61. epoch loss: 1.8539130687713623
Total epoch: 62. epoch loss: 1.8182634115219116
Total epoch: 63. epoch loss: 1.7841451168060303
Total epoch: 64. epoch loss: 1.751482367515564
Total epoch: 65. epoch loss: 1.7201945781707764
Total epoch: 66. epoch loss: 1.6901757717132568
Total epoch: 67. epoch loss: 1.661391019821167
Total epoch: 68. epoch loss: 1.6337443590164185
Total epoch: 69. epoch loss: 1.607176661491394
Total epoch: 70. epoch loss: 1.58162522315979
Total epoch: 71. epoch loss: 1.5570228099822998
Total epoch: 72. epoch loss: 1.5333296060562134
Total epoch: 73. epoch loss: 1.5104888677597046
Total epoch: 74. epoch loss: 1.4884504079818726
Total epoch: 75. epoch loss: 1.4671791791915894
Total epoch: 76. epoch loss: 1.446629524230957
Total epoch: 77. epoch loss: 1.4267597198486328
Total epoch: 78. epoch loss: 1.4075371026992798
Total epoch: 79. epoch loss: 1.3889354467391968
Total epoch: 80. epoch loss: 1.370917797088623
Total epoch: 81. epoch loss: 1.353456974029541
Total epoch: 82. epoch loss: 1.3365285396575928
Total epoch: 83. epoch loss: 1.3201085329055786
Total epoch: 84. epoch loss: 1.304175615310669
Total epoch: 85. epoch loss: 1.2887040376663208
Total epoch: 86. epoch loss: 1.2736729383468628
Total epoch: 87. epoch loss: 1.2590550184249878
Total epoch: 88. epoch loss: 1.244850516319275
Total epoch: 89. epoch loss: 1.2310187816619873
Total epoch: 90. epoch loss: 1.2175607681274414
Total epoch: 91. epoch loss: 1.2044490575790405
Total epoch: 92. epoch loss: 1.1916855573654175
Total epoch: 93. epoch loss: 1.1792446374893188
Total epoch: 94. epoch loss: 1.1671116352081299
Total epoch: 95. epoch loss: 1.1552802324295044
Total epoch: 96. epoch loss: 1.1437373161315918
Total epoch: 97. epoch loss: 1.1324708461761475
Total epoch: 98. epoch loss: 1.1214725971221924
Total epoch: 99. epoch loss: 1.1107301712036133
Total epoch: 99. DecT loss: 1.1107301712036133
Training time: 0.5582726001739502
APL_precision: 0.20766773162939298, APL_recall: 0.38235294117647056, APL_f1: 0.2691511387163561, APL_number: 170
CMT_precision: 0.24481327800829875, CMT_recall: 0.6051282051282051, CMT_f1: 0.3485967503692762, CMT_number: 195
DSC_precision: 0.3746130030959752, DSC_recall: 0.5537757437070938, DSC_f1: 0.44690674053554935, DSC_number: 437
MAT_precision: 0.5513784461152882, MAT_recall: 0.6451612903225806, MAT_f1: 0.5945945945945945, MAT_number: 682
PRO_precision: 0.4943820224719101, PRO_recall: 0.4565499351491569, PRO_f1: 0.47471341874578554, PRO_number: 771
SMT_precision: 0.3471502590673575, SMT_recall: 0.391812865497076, SMT_f1: 0.36813186813186816, SMT_number: 171
SPL_precision: 0.6739130434782609, SPL_recall: 0.41333333333333333, SPL_f1: 0.512396694214876, SPL_number: 75
overall_precision: 0.4122257053291536, overall_recall: 0.5257896841263494, overall_f1: 0.462133192760499, overall_accuracy: 0.8122364377099563
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]