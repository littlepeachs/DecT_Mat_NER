/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:31:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:31:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1259.74it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:44 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:44 - INFO - __main__ -   Num examples = 77
06/01/2023 02:31:44 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:44 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:44 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:44 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:44 - INFO - __main__ -   Total optimization steps = 300
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 17.834627151489258
Total epoch: 1. epoch loss: 16.986427307128906
Total epoch: 2. epoch loss: 16.188453674316406
Total epoch: 3. epoch loss: 15.4431791305542
Total epoch: 4. epoch loss: 14.75119686126709
Total epoch: 5. epoch loss: 14.110657691955566
Total epoch: 6. epoch loss: 13.517399787902832
Total epoch: 7. epoch loss: 12.965715408325195
Total epoch: 8. epoch loss: 12.449538230895996
Total epoch: 9. epoch loss: 11.963312149047852
Total epoch: 10. epoch loss: 11.502303123474121
Total epoch: 11. epoch loss: 11.0625638961792
Total epoch: 12. epoch loss: 10.640727996826172
Total epoch: 13. epoch loss: 10.23398494720459
Total epoch: 14. epoch loss: 9.840167999267578
Total epoch: 15. epoch loss: 9.457645416259766
Total epoch: 16. epoch loss: 9.085433006286621
Total epoch: 17. epoch loss: 8.72297191619873
Total epoch: 18. epoch loss: 8.370068550109863
Total epoch: 19. epoch loss: 8.026784896850586
Total epoch: 20. epoch loss: 7.693325519561768
Total epoch: 21. epoch loss: 7.3700361251831055
Total epoch: 22. epoch loss: 7.057252883911133
Total epoch: 23. epoch loss: 6.755431175231934
Total epoch: 24. epoch loss: 6.464867115020752
Total epoch: 25. epoch loss: 6.185920715332031
Total epoch: 26. epoch loss: 5.918886661529541
Total epoch: 27. epoch loss: 5.663908004760742
Total epoch: 28. epoch loss: 5.420986652374268
Total epoch: 29. epoch loss: 5.190157413482666
Total epoch: 30. epoch loss: 4.972111701965332
Total epoch: 31. epoch loss: 4.766810417175293
Total epoch: 32. epoch loss: 4.573667049407959
Total epoch: 33. epoch loss: 4.391793251037598
Total epoch: 34. epoch loss: 4.220316410064697
Total epoch: 35. epoch loss: 4.058346271514893
Total epoch: 36. epoch loss: 3.9052374362945557
Total epoch: 37. epoch loss: 3.7605092525482178
Total epoch: 38. epoch loss: 3.6237523555755615
Total epoch: 39. epoch loss: 3.494663715362549
Total epoch: 40. epoch loss: 3.373006582260132
Total epoch: 41. epoch loss: 3.25850248336792
Total epoch: 42. epoch loss: 3.150866985321045
Total epoch: 43. epoch loss: 3.0497500896453857
Total epoch: 44. epoch loss: 2.9547626972198486
Total epoch: 45. epoch loss: 2.865487813949585
Total epoch: 46. epoch loss: 2.7814512252807617
Total epoch: 47. epoch loss: 2.7022242546081543
Total epoch: 48. epoch loss: 2.627394437789917
Total epoch: 49. epoch loss: 2.556635856628418
Total epoch: 50. epoch loss: 2.489654779434204
Total epoch: 51. epoch loss: 2.426203489303589
Total epoch: 52. epoch loss: 2.3660809993743896
Total epoch: 53. epoch loss: 2.3090858459472656
Total epoch: 54. epoch loss: 2.255030870437622
Total epoch: 55. epoch loss: 2.2037100791931152
Total epoch: 56. epoch loss: 2.154953718185425
Total epoch: 57. epoch loss: 2.10858154296875
Total epoch: 58. epoch loss: 2.0644381046295166
Total epoch: 59. epoch loss: 2.022353410720825
Total epoch: 60. epoch loss: 1.9821950197219849
Total epoch: 61. epoch loss: 1.9438250064849854
Total epoch: 62. epoch loss: 1.9071249961853027
Total epoch: 63. epoch loss: 1.87197744846344
Total epoch: 64. epoch loss: 1.8382716178894043
Total epoch: 65. epoch loss: 1.8059414625167847
Total epoch: 66. epoch loss: 1.7748970985412598
Total epoch: 67. epoch loss: 1.7450674772262573
Total epoch: 68. epoch loss: 1.7163949012756348
Total epoch: 69. epoch loss: 1.6888269186019897
Total epoch: 70. epoch loss: 1.662300705909729
Total epoch: 71. epoch loss: 1.6367568969726562
Total epoch: 72. epoch loss: 1.6121513843536377
Total epoch: 73. epoch loss: 1.5884180068969727
Total epoch: 74. epoch loss: 1.5655181407928467
Total epoch: 75. epoch loss: 1.5433893203735352
Total epoch: 76. epoch loss: 1.5219966173171997
Total epoch: 77. epoch loss: 1.5013141632080078
Total epoch: 78. epoch loss: 1.4812815189361572
Total epoch: 79. epoch loss: 1.4618960618972778
Total epoch: 80. epoch loss: 1.443098783493042
Total epoch: 81. epoch loss: 1.4248907566070557
Total epoch: 82. epoch loss: 1.4072233438491821
Total epoch: 83. epoch loss: 1.390083909034729
Total epoch: 84. epoch loss: 1.3734517097473145
Total epoch: 85. epoch loss: 1.3573018312454224
Total epoch: 86. epoch loss: 1.3416107892990112
Total epoch: 87. epoch loss: 1.3263598680496216
Total epoch: 88. epoch loss: 1.3115308284759521
Total epoch: 89. epoch loss: 1.2970998287200928
Total epoch: 90. epoch loss: 1.2830523252487183
Total epoch: 91. epoch loss: 1.2693698406219482
Total epoch: 92. epoch loss: 1.256039023399353
Total epoch: 93. epoch loss: 1.2430378198623657
Total epoch: 94. epoch loss: 1.230371356010437
Total epoch: 95. epoch loss: 1.218013048171997
Total epoch: 96. epoch loss: 1.2059513330459595
Total epoch: 97. epoch loss: 1.1941804885864258
Total epoch: 98. epoch loss: 1.1826850175857544
Total epoch: 99. epoch loss: 1.1714626550674438
Total epoch: 99. DecT loss: 1.1714626550674438
Training time: 0.5581395626068115
APL_precision: 0.2508710801393728, APL_recall: 0.4235294117647059, APL_f1: 0.31509846827133475, APL_number: 170
CMT_precision: 0.2668269230769231, CMT_recall: 0.5692307692307692, CMT_f1: 0.36333878887070375, CMT_number: 195
DSC_precision: 0.45038167938931295, DSC_recall: 0.540045766590389, DSC_f1: 0.4911550468262227, DSC_number: 437
MAT_precision: 0.49876237623762376, MAT_recall: 0.5909090909090909, MAT_f1: 0.5409395973154363, MAT_number: 682
PRO_precision: 0.3762057877813505, PRO_recall: 0.45525291828793774, PRO_f1: 0.41197183098591555, PRO_number: 771
SMT_precision: 0.36160714285714285, SMT_recall: 0.47368421052631576, SMT_f1: 0.41012658227848103, SMT_number: 171
SPL_precision: 0.49019607843137253, SPL_recall: 0.3333333333333333, SPL_f1: 0.3968253968253968, SPL_number: 75
overall_precision: 0.39438791242676535, overall_recall: 0.5113954418232707, overall_f1: 0.44533426183844016, overall_accuracy: 0.8203130583946823
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]