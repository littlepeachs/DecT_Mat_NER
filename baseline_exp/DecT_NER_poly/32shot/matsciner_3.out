/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:31:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:31:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-21be78dd80d291c5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1167.19it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/66 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4893.06 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:45 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:45 - INFO - __main__ -   Num examples = 66
06/01/2023 02:31:45 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:45 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:45 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:45 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:45 - INFO - __main__ -   Total optimization steps = 300
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.54023551940918
Total epoch: 1. epoch loss: 15.751877784729004
Total epoch: 2. epoch loss: 15.009222984313965
Total epoch: 3. epoch loss: 14.312082290649414
Total epoch: 4. epoch loss: 13.65900707244873
Total epoch: 5. epoch loss: 13.04763126373291
Total epoch: 6. epoch loss: 12.474923133850098
Total epoch: 7. epoch loss: 11.937427520751953
Total epoch: 8. epoch loss: 11.431410789489746
Total epoch: 9. epoch loss: 10.953141212463379
Total epoch: 10. epoch loss: 10.498859405517578
Total epoch: 11. epoch loss: 10.065020561218262
Total epoch: 12. epoch loss: 9.648425102233887
Total epoch: 13. epoch loss: 9.246461868286133
Total epoch: 14. epoch loss: 8.857255935668945
Total epoch: 15. epoch loss: 8.479656219482422
Total epoch: 16. epoch loss: 8.113237380981445
Total epoch: 17. epoch loss: 7.758090019226074
Total epoch: 18. epoch loss: 7.41462516784668
Total epoch: 19. epoch loss: 7.083510875701904
Total epoch: 20. epoch loss: 6.765398979187012
Total epoch: 21. epoch loss: 6.460753917694092
Total epoch: 22. epoch loss: 6.169828414916992
Total epoch: 23. epoch loss: 5.892626762390137
Total epoch: 24. epoch loss: 5.628852844238281
Total epoch: 25. epoch loss: 5.378051280975342
Total epoch: 26. epoch loss: 5.139645099639893
Total epoch: 27. epoch loss: 4.9135212898254395
Total epoch: 28. epoch loss: 4.699877738952637
Total epoch: 29. epoch loss: 4.498588562011719
Total epoch: 30. epoch loss: 4.309311389923096
Total epoch: 31. epoch loss: 4.131503582000732
Total epoch: 32. epoch loss: 3.9645869731903076
Total epoch: 33. epoch loss: 3.8078949451446533
Total epoch: 34. epoch loss: 3.6607351303100586
Total epoch: 35. epoch loss: 3.522475481033325
Total epoch: 36. epoch loss: 3.3924903869628906
Total epoch: 37. epoch loss: 3.2702410221099854
Total epoch: 38. epoch loss: 3.155207395553589
Total epoch: 39. epoch loss: 3.0469307899475098
Total epoch: 40. epoch loss: 2.944985866546631
Total epoch: 41. epoch loss: 2.848961591720581
Total epoch: 42. epoch loss: 2.758500814437866
Total epoch: 43. epoch loss: 2.673213005065918
Total epoch: 44. epoch loss: 2.592766284942627
Total epoch: 45. epoch loss: 2.5168426036834717
Total epoch: 46. epoch loss: 2.445125102996826
Total epoch: 47. epoch loss: 2.377333879470825
Total epoch: 48. epoch loss: 2.3131933212280273
Total epoch: 49. epoch loss: 2.2524588108062744
Total epoch: 50. epoch loss: 2.194873094558716
Total epoch: 51. epoch loss: 2.1402223110198975
Total epoch: 52. epoch loss: 2.088279962539673
Total epoch: 53. epoch loss: 2.038877010345459
Total epoch: 54. epoch loss: 1.991827368736267
Total epoch: 55. epoch loss: 1.9469863176345825
Total epoch: 56. epoch loss: 1.904194712638855
Total epoch: 57. epoch loss: 1.8633302450180054
Total epoch: 58. epoch loss: 1.8242676258087158
Total epoch: 59. epoch loss: 1.786895513534546
Total epoch: 60. epoch loss: 1.751102328300476
Total epoch: 61. epoch loss: 1.7167855501174927
Total epoch: 62. epoch loss: 1.6838465929031372
Total epoch: 63. epoch loss: 1.6522184610366821
Total epoch: 64. epoch loss: 1.621800184249878
Total epoch: 65. epoch loss: 1.5925583839416504
Total epoch: 66. epoch loss: 1.564410924911499
Total epoch: 67. epoch loss: 1.537305235862732
Total epoch: 68. epoch loss: 1.5111840963363647
Total epoch: 69. epoch loss: 1.4860142469406128
Total epoch: 70. epoch loss: 1.4617414474487305
Total epoch: 71. epoch loss: 1.4383231401443481
Total epoch: 72. epoch loss: 1.4157143831253052
Total epoch: 73. epoch loss: 1.3938755989074707
Total epoch: 74. epoch loss: 1.372766137123108
Total epoch: 75. epoch loss: 1.3523640632629395
Total epoch: 76. epoch loss: 1.332621455192566
Total epoch: 77. epoch loss: 1.313510775566101
Total epoch: 78. epoch loss: 1.2950077056884766
Total epoch: 79. epoch loss: 1.2770742177963257
Total epoch: 80. epoch loss: 1.259695291519165
Total epoch: 81. epoch loss: 1.2428556680679321
Total epoch: 82. epoch loss: 1.2265093326568604
Total epoch: 83. epoch loss: 1.210645079612732
Total epoch: 84. epoch loss: 1.1952464580535889
Total epoch: 85. epoch loss: 1.1802892684936523
Total epoch: 86. epoch loss: 1.1657538414001465
Total epoch: 87. epoch loss: 1.1516125202178955
Total epoch: 88. epoch loss: 1.1378575563430786
Total epoch: 89. epoch loss: 1.124468445777893
Total epoch: 90. epoch loss: 1.111436128616333
Total epoch: 91. epoch loss: 1.0987372398376465
Total epoch: 92. epoch loss: 1.0863604545593262
Total epoch: 93. epoch loss: 1.074293613433838
Total epoch: 94. epoch loss: 1.0625203847885132
Total epoch: 95. epoch loss: 1.0510414838790894
Total epoch: 96. epoch loss: 1.039833426475525
Total epoch: 97. epoch loss: 1.0288885831832886
Total epoch: 98. epoch loss: 1.0182008743286133
Total epoch: 99. epoch loss: 1.0077598094940186
Total epoch: 99. DecT loss: 1.0077598094940186
Training time: 0.43854689598083496
APL_precision: 0.3054545454545455, APL_recall: 0.49411764705882355, APL_f1: 0.37752808988764053, APL_number: 170
CMT_precision: 0.30927835051546393, CMT_recall: 0.6153846153846154, CMT_f1: 0.411663807890223, CMT_number: 195
DSC_precision: 0.3945086705202312, DSC_recall: 0.6247139588100686, DSC_f1: 0.4836138175376439, DSC_number: 437
MAT_precision: 0.5421686746987951, MAT_recall: 0.6598240469208211, MAT_f1: 0.5952380952380952, MAT_number: 682
PRO_precision: 0.5205479452054794, PRO_recall: 0.44357976653696496, PRO_f1: 0.4789915966386554, PRO_number: 771
SMT_precision: 0.365296803652968, SMT_recall: 0.4678362573099415, SMT_f1: 0.41025641025641024, SMT_number: 171
SPL_precision: 0.5272727272727272, SPL_recall: 0.38666666666666666, SPL_f1: 0.4461538461538462, SPL_number: 75
overall_precision: 0.44223363286264444, overall_recall: 0.5509796081567373, overall_f1: 0.4906533736870216, overall_accuracy: 0.8293188478307484
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:02<?, ?it/s]