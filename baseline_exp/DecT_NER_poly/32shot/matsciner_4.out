/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:31:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:31:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 814.19it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:44 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:44 - INFO - __main__ -   Num examples = 72
06/01/2023 02:31:44 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:44 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:44 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:44 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:44 - INFO - __main__ -   Total optimization steps = 300
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 17.448808670043945
Total epoch: 1. epoch loss: 16.653919219970703
Total epoch: 2. epoch loss: 15.900774955749512
Total epoch: 3. epoch loss: 15.191176414489746
Total epoch: 4. epoch loss: 14.525629043579102
Total epoch: 5. epoch loss: 13.903279304504395
Total epoch: 6. epoch loss: 13.321934700012207
Total epoch: 7. epoch loss: 12.778308868408203
Total epoch: 8. epoch loss: 12.268363952636719
Total epoch: 9. epoch loss: 11.78774642944336
Total epoch: 10. epoch loss: 11.332072257995605
Total epoch: 11. epoch loss: 10.897289276123047
Total epoch: 12. epoch loss: 10.479818344116211
Total epoch: 13. epoch loss: 10.076911926269531
Total epoch: 14. epoch loss: 9.686601638793945
Total epoch: 15. epoch loss: 9.307722091674805
Total epoch: 16. epoch loss: 8.939780235290527
Total epoch: 17. epoch loss: 8.582752227783203
Total epoch: 18. epoch loss: 8.2369966506958
Total epoch: 19. epoch loss: 7.9029693603515625
Total epoch: 20. epoch loss: 7.581134796142578
Total epoch: 21. epoch loss: 7.271913528442383
Total epoch: 22. epoch loss: 6.975523948669434
Total epoch: 23. epoch loss: 6.6919965744018555
Total epoch: 24. epoch loss: 6.4211106300354
Total epoch: 25. epoch loss: 6.162507057189941
Total epoch: 26. epoch loss: 5.915710926055908
Total epoch: 27. epoch loss: 5.680146217346191
Total epoch: 28. epoch loss: 5.455897808074951
Total epoch: 29. epoch loss: 5.243009567260742
Total epoch: 30. epoch loss: 5.041252613067627
Total epoch: 31. epoch loss: 4.850254058837891
Total epoch: 32. epoch loss: 4.669490337371826
Total epoch: 33. epoch loss: 4.498516082763672
Total epoch: 34. epoch loss: 4.336835861206055
Total epoch: 35. epoch loss: 4.184021472930908
Total epoch: 36. epoch loss: 4.039638042449951
Total epoch: 37. epoch loss: 3.9032607078552246
Total epoch: 38. epoch loss: 3.77447772026062
Total epoch: 39. epoch loss: 3.652866840362549
Total epoch: 40. epoch loss: 3.538032054901123
Total epoch: 41. epoch loss: 3.4295146465301514
Total epoch: 42. epoch loss: 3.326934814453125
Total epoch: 43. epoch loss: 3.22990345954895
Total epoch: 44. epoch loss: 3.138059139251709
Total epoch: 45. epoch loss: 3.0510432720184326
Total epoch: 46. epoch loss: 2.9685721397399902
Total epoch: 47. epoch loss: 2.8903543949127197
Total epoch: 48. epoch loss: 2.8161213397979736
Total epoch: 49. epoch loss: 2.745661973953247
Total epoch: 50. epoch loss: 2.6787381172180176
Total epoch: 51. epoch loss: 2.615147352218628
Total epoch: 52. epoch loss: 2.5547003746032715
Total epoch: 53. epoch loss: 2.4971923828125
Total epoch: 54. epoch loss: 2.442434072494507
Total epoch: 55. epoch loss: 2.390232563018799
Total epoch: 56. epoch loss: 2.3404202461242676
Total epoch: 57. epoch loss: 2.2928268909454346
Total epoch: 58. epoch loss: 2.247312307357788
Total epoch: 59. epoch loss: 2.2037413120269775
Total epoch: 60. epoch loss: 2.161991834640503
Total epoch: 61. epoch loss: 2.121979236602783
Total epoch: 62. epoch loss: 2.0835976600646973
Total epoch: 63. epoch loss: 2.0467631816864014
Total epoch: 64. epoch loss: 2.0113744735717773
Total epoch: 65. epoch loss: 1.9773807525634766
Total epoch: 66. epoch loss: 1.9446924924850464
Total epoch: 67. epoch loss: 1.9132235050201416
Total epoch: 68. epoch loss: 1.8829103708267212
Total epoch: 69. epoch loss: 1.8536896705627441
Total epoch: 70. epoch loss: 1.825495719909668
Total epoch: 71. epoch loss: 1.7982630729675293
Total epoch: 72. epoch loss: 1.7719347476959229
Total epoch: 73. epoch loss: 1.7464683055877686
Total epoch: 74. epoch loss: 1.7218116521835327
Total epoch: 75. epoch loss: 1.6979280710220337
Total epoch: 76. epoch loss: 1.6747779846191406
Total epoch: 77. epoch loss: 1.6523265838623047
Total epoch: 78. epoch loss: 1.6305466890335083
Total epoch: 79. epoch loss: 1.6093971729278564
Total epoch: 80. epoch loss: 1.58885657787323
Total epoch: 81. epoch loss: 1.5688979625701904
Total epoch: 82. epoch loss: 1.5494906902313232
Total epoch: 83. epoch loss: 1.5306146144866943
Total epoch: 84. epoch loss: 1.5122418403625488
Total epoch: 85. epoch loss: 1.4943536520004272
Total epoch: 86. epoch loss: 1.4769301414489746
Total epoch: 87. epoch loss: 1.4599467515945435
Total epoch: 88. epoch loss: 1.4433952569961548
Total epoch: 89. epoch loss: 1.4272520542144775
Total epoch: 90. epoch loss: 1.4114973545074463
Total epoch: 91. epoch loss: 1.3961304426193237
Total epoch: 92. epoch loss: 1.3811184167861938
Total epoch: 93. epoch loss: 1.3664675951004028
Total epoch: 94. epoch loss: 1.3521486520767212
Total epoch: 95. epoch loss: 1.338160514831543
Total epoch: 96. epoch loss: 1.3244935274124146
Total epoch: 97. epoch loss: 1.3111207485198975
Total epoch: 98. epoch loss: 1.298052191734314
Total epoch: 99. epoch loss: 1.2852656841278076
Total epoch: 99. DecT loss: 1.2852656841278076
Training time: 0.5404860973358154
APL_precision: 0.22184300341296928, APL_recall: 0.38235294117647056, APL_f1: 0.2807775377969763, APL_number: 170
CMT_precision: 0.44696969696969696, CMT_recall: 0.6051282051282051, CMT_f1: 0.5141612200435729, CMT_number: 195
DSC_precision: 0.4239290989660266, DSC_recall: 0.6567505720823799, DSC_f1: 0.5152603231597845, DSC_number: 437
MAT_precision: 0.4549019607843137, MAT_recall: 0.6803519061583577, MAT_f1: 0.54524089306698, MAT_number: 682
PRO_precision: 0.3707482993197279, PRO_recall: 0.42412451361867703, PRO_f1: 0.3956442831215971, PRO_number: 771
SMT_precision: 0.37155963302752293, SMT_recall: 0.47368421052631576, SMT_f1: 0.416452442159383, SMT_number: 171
SPL_precision: 0.38372093023255816, SPL_recall: 0.44, SPL_f1: 0.4099378881987578, SPL_number: 75
overall_precision: 0.3997093023255814, overall_recall: 0.549780087964814, overall_f1: 0.46288503618919374, overall_accuracy: 0.813451504538632
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]