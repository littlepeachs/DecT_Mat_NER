/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:30:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:30:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1198.20it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:05 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:05 - INFO - __main__ -   Num examples = 21
06/01/2023 02:31:05 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:05 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:05 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:05 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:05 - INFO - __main__ -   Total optimization steps = 100
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.675902366638184
Total epoch: 1. epoch loss: 14.812196731567383
Total epoch: 2. epoch loss: 13.977202415466309
Total epoch: 3. epoch loss: 13.179089546203613
Total epoch: 4. epoch loss: 12.42536449432373
Total epoch: 5. epoch loss: 11.719956398010254
Total epoch: 6. epoch loss: 11.061302185058594
Total epoch: 7. epoch loss: 10.443684577941895
Total epoch: 8. epoch loss: 9.860225677490234
Total epoch: 9. epoch loss: 9.305193901062012
Total epoch: 10. epoch loss: 8.774757385253906
Total epoch: 11. epoch loss: 8.266807556152344
Total epoch: 12. epoch loss: 7.780388832092285
Total epoch: 13. epoch loss: 7.315252780914307
Total epoch: 14. epoch loss: 6.87147855758667
Total epoch: 15. epoch loss: 6.449220657348633
Total epoch: 16. epoch loss: 6.0485382080078125
Total epoch: 17. epoch loss: 5.66925573348999
Total epoch: 18. epoch loss: 5.311009407043457
Total epoch: 19. epoch loss: 4.973176002502441
Total epoch: 20. epoch loss: 4.655036449432373
Total epoch: 21. epoch loss: 4.355905055999756
Total epoch: 22. epoch loss: 4.075144290924072
Total epoch: 23. epoch loss: 3.812286138534546
Total epoch: 24. epoch loss: 3.566821813583374
Total epoch: 25. epoch loss: 3.33809494972229
Total epoch: 26. epoch loss: 3.126481056213379
Total epoch: 27. epoch loss: 2.9314279556274414
Total epoch: 28. epoch loss: 2.7518138885498047
Total epoch: 29. epoch loss: 2.5863428115844727
Total epoch: 30. epoch loss: 2.43386173248291
Total epoch: 31. epoch loss: 2.293452739715576
Total epoch: 32. epoch loss: 2.1643457412719727
Total epoch: 33. epoch loss: 2.045870542526245
Total epoch: 34. epoch loss: 1.9372972249984741
Total epoch: 35. epoch loss: 1.8378808498382568
Total epoch: 36. epoch loss: 1.7468037605285645
Total epoch: 37. epoch loss: 1.663315773010254
Total epoch: 38. epoch loss: 1.5866849422454834
Total epoch: 39. epoch loss: 1.5162347555160522
Total epoch: 40. epoch loss: 1.4513970613479614
Total epoch: 41. epoch loss: 1.391629695892334
Total epoch: 42. epoch loss: 1.3364315032958984
Total epoch: 43. epoch loss: 1.2853384017944336
Total epoch: 44. epoch loss: 1.2379310131072998
Total epoch: 45. epoch loss: 1.193833589553833
Total epoch: 46. epoch loss: 1.152718424797058
Total epoch: 47. epoch loss: 1.114317536354065
Total epoch: 48. epoch loss: 1.078386902809143
Total epoch: 49. epoch loss: 1.0447219610214233
Total epoch: 50. epoch loss: 1.0131551027297974
Total epoch: 51. epoch loss: 0.9835100769996643
Total epoch: 52. epoch loss: 0.9556440114974976
Total epoch: 53. epoch loss: 0.9293988943099976
Total epoch: 54. epoch loss: 0.9046582579612732
Total epoch: 55. epoch loss: 0.88128662109375
Total epoch: 56. epoch loss: 0.8591834306716919
Total epoch: 57. epoch loss: 0.8382477760314941
Total epoch: 58. epoch loss: 0.8183916807174683
Total epoch: 59. epoch loss: 0.7995426058769226
Total epoch: 60. epoch loss: 0.7816276550292969
Total epoch: 61. epoch loss: 0.764572262763977
Total epoch: 62. epoch loss: 0.7483335137367249
Total epoch: 63. epoch loss: 0.7328440546989441
Total epoch: 64. epoch loss: 0.718065083026886
Total epoch: 65. epoch loss: 0.7039433717727661
Total epoch: 66. epoch loss: 0.6904423832893372
Total epoch: 67. epoch loss: 0.6775155663490295
Total epoch: 68. epoch loss: 0.665136992931366
Total epoch: 69. epoch loss: 0.653266429901123
Total epoch: 70. epoch loss: 0.641880214214325
Total epoch: 71. epoch loss: 0.6309422850608826
Total epoch: 72. epoch loss: 0.6204276084899902
Total epoch: 73. epoch loss: 0.6103157997131348
Total epoch: 74. epoch loss: 0.6005779504776001
Total epoch: 75. epoch loss: 0.5911849141120911
Total epoch: 76. epoch loss: 0.5821290016174316
Total epoch: 77. epoch loss: 0.5733835697174072
Total epoch: 78. epoch loss: 0.5649287104606628
Total epoch: 79. epoch loss: 0.5567518472671509
Total epoch: 80. epoch loss: 0.5488358736038208
Total epoch: 81. epoch loss: 0.5411676168441772
Total epoch: 82. epoch loss: 0.5337302684783936
Total epoch: 83. epoch loss: 0.5265214443206787
Total epoch: 84. epoch loss: 0.5195198059082031
Total epoch: 85. epoch loss: 0.5127195119857788
Total epoch: 86. epoch loss: 0.5061147212982178
Total epoch: 87. epoch loss: 0.49969372153282166
Total epoch: 88. epoch loss: 0.4934456944465637
Total epoch: 89. epoch loss: 0.48736798763275146
Total epoch: 90. epoch loss: 0.4814475476741791
Total epoch: 91. epoch loss: 0.4756786823272705
Total epoch: 92. epoch loss: 0.4700581729412079
Total epoch: 93. epoch loss: 0.4645754396915436
Total epoch: 94. epoch loss: 0.459229439496994
Total epoch: 95. epoch loss: 0.45401328802108765
Total epoch: 96. epoch loss: 0.44891512393951416
Total epoch: 97. epoch loss: 0.4439402222633362
Total epoch: 98. epoch loss: 0.4390789568424225
Total epoch: 99. epoch loss: 0.43432608246803284
Total epoch: 99. DecT loss: 0.43432608246803284
Training time: 0.42925000190734863
APL_precision: 0.25510204081632654, APL_recall: 0.29411764705882354, APL_f1: 0.273224043715847, APL_number: 170
CMT_precision: 0.07666666666666666, CMT_recall: 0.2358974358974359, CMT_f1: 0.11572327044025157, CMT_number: 195
DSC_precision: 0.5129151291512916, DSC_recall: 0.3180778032036613, DSC_f1: 0.3926553672316384, DSC_number: 437
MAT_precision: 0.5434131736526946, MAT_recall: 0.532258064516129, MAT_f1: 0.5377777777777777, MAT_number: 682
PRO_precision: 0.36893203883495146, PRO_recall: 0.19714656290531776, PRO_f1: 0.2569737954353339, PRO_number: 771
SMT_precision: 0.1774193548387097, SMT_recall: 0.38596491228070173, SMT_f1: 0.24309392265193372, SMT_number: 171
SPL_precision: 0.48514851485148514, SPL_recall: 0.6533333333333333, SPL_f1: 0.5568181818181818, SPL_number: 75
overall_precision: 0.3301526717557252, overall_recall: 0.34586165533786484, overall_f1: 0.3378246436242921, overall_accuracy: 0.7672789650489601
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:03<?, ?it/s]