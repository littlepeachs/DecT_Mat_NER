/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:30:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:30:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-18e93fae9dc4599b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1220.52it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:30:56 - INFO - __main__ - ***** Running training *****
06/01/2023 02:30:56 - INFO - __main__ -   Num examples = 21
06/01/2023 02:30:56 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:30:56 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:30:56 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:30:56 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:30:56 - INFO - __main__ -   Total optimization steps = 100
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.198246002197266
Total epoch: 1. epoch loss: 15.315003395080566
Total epoch: 2. epoch loss: 14.460922241210938
Total epoch: 3. epoch loss: 13.641417503356934
Total epoch: 4. epoch loss: 12.860576629638672
Total epoch: 5. epoch loss: 12.12049674987793
Total epoch: 6. epoch loss: 11.421182632446289
Total epoch: 7. epoch loss: 10.760941505432129
Total epoch: 8. epoch loss: 10.137385368347168
Total epoch: 9. epoch loss: 9.548280715942383
Total epoch: 10. epoch loss: 8.991912841796875
Total epoch: 11. epoch loss: 8.46704387664795
Total epoch: 12. epoch loss: 7.972555160522461
Total epoch: 13. epoch loss: 7.507195472717285
Total epoch: 14. epoch loss: 7.069495677947998
Total epoch: 15. epoch loss: 6.657588005065918
Total epoch: 16. epoch loss: 6.2696123123168945
Total epoch: 17. epoch loss: 5.903879642486572
Total epoch: 18. epoch loss: 5.558979511260986
Total epoch: 19. epoch loss: 5.233816623687744
Total epoch: 20. epoch loss: 4.927489757537842
Total epoch: 21. epoch loss: 4.639204978942871
Total epoch: 22. epoch loss: 4.368061542510986
Total epoch: 23. epoch loss: 4.113117694854736
Total epoch: 24. epoch loss: 3.873326301574707
Total epoch: 25. epoch loss: 3.647716760635376
Total epoch: 26. epoch loss: 3.4361937046051025
Total epoch: 27. epoch loss: 3.238586187362671
Total epoch: 28. epoch loss: 3.0544536113739014
Total epoch: 29. epoch loss: 2.8832216262817383
Total epoch: 30. epoch loss: 2.7241768836975098
Total epoch: 31. epoch loss: 2.5765130519866943
Total epoch: 32. epoch loss: 2.4394078254699707
Total epoch: 33. epoch loss: 2.312190055847168
Total epoch: 34. epoch loss: 2.194218873977661
Total epoch: 35. epoch loss: 2.084960460662842
Total epoch: 36. epoch loss: 1.9838192462921143
Total epoch: 37. epoch loss: 1.890199899673462
Total epoch: 38. epoch loss: 1.8035123348236084
Total epoch: 39. epoch loss: 1.7231841087341309
Total epoch: 40. epoch loss: 1.6486690044403076
Total epoch: 41. epoch loss: 1.5795214176177979
Total epoch: 42. epoch loss: 1.5153318643569946
Total epoch: 43. epoch loss: 1.455709457397461
Total epoch: 44. epoch loss: 1.4002857208251953
Total epoch: 45. epoch loss: 1.3487061262130737
Total epoch: 46. epoch loss: 1.300648808479309
Total epoch: 47. epoch loss: 1.255795955657959
Total epoch: 48. epoch loss: 1.2138837575912476
Total epoch: 49. epoch loss: 1.1746892929077148
Total epoch: 50. epoch loss: 1.137993335723877
Total epoch: 51. epoch loss: 1.1035887002944946
Total epoch: 52. epoch loss: 1.071295142173767
Total epoch: 53. epoch loss: 1.0409401655197144
Total epoch: 54. epoch loss: 1.0123652219772339
Total epoch: 55. epoch loss: 0.9854356646537781
Total epoch: 56. epoch loss: 0.9600295424461365
Total epoch: 57. epoch loss: 0.9360249638557434
Total epoch: 58. epoch loss: 0.9133334159851074
Total epoch: 59. epoch loss: 0.8918452262878418
Total epoch: 60. epoch loss: 0.8714659214019775
Total epoch: 61. epoch loss: 0.8521196246147156
Total epoch: 62. epoch loss: 0.8337244987487793
Total epoch: 63. epoch loss: 0.8162120580673218
Total epoch: 64. epoch loss: 0.7995165586471558
Total epoch: 65. epoch loss: 0.7835880517959595
Total epoch: 66. epoch loss: 0.7683742046356201
Total epoch: 67. epoch loss: 0.7538238763809204
Total epoch: 68. epoch loss: 0.7398933172225952
Total epoch: 69. epoch loss: 0.7265421152114868
Total epoch: 70. epoch loss: 0.7137376666069031
Total epoch: 71. epoch loss: 0.7014414072036743
Total epoch: 72. epoch loss: 0.689619243144989
Total epoch: 73. epoch loss: 0.6782479882240295
Total epoch: 74. epoch loss: 0.6672938466072083
Total epoch: 75. epoch loss: 0.6567422151565552
Total epoch: 76. epoch loss: 0.6465626955032349
Total epoch: 77. epoch loss: 0.6367329955101013
Total epoch: 78. epoch loss: 0.627238392829895
Total epoch: 79. epoch loss: 0.6180556416511536
Total epoch: 80. epoch loss: 0.6091697812080383
Total epoch: 81. epoch loss: 0.6005669832229614
Total epoch: 82. epoch loss: 0.5922352075576782
Total epoch: 83. epoch loss: 0.5841511487960815
Total epoch: 84. epoch loss: 0.5763185620307922
Total epoch: 85. epoch loss: 0.5687123537063599
Total epoch: 86. epoch loss: 0.56132572889328
Total epoch: 87. epoch loss: 0.5541517734527588
Total epoch: 88. epoch loss: 0.547174870967865
Total epoch: 89. epoch loss: 0.540394127368927
Total epoch: 90. epoch loss: 0.5337960124015808
Total epoch: 91. epoch loss: 0.5273683667182922
Total epoch: 92. epoch loss: 0.5211098194122314
Total epoch: 93. epoch loss: 0.5150160789489746
Total epoch: 94. epoch loss: 0.5090699195861816
Total epoch: 95. epoch loss: 0.5032723546028137
Total epoch: 96. epoch loss: 0.49761730432510376
Total epoch: 97. epoch loss: 0.49210160970687866
Total epoch: 98. epoch loss: 0.4867122769355774
Total epoch: 99. epoch loss: 0.4814451038837433
Total epoch: 99. DecT loss: 0.4814451038837433
Training time: 0.4432237148284912
APL_precision: 0.2342007434944238, APL_recall: 0.37058823529411766, APL_f1: 0.2870159453302961, APL_number: 170
CMT_precision: 0.16938110749185667, CMT_recall: 0.26666666666666666, CMT_f1: 0.20717131474103584, CMT_number: 195
DSC_precision: 0.45739910313901344, DSC_recall: 0.2334096109839817, DSC_f1: 0.3090909090909091, DSC_number: 437
MAT_precision: 0.4423963133640553, MAT_recall: 0.5630498533724341, MAT_f1: 0.49548387096774205, MAT_number: 682
PRO_precision: 0.3020833333333333, PRO_recall: 0.22568093385214008, PRO_f1: 0.25835189309576834, PRO_number: 771
SMT_precision: 0.21951219512195122, SMT_recall: 0.15789473684210525, SMT_f1: 0.18367346938775508, SMT_number: 171
SPL_precision: 0.39622641509433965, SPL_recall: 0.56, SPL_f1: 0.4640883977900553, SPL_number: 75
overall_precision: 0.3414239482200647, overall_recall: 0.33746501399440226, overall_f1: 0.33943293786446815, overall_accuracy: 0.7790007862197127
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:02<?, ?it/s]