/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:30:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:30:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0b40478253d0b33b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1258.61it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/23 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:30:56 - INFO - __main__ - ***** Running training *****
06/01/2023 02:30:56 - INFO - __main__ -   Num examples = 23
06/01/2023 02:30:56 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:30:56 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:30:56 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:30:56 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:30:56 - INFO - __main__ -   Total optimization steps = 100
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.738194465637207
Total epoch: 1. epoch loss: 14.930891990661621
Total epoch: 2. epoch loss: 14.14219856262207
Total epoch: 3. epoch loss: 13.373488426208496
Total epoch: 4. epoch loss: 12.626151084899902
Total epoch: 5. epoch loss: 11.90177059173584
Total epoch: 6. epoch loss: 11.20202922821045
Total epoch: 7. epoch loss: 10.52853775024414
Total epoch: 8. epoch loss: 9.882518768310547
Total epoch: 9. epoch loss: 9.264766693115234
Total epoch: 10. epoch loss: 8.67562198638916
Total epoch: 11. epoch loss: 8.115118026733398
Total epoch: 12. epoch loss: 7.582997798919678
Total epoch: 13. epoch loss: 7.078939437866211
Total epoch: 14. epoch loss: 6.602517604827881
Total epoch: 15. epoch loss: 6.153373718261719
Total epoch: 16. epoch loss: 5.7309699058532715
Total epoch: 17. epoch loss: 5.3346357345581055
Total epoch: 18. epoch loss: 4.963536262512207
Total epoch: 19. epoch loss: 4.616625785827637
Total epoch: 20. epoch loss: 4.292741298675537
Total epoch: 21. epoch loss: 3.9907336235046387
Total epoch: 22. epoch loss: 3.709311008453369
Total epoch: 23. epoch loss: 3.4473211765289307
Total epoch: 24. epoch loss: 3.2036352157592773
Total epoch: 25. epoch loss: 2.977243185043335
Total epoch: 26. epoch loss: 2.7682504653930664
Total epoch: 27. epoch loss: 2.576307535171509
Total epoch: 28. epoch loss: 2.4006175994873047
Total epoch: 29. epoch loss: 2.2402682304382324
Total epoch: 30. epoch loss: 2.0941710472106934
Total epoch: 31. epoch loss: 1.9612339735031128
Total epoch: 32. epoch loss: 1.8403339385986328
Total epoch: 33. epoch loss: 1.7304227352142334
Total epoch: 34. epoch loss: 1.6304712295532227
Total epoch: 35. epoch loss: 1.5395299196243286
Total epoch: 36. epoch loss: 1.4567488431930542
Total epoch: 37. epoch loss: 1.3813345432281494
Total epoch: 38. epoch loss: 1.3125838041305542
Total epoch: 39. epoch loss: 1.2498544454574585
Total epoch: 40. epoch loss: 1.1925615072250366
Total epoch: 41. epoch loss: 1.1401585340499878
Total epoch: 42. epoch loss: 1.0921598672866821
Total epoch: 43. epoch loss: 1.048128366470337
Total epoch: 44. epoch loss: 1.0076473951339722
Total epoch: 45. epoch loss: 0.9703449606895447
Total epoch: 46. epoch loss: 0.9359104037284851
Total epoch: 47. epoch loss: 0.9040482044219971
Total epoch: 48. epoch loss: 0.8745033740997314
Total epoch: 49. epoch loss: 0.8470545411109924
Total epoch: 50. epoch loss: 0.8214908838272095
Total epoch: 51. epoch loss: 0.7976391911506653
Total epoch: 52. epoch loss: 0.7753313183784485
Total epoch: 53. epoch loss: 0.7544271349906921
Total epoch: 54. epoch loss: 0.7348012328147888
Total epoch: 55. epoch loss: 0.716338574886322
Total epoch: 56. epoch loss: 0.6989395022392273
Total epoch: 57. epoch loss: 0.6825085878372192
Total epoch: 58. epoch loss: 0.6669754981994629
Total epoch: 59. epoch loss: 0.652254045009613
Total epoch: 60. epoch loss: 0.6382930278778076
Total epoch: 61. epoch loss: 0.6250231862068176
Total epoch: 62. epoch loss: 0.6123978495597839
Total epoch: 63. epoch loss: 0.6003655195236206
Total epoch: 64. epoch loss: 0.5888888835906982
Total epoch: 65. epoch loss: 0.5779201984405518
Total epoch: 66. epoch loss: 0.5674300789833069
Total epoch: 67. epoch loss: 0.5573825836181641
Total epoch: 68. epoch loss: 0.5477462410926819
Total epoch: 69. epoch loss: 0.5384992957115173
Total epoch: 70. epoch loss: 0.5296090841293335
Total epoch: 71. epoch loss: 0.5210584998130798
Total epoch: 72. epoch loss: 0.5128273367881775
Total epoch: 73. epoch loss: 0.504886269569397
Total epoch: 74. epoch loss: 0.49722859263420105
Total epoch: 75. epoch loss: 0.48982933163642883
Total epoch: 76. epoch loss: 0.4826790988445282
Total epoch: 77. epoch loss: 0.4757591784000397
Total epoch: 78. epoch loss: 0.46906107664108276
Total epoch: 79. epoch loss: 0.46257156133651733
Total epoch: 80. epoch loss: 0.45627421140670776
Total epoch: 81. epoch loss: 0.4501647651195526
Total epoch: 82. epoch loss: 0.44423016905784607
Total epoch: 83. epoch loss: 0.43846532702445984
Total epoch: 84. epoch loss: 0.4328632652759552
Total epoch: 85. epoch loss: 0.42741185426712036
Total epoch: 86. epoch loss: 0.4221060276031494
Total epoch: 87. epoch loss: 0.4169372320175171
Total epoch: 88. epoch loss: 0.4119039475917816
Total epoch: 89. epoch loss: 0.40699881315231323
Total epoch: 90. epoch loss: 0.4022109806537628
Total epoch: 91. epoch loss: 0.3975444436073303
Total epoch: 92. epoch loss: 0.3929903209209442
Total epoch: 93. epoch loss: 0.38854148983955383
Total epoch: 94. epoch loss: 0.3841964602470398
Total epoch: 95. epoch loss: 0.3799516558647156
Total epoch: 96. epoch loss: 0.3758013844490051
Total epoch: 97. epoch loss: 0.3717445135116577
Total epoch: 98. epoch loss: 0.36777424812316895
Total epoch: 99. epoch loss: 0.36389055848121643
Total epoch: 99. DecT loss: 0.36389055848121643
Training time: 0.4402434825897217
APL_precision: 0.18427518427518427, APL_recall: 0.4411764705882353, APL_f1: 0.2599653379549393, APL_number: 170
CMT_precision: 0.13111888111888112, CMT_recall: 0.38461538461538464, CMT_f1: 0.1955671447196871, CMT_number: 195
DSC_precision: 0.3576923076923077, DSC_recall: 0.425629290617849, DSC_f1: 0.3887147335423198, DSC_number: 437
MAT_precision: 0.5774877650897227, MAT_recall: 0.5190615835777126, MAT_f1: 0.5467181467181467, MAT_number: 682
PRO_precision: 0.32346723044397463, PRO_recall: 0.19844357976653695, PRO_f1: 0.2459807073954984, PRO_number: 771
SMT_precision: 0.31543624161073824, SMT_recall: 0.27485380116959063, SMT_f1: 0.29375, SMT_number: 171
SPL_precision: 0.4339622641509434, SPL_recall: 0.6133333333333333, SPL_f1: 0.5082872928176795, SPL_number: 75
overall_precision: 0.3295774647887324, overall_recall: 0.374250299880048, overall_f1: 0.3504961617674593, overall_accuracy: 0.7643485097562719
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:03<?, ?it/s]