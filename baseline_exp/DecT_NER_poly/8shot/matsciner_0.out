/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:30:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:30:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-e2e87af22853dbef/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1231.08it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/22 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:30:56 - INFO - __main__ - ***** Running training *****
06/01/2023 02:30:56 - INFO - __main__ -   Num examples = 22
06/01/2023 02:30:56 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:30:56 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:30:56 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:30:56 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:30:56 - INFO - __main__ -   Total optimization steps = 100
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.683095932006836
Total epoch: 1. epoch loss: 14.860960006713867
Total epoch: 2. epoch loss: 14.05665397644043
Total epoch: 3. epoch loss: 13.271550178527832
Total epoch: 4. epoch loss: 12.507320404052734
Total epoch: 5. epoch loss: 11.765789985656738
Total epoch: 6. epoch loss: 11.048871994018555
Total epoch: 7. epoch loss: 10.358170509338379
Total epoch: 8. epoch loss: 9.694976806640625
Total epoch: 9. epoch loss: 9.060193061828613
Total epoch: 10. epoch loss: 8.454489707946777
Total epoch: 11. epoch loss: 7.878256320953369
Total epoch: 12. epoch loss: 7.331773281097412
Total epoch: 13. epoch loss: 6.815160274505615
Total epoch: 14. epoch loss: 6.3284406661987305
Total epoch: 15. epoch loss: 5.87144660949707
Total epoch: 16. epoch loss: 5.443922996520996
Total epoch: 17. epoch loss: 5.045409202575684
Total epoch: 18. epoch loss: 4.675247669219971
Total epoch: 19. epoch loss: 4.332522392272949
Total epoch: 20. epoch loss: 4.016061305999756
Total epoch: 21. epoch loss: 3.7243900299072266
Total epoch: 22. epoch loss: 3.4557290077209473
Total epoch: 23. epoch loss: 3.208235263824463
Total epoch: 24. epoch loss: 2.980220317840576
Total epoch: 25. epoch loss: 2.7709507942199707
Total epoch: 26. epoch loss: 2.579411506652832
Total epoch: 27. epoch loss: 2.404393434524536
Total epoch: 28. epoch loss: 2.2446062564849854
Total epoch: 29. epoch loss: 2.0987775325775146
Total epoch: 30. epoch loss: 1.96567964553833
Total epoch: 31. epoch loss: 1.8441118001937866
Total epoch: 32. epoch loss: 1.7329961061477661
Total epoch: 33. epoch loss: 1.631319284439087
Total epoch: 34. epoch loss: 1.5382150411605835
Total epoch: 35. epoch loss: 1.4528906345367432
Total epoch: 36. epoch loss: 1.3746414184570312
Total epoch: 37. epoch loss: 1.302875280380249
Total epoch: 38. epoch loss: 1.2370271682739258
Total epoch: 39. epoch loss: 1.1766082048416138
Total epoch: 40. epoch loss: 1.1211520433425903
Total epoch: 41. epoch loss: 1.070241928100586
Total epoch: 42. epoch loss: 1.0234719514846802
Total epoch: 43. epoch loss: 0.9804660081863403
Total epoch: 44. epoch loss: 0.940886378288269
Total epoch: 45. epoch loss: 0.9044046998023987
Total epoch: 46. epoch loss: 0.8707288503646851
Total epoch: 47. epoch loss: 0.8395981192588806
Total epoch: 48. epoch loss: 0.8107539415359497
Total epoch: 49. epoch loss: 0.783996045589447
Total epoch: 50. epoch loss: 0.759107768535614
Total epoch: 51. epoch loss: 0.7359189391136169
Total epoch: 52. epoch loss: 0.714264452457428
Total epoch: 53. epoch loss: 0.6940025687217712
Total epoch: 54. epoch loss: 0.6750048398971558
Total epoch: 55. epoch loss: 0.657155454158783
Total epoch: 56. epoch loss: 0.640363335609436
Total epoch: 57. epoch loss: 0.6245221495628357
Total epoch: 58. epoch loss: 0.6095644235610962
Total epoch: 59. epoch loss: 0.5954179167747498
Total epoch: 60. epoch loss: 0.5820153951644897
Total epoch: 61. epoch loss: 0.5693005919456482
Total epoch: 62. epoch loss: 0.557222843170166
Total epoch: 63. epoch loss: 0.545739471912384
Total epoch: 64. epoch loss: 0.5348024964332581
Total epoch: 65. epoch loss: 0.52437424659729
Total epoch: 66. epoch loss: 0.5144196152687073
Total epoch: 67. epoch loss: 0.5049058794975281
Total epoch: 68. epoch loss: 0.49580085277557373
Total epoch: 69. epoch loss: 0.48708266019821167
Total epoch: 70. epoch loss: 0.47871899604797363
Total epoch: 71. epoch loss: 0.47069069743156433
Total epoch: 72. epoch loss: 0.46297487616539
Total epoch: 73. epoch loss: 0.4555497169494629
Total epoch: 74. epoch loss: 0.44840008020401
Total epoch: 75. epoch loss: 0.4415065050125122
Total epoch: 76. epoch loss: 0.4348621666431427
Total epoch: 77. epoch loss: 0.428439199924469
Total epoch: 78. epoch loss: 0.42223647236824036
Total epoch: 79. epoch loss: 0.416233628988266
Total epoch: 80. epoch loss: 0.4104243814945221
Total epoch: 81. epoch loss: 0.4047950208187103
Total epoch: 82. epoch loss: 0.3993411064147949
Total epoch: 83. epoch loss: 0.39404726028442383
Total epoch: 84. epoch loss: 0.3889106512069702
Total epoch: 85. epoch loss: 0.38391953706741333
Total epoch: 86. epoch loss: 0.3790687918663025
Total epoch: 87. epoch loss: 0.3743530213832855
Total epoch: 88. epoch loss: 0.3697657883167267
Total epoch: 89. epoch loss: 0.3653007745742798
Total epoch: 90. epoch loss: 0.3609527051448822
Total epoch: 91. epoch loss: 0.35671406984329224
Total epoch: 92. epoch loss: 0.3525835871696472
Total epoch: 93. epoch loss: 0.34855350852012634
Total epoch: 94. epoch loss: 0.3446226119995117
Total epoch: 95. epoch loss: 0.340785413980484
Total epoch: 96. epoch loss: 0.33704182505607605
Total epoch: 97. epoch loss: 0.333381712436676
Total epoch: 98. epoch loss: 0.3298070430755615
Total epoch: 99. epoch loss: 0.32631194591522217
Total epoch: 99. DecT loss: 0.32631194591522217
Training time: 0.437211275100708
APL_precision: 0.22344322344322345, APL_recall: 0.3588235294117647, APL_f1: 0.27539503386004516, APL_number: 170
CMT_precision: 0.22879177377892032, CMT_recall: 0.4564102564102564, CMT_f1: 0.3047945205479452, CMT_number: 195
DSC_precision: 0.31116389548693585, DSC_recall: 0.2997711670480549, DSC_f1: 0.30536130536130535, DSC_number: 437
MAT_precision: 0.5054545454545455, MAT_recall: 0.6114369501466276, MAT_f1: 0.553417385534174, MAT_number: 682
PRO_precision: 0.235, PRO_recall: 0.1219195849546044, PRO_f1: 0.16054654141759178, PRO_number: 771
SMT_precision: 0.23931623931623933, SMT_recall: 0.32748538011695905, SMT_f1: 0.2765432098765432, SMT_number: 171
SPL_precision: 0.3229166666666667, SPL_recall: 0.41333333333333333, SPL_f1: 0.3625730994152047, SPL_number: 75
overall_precision: 0.33320697498104623, overall_recall: 0.35145941623350657, overall_f1: 0.3420899007589025, overall_accuracy: 0.7697805732256451
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:02<?, ?it/s]