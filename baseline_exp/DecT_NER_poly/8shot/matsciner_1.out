/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:30:37 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:30:38 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-90b165f8431f5e0a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1213.63it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/24 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:05 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:05 - INFO - __main__ -   Num examples = 24
06/01/2023 02:31:05 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:05 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:05 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:05 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:05 - INFO - __main__ -   Total optimization steps = 100
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.669960021972656
Total epoch: 1. epoch loss: 14.888826370239258
Total epoch: 2. epoch loss: 14.123353958129883
Total epoch: 3. epoch loss: 13.374249458312988
Total epoch: 4. epoch loss: 12.642495155334473
Total epoch: 5. epoch loss: 11.929360389709473
Total epoch: 6. epoch loss: 11.236557960510254
Total epoch: 7. epoch loss: 10.56594467163086
Total epoch: 8. epoch loss: 9.91943073272705
Total epoch: 9. epoch loss: 9.29871940612793
Total epoch: 10. epoch loss: 8.705195426940918
Total epoch: 11. epoch loss: 8.139893531799316
Total epoch: 12. epoch loss: 7.603414058685303
Total epoch: 13. epoch loss: 7.096001148223877
Total epoch: 14. epoch loss: 6.617549419403076
Total epoch: 15. epoch loss: 6.167704105377197
Total epoch: 16. epoch loss: 5.745833396911621
Total epoch: 17. epoch loss: 5.351146697998047
Total epoch: 18. epoch loss: 4.982661724090576
Total epoch: 19. epoch loss: 4.639244556427002
Total epoch: 20. epoch loss: 4.319579601287842
Total epoch: 21. epoch loss: 4.022268772125244
Total epoch: 22. epoch loss: 3.745758295059204
Total epoch: 23. epoch loss: 3.488558053970337
Total epoch: 24. epoch loss: 3.249110698699951
Total epoch: 25. epoch loss: 3.0263824462890625
Total epoch: 26. epoch loss: 2.8200502395629883
Total epoch: 27. epoch loss: 2.629504442214966
Total epoch: 28. epoch loss: 2.453951835632324
Total epoch: 29. epoch loss: 2.2925586700439453
Total epoch: 30. epoch loss: 2.144437313079834
Total epoch: 31. epoch loss: 2.008746385574341
Total epoch: 32. epoch loss: 1.8845843076705933
Total epoch: 33. epoch loss: 1.7711623907089233
Total epoch: 34. epoch loss: 1.6676175594329834
Total epoch: 35. epoch loss: 1.5731823444366455
Total epoch: 36. epoch loss: 1.4870829582214355
Total epoch: 37. epoch loss: 1.4086017608642578
Total epoch: 38. epoch loss: 1.3370444774627686
Total epoch: 39. epoch loss: 1.2717753648757935
Total epoch: 40. epoch loss: 1.2121834754943848
Total epoch: 41. epoch loss: 1.157721996307373
Total epoch: 42. epoch loss: 1.1078792810440063
Total epoch: 43. epoch loss: 1.0621931552886963
Total epoch: 44. epoch loss: 1.020215392112732
Total epoch: 45. epoch loss: 0.9815835952758789
Total epoch: 46. epoch loss: 0.9459463953971863
Total epoch: 47. epoch loss: 0.912994384765625
Total epoch: 48. epoch loss: 0.8824526071548462
Total epoch: 49. epoch loss: 0.8540773987770081
Total epoch: 50. epoch loss: 0.827660858631134
Total epoch: 51. epoch loss: 0.8030217885971069
Total epoch: 52. epoch loss: 0.7799746990203857
Total epoch: 53. epoch loss: 0.7583914399147034
Total epoch: 54. epoch loss: 0.7381243705749512
Total epoch: 55. epoch loss: 0.7190621495246887
Total epoch: 56. epoch loss: 0.7010993957519531
Total epoch: 57. epoch loss: 0.684145450592041
Total epoch: 58. epoch loss: 0.6681141257286072
Total epoch: 59. epoch loss: 0.6529362797737122
Total epoch: 60. epoch loss: 0.6385438442230225
Total epoch: 61. epoch loss: 0.6248674392700195
Total epoch: 62. epoch loss: 0.6118659377098083
Total epoch: 63. epoch loss: 0.5994819402694702
Total epoch: 64. epoch loss: 0.5876740217208862
Total epoch: 65. epoch loss: 0.5763994455337524
Total epoch: 66. epoch loss: 0.5656211376190186
Total epoch: 67. epoch loss: 0.5553098320960999
Total epoch: 68. epoch loss: 0.5454263091087341
Total epoch: 69. epoch loss: 0.5359506607055664
Total epoch: 70. epoch loss: 0.5268481969833374
Total epoch: 71. epoch loss: 0.5180961489677429
Total epoch: 72. epoch loss: 0.509678065776825
Total epoch: 73. epoch loss: 0.5015672445297241
Total epoch: 74. epoch loss: 0.49374687671661377
Total epoch: 75. epoch loss: 0.4861977696418762
Total epoch: 76. epoch loss: 0.47890618443489075
Total epoch: 77. epoch loss: 0.4718606770038605
Total epoch: 78. epoch loss: 0.4650370478630066
Total epoch: 79. epoch loss: 0.45843398571014404
Total epoch: 80. epoch loss: 0.45203304290771484
Total epoch: 81. epoch loss: 0.44582265615463257
Total epoch: 82. epoch loss: 0.43979573249816895
Total epoch: 83. epoch loss: 0.4339434802532196
Total epoch: 84. epoch loss: 0.4282560646533966
Total epoch: 85. epoch loss: 0.4227248430252075
Total epoch: 86. epoch loss: 0.41734200716018677
Total epoch: 87. epoch loss: 0.412104070186615
Total epoch: 88. epoch loss: 0.40700095891952515
Total epoch: 89. epoch loss: 0.4020262360572815
Total epoch: 90. epoch loss: 0.39717724919319153
Total epoch: 91. epoch loss: 0.3924475312232971
Total epoch: 92. epoch loss: 0.3878275752067566
Total epoch: 93. epoch loss: 0.3833228051662445
Total epoch: 94. epoch loss: 0.37892138957977295
Total epoch: 95. epoch loss: 0.37461885809898376
Total epoch: 96. epoch loss: 0.3704131543636322
Total epoch: 97. epoch loss: 0.3663027584552765
Total epoch: 98. epoch loss: 0.362280935049057
Total epoch: 99. epoch loss: 0.3583478331565857
Total epoch: 99. DecT loss: 0.3583478331565857
Training time: 0.4522714614868164
APL_precision: 0.24064171122994651, APL_recall: 0.5294117647058824, APL_f1: 0.33088235294117646, APL_number: 170
CMT_precision: 0.25263157894736843, CMT_recall: 0.36923076923076925, CMT_f1: 0.3, CMT_number: 195
DSC_precision: 0.39106145251396646, DSC_recall: 0.32036613272311215, DSC_f1: 0.3522012578616352, DSC_number: 437
MAT_precision: 0.5927601809954751, MAT_recall: 0.5762463343108505, MAT_f1: 0.5843866171003718, MAT_number: 682
PRO_precision: 0.3397129186602871, PRO_recall: 0.18417639429312582, PRO_f1: 0.23885618166526493, PRO_number: 771
SMT_precision: 0.2796208530805687, SMT_recall: 0.34502923976608185, SMT_f1: 0.3089005235602095, SMT_number: 171
SPL_precision: 0.6129032258064516, SPL_recall: 0.5066666666666667, SPL_f1: 0.5547445255474452, SPL_number: 75
overall_precision: 0.39392661324335726, overall_recall: 0.37345061975209914, overall_f1: 0.38341543513957305, overall_accuracy: 0.7855049674790937
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:02<?, ?it/s]