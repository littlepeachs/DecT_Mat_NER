/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:31:16 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:31:17 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-68fc820920ef3b72/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1150.39it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/41 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:24 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:24 - INFO - __main__ -   Num examples = 41
06/01/2023 02:31:24 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:24 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:24 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:24 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:24 - INFO - __main__ -   Total optimization steps = 200
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.842604637145996
Total epoch: 1. epoch loss: 15.0891752243042
Total epoch: 2. epoch loss: 14.354609489440918
Total epoch: 3. epoch loss: 13.64118480682373
Total epoch: 4. epoch loss: 12.951229095458984
Total epoch: 5. epoch loss: 12.286964416503906
Total epoch: 6. epoch loss: 11.649820327758789
Total epoch: 7. epoch loss: 11.040290832519531
Total epoch: 8. epoch loss: 10.457782745361328
Total epoch: 9. epoch loss: 9.901107788085938
Total epoch: 10. epoch loss: 9.368803024291992
Total epoch: 11. epoch loss: 8.859711647033691
Total epoch: 12. epoch loss: 8.372872352600098
Total epoch: 13. epoch loss: 7.907738208770752
Total epoch: 14. epoch loss: 7.463916301727295
Total epoch: 15. epoch loss: 7.041202545166016
Total epoch: 16. epoch loss: 6.639416217803955
Total epoch: 17. epoch loss: 6.2583136558532715
Total epoch: 18. epoch loss: 5.897567272186279
Total epoch: 19. epoch loss: 5.556756019592285
Total epoch: 20. epoch loss: 5.235454082489014
Total epoch: 21. epoch loss: 4.93308687210083
Total epoch: 22. epoch loss: 4.649017810821533
Total epoch: 23. epoch loss: 4.38248872756958
Total epoch: 24. epoch loss: 4.132558822631836
Total epoch: 25. epoch loss: 3.898186445236206
Total epoch: 26. epoch loss: 3.6789612770080566
Total epoch: 27. epoch loss: 3.474472999572754
Total epoch: 28. epoch loss: 3.2839953899383545
Total epoch: 29. epoch loss: 3.1067137718200684
Total epoch: 30. epoch loss: 2.941854953765869
Total epoch: 31. epoch loss: 2.788623571395874
Total epoch: 32. epoch loss: 2.6463589668273926
Total epoch: 33. epoch loss: 2.514346122741699
Total epoch: 34. epoch loss: 2.391934871673584
Total epoch: 35. epoch loss: 2.2785422801971436
Total epoch: 36. epoch loss: 2.173532009124756
Total epoch: 37. epoch loss: 2.0763180255889893
Total epoch: 38. epoch loss: 1.986342430114746
Total epoch: 39. epoch loss: 1.9030508995056152
Total epoch: 40. epoch loss: 1.8259485960006714
Total epoch: 41. epoch loss: 1.7545278072357178
Total epoch: 42. epoch loss: 1.688340425491333
Total epoch: 43. epoch loss: 1.6269609928131104
Total epoch: 44. epoch loss: 1.5699878931045532
Total epoch: 45. epoch loss: 1.5170613527297974
Total epoch: 46. epoch loss: 1.467836618423462
Total epoch: 47. epoch loss: 1.4220103025436401
Total epoch: 48. epoch loss: 1.3792682886123657
Total epoch: 49. epoch loss: 1.339369535446167
Total epoch: 50. epoch loss: 1.3020497560501099
Total epoch: 51. epoch loss: 1.2670907974243164
Total epoch: 52. epoch loss: 1.2342950105667114
Total epoch: 53. epoch loss: 1.2034518718719482
Total epoch: 54. epoch loss: 1.1744102239608765
Total epoch: 55. epoch loss: 1.1470216512680054
Total epoch: 56. epoch loss: 1.1211308240890503
Total epoch: 57. epoch loss: 1.0966302156448364
Total epoch: 58. epoch loss: 1.073408603668213
Total epoch: 59. epoch loss: 1.051364779472351
Total epoch: 60. epoch loss: 1.0304032564163208
Total epoch: 61. epoch loss: 1.0104668140411377
Total epoch: 62. epoch loss: 0.9914605021476746
Total epoch: 63. epoch loss: 0.9733176827430725
Total epoch: 64. epoch loss: 0.9559977054595947
Total epoch: 65. epoch loss: 0.93942791223526
Total epoch: 66. epoch loss: 0.9235644340515137
Total epoch: 67. epoch loss: 0.9083631634712219
Total epoch: 68. epoch loss: 0.8937652707099915
Total epoch: 69. epoch loss: 0.8797579407691956
Total epoch: 70. epoch loss: 0.8662850260734558
Total epoch: 71. epoch loss: 0.8533242344856262
Total epoch: 72. epoch loss: 0.8408384323120117
Total epoch: 73. epoch loss: 0.8288091421127319
Total epoch: 74. epoch loss: 0.81719571352005
Total epoch: 75. epoch loss: 0.8059855699539185
Total epoch: 76. epoch loss: 0.7951505184173584
Total epoch: 77. epoch loss: 0.7846732139587402
Total epoch: 78. epoch loss: 0.7745265364646912
Total epoch: 79. epoch loss: 0.764703631401062
Total epoch: 80. epoch loss: 0.7551823854446411
Total epoch: 81. epoch loss: 0.7459442615509033
Total epoch: 82. epoch loss: 0.7369785904884338
Total epoch: 83. epoch loss: 0.7282705903053284
Total epoch: 84. epoch loss: 0.719810426235199
Total epoch: 85. epoch loss: 0.7115801572799683
Total epoch: 86. epoch loss: 0.7035740613937378
Total epoch: 87. epoch loss: 0.6957769393920898
Total epoch: 88. epoch loss: 0.6881811618804932
Total epoch: 89. epoch loss: 0.6807804703712463
Total epoch: 90. epoch loss: 0.6735625863075256
Total epoch: 91. epoch loss: 0.6665207743644714
Total epoch: 92. epoch loss: 0.6596516966819763
Total epoch: 93. epoch loss: 0.6529433131217957
Total epoch: 94. epoch loss: 0.6463889479637146
Total epoch: 95. epoch loss: 0.6399853229522705
Total epoch: 96. epoch loss: 0.6337253451347351
Total epoch: 97. epoch loss: 0.6276024580001831
Total epoch: 98. epoch loss: 0.6216140985488892
Total epoch: 99. epoch loss: 0.6157537698745728
Total epoch: 99. DecT loss: 0.6157537698745728
Training time: 0.4681422710418701
APL_precision: 0.3051948051948052, APL_recall: 0.5529411764705883, APL_f1: 0.39330543933054396, APL_number: 170
CMT_precision: 0.4647058823529412, CMT_recall: 0.40512820512820513, CMT_f1: 0.4328767123287671, CMT_number: 195
DSC_precision: 0.5527638190954773, DSC_recall: 0.5034324942791762, DSC_f1: 0.5269461077844312, DSC_number: 437
MAT_precision: 0.4826862539349423, MAT_recall: 0.6744868035190615, MAT_f1: 0.5626911314984709, MAT_number: 682
PRO_precision: 0.4485294117647059, PRO_recall: 0.3164721141374838, PRO_f1: 0.3711026615969582, PRO_number: 771
SMT_precision: 0.3755868544600939, SMT_recall: 0.4678362573099415, SMT_f1: 0.41666666666666663, SMT_number: 171
SPL_precision: 0.47761194029850745, SPL_recall: 0.4266666666666667, SPL_f1: 0.4507042253521127, SPL_number: 75
overall_precision: 0.4557105163965322, overall_recall: 0.48340663734506195, overall_f1: 0.4691501746216531, overall_accuracy: 0.8211707526266886
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:02<?, ?it/s]