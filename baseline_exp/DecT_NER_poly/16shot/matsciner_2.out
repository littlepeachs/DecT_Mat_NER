/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:31:16 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:31:17 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-545026288f6cba7d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1243.68it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/46 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:25 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:25 - INFO - __main__ -   Num examples = 46
06/01/2023 02:31:25 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:25 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:25 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:25 - INFO - __main__ -   Total optimization steps = 200
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.841025352478027
Total epoch: 1. epoch loss: 15.119665145874023
Total epoch: 2. epoch loss: 14.421175003051758
Total epoch: 3. epoch loss: 13.746894836425781
Total epoch: 4. epoch loss: 13.09794807434082
Total epoch: 5. epoch loss: 12.47520637512207
Total epoch: 6. epoch loss: 11.87874698638916
Total epoch: 7. epoch loss: 11.307632446289062
Total epoch: 8. epoch loss: 10.7601318359375
Total epoch: 9. epoch loss: 10.234329223632812
Total epoch: 10. epoch loss: 9.728482246398926
Total epoch: 11. epoch loss: 9.241394996643066
Total epoch: 12. epoch loss: 8.772503852844238
Total epoch: 13. epoch loss: 8.321663856506348
Total epoch: 14. epoch loss: 7.888985633850098
Total epoch: 15. epoch loss: 7.4747209548950195
Total epoch: 16. epoch loss: 7.079106330871582
Total epoch: 17. epoch loss: 6.702213764190674
Total epoch: 18. epoch loss: 6.343977451324463
Total epoch: 19. epoch loss: 6.00404691696167
Total epoch: 20. epoch loss: 5.681914329528809
Total epoch: 21. epoch loss: 5.376861572265625
Total epoch: 22. epoch loss: 5.088159561157227
Total epoch: 23. epoch loss: 4.814980983734131
Total epoch: 24. epoch loss: 4.556637763977051
Total epoch: 25. epoch loss: 4.312351703643799
Total epoch: 26. epoch loss: 4.082169055938721
Total epoch: 27. epoch loss: 3.8661139011383057
Total epoch: 28. epoch loss: 3.6638906002044678
Total epoch: 29. epoch loss: 3.474914312362671
Total epoch: 30. epoch loss: 3.298496723175049
Total epoch: 31. epoch loss: 3.1339240074157715
Total epoch: 32. epoch loss: 2.9804375171661377
Total epoch: 33. epoch loss: 2.8373451232910156
Total epoch: 34. epoch loss: 2.70398211479187
Total epoch: 35. epoch loss: 2.5797016620635986
Total epoch: 36. epoch loss: 2.4639177322387695
Total epoch: 37. epoch loss: 2.3560502529144287
Total epoch: 38. epoch loss: 2.2555718421936035
Total epoch: 39. epoch loss: 2.1619670391082764
Total epoch: 40. epoch loss: 2.074737310409546
Total epoch: 41. epoch loss: 1.9934293031692505
Total epoch: 42. epoch loss: 1.9176093339920044
Total epoch: 43. epoch loss: 1.8468838930130005
Total epoch: 44. epoch loss: 1.7808512449264526
Total epoch: 45. epoch loss: 1.7191933393478394
Total epoch: 46. epoch loss: 1.6615570783615112
Total epoch: 47. epoch loss: 1.607628583908081
Total epoch: 48. epoch loss: 1.5571388006210327
Total epoch: 49. epoch loss: 1.5098233222961426
Total epoch: 50. epoch loss: 1.4654390811920166
Total epoch: 51. epoch loss: 1.4237505197525024
Total epoch: 52. epoch loss: 1.3845677375793457
Total epoch: 53. epoch loss: 1.3476847410202026
Total epoch: 54. epoch loss: 1.3129363059997559
Total epoch: 55. epoch loss: 1.2801581621170044
Total epoch: 56. epoch loss: 1.2492034435272217
Total epoch: 57. epoch loss: 1.219929814338684
Total epoch: 58. epoch loss: 1.1922271251678467
Total epoch: 59. epoch loss: 1.165962815284729
Total epoch: 60. epoch loss: 1.1410402059555054
Total epoch: 61. epoch loss: 1.117362380027771
Total epoch: 62. epoch loss: 1.0948301553726196
Total epoch: 63. epoch loss: 1.0733673572540283
Total epoch: 64. epoch loss: 1.0529066324234009
Total epoch: 65. epoch loss: 1.033371090888977
Total epoch: 66. epoch loss: 1.014699935913086
Total epoch: 67. epoch loss: 0.9968328475952148
Total epoch: 68. epoch loss: 0.979722797870636
Total epoch: 69. epoch loss: 0.9633127450942993
Total epoch: 70. epoch loss: 0.9475717544555664
Total epoch: 71. epoch loss: 0.9324465394020081
Total epoch: 72. epoch loss: 0.9179092645645142
Total epoch: 73. epoch loss: 0.9039140343666077
Total epoch: 74. epoch loss: 0.8904311656951904
Total epoch: 75. epoch loss: 0.8774353265762329
Total epoch: 76. epoch loss: 0.8648963570594788
Total epoch: 77. epoch loss: 0.8527845740318298
Total epoch: 78. epoch loss: 0.8410735726356506
Total epoch: 79. epoch loss: 0.8297532796859741
Total epoch: 80. epoch loss: 0.8187977075576782
Total epoch: 81. epoch loss: 0.8081831336021423
Total epoch: 82. epoch loss: 0.7978907227516174
Total epoch: 83. epoch loss: 0.7879055142402649
Total epoch: 84. epoch loss: 0.7782216668128967
Total epoch: 85. epoch loss: 0.7688153386116028
Total epoch: 86. epoch loss: 0.7596702575683594
Total epoch: 87. epoch loss: 0.7507824301719666
Total epoch: 88. epoch loss: 0.7421302795410156
Total epoch: 89. epoch loss: 0.7337150573730469
Total epoch: 90. epoch loss: 0.725515604019165
Total epoch: 91. epoch loss: 0.7175269722938538
Total epoch: 92. epoch loss: 0.7097382545471191
Total epoch: 93. epoch loss: 0.7021443247795105
Total epoch: 94. epoch loss: 0.6947310566902161
Total epoch: 95. epoch loss: 0.6875008940696716
Total epoch: 96. epoch loss: 0.6804371476173401
Total epoch: 97. epoch loss: 0.6735367774963379
Total epoch: 98. epoch loss: 0.6667880415916443
Total epoch: 99. epoch loss: 0.6601946949958801
Total epoch: 99. DecT loss: 0.6601946949958801
Training time: 0.518465518951416
APL_precision: 0.19935691318327975, APL_recall: 0.36470588235294116, APL_f1: 0.25779625779625776, APL_number: 170
CMT_precision: 0.20630372492836677, CMT_recall: 0.36923076923076925, CMT_f1: 0.2647058823529411, CMT_number: 195
DSC_precision: 0.4073319755600815, DSC_recall: 0.4576659038901602, DSC_f1: 0.43103448275862066, DSC_number: 437
MAT_precision: 0.5614849187935035, MAT_recall: 0.7096774193548387, MAT_f1: 0.6269430051813472, MAT_number: 682
PRO_precision: 0.5034578146611342, PRO_recall: 0.4721141374837873, PRO_f1: 0.48728246318607765, PRO_number: 771
SMT_precision: 0.3403361344537815, SMT_recall: 0.47368421052631576, SMT_f1: 0.3960880195599022, SMT_number: 171
SPL_precision: 0.4418604651162791, SPL_recall: 0.5066666666666667, SPL_f1: 0.47204968944099385, SPL_number: 75
overall_precision: 0.42516339869281045, overall_recall: 0.5201919232307077, overall_f1: 0.4679014565725589, overall_accuracy: 0.8251018511900508
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]