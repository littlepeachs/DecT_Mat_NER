/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:31:16 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:31:17 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-2a29c2bd2933348e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1213.81it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/36 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:25 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:25 - INFO - __main__ -   Num examples = 36
06/01/2023 02:31:25 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:25 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:25 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:25 - INFO - __main__ -   Total optimization steps = 200
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.758546829223633
Total epoch: 1. epoch loss: 14.98269271850586
Total epoch: 2. epoch loss: 14.237242698669434
Total epoch: 3. epoch loss: 13.527471542358398
Total epoch: 4. epoch loss: 12.85665512084961
Total epoch: 5. epoch loss: 12.224300384521484
Total epoch: 6. epoch loss: 11.625938415527344
Total epoch: 7. epoch loss: 11.055416107177734
Total epoch: 8. epoch loss: 10.507484436035156
Total epoch: 9. epoch loss: 9.97890853881836
Total epoch: 10. epoch loss: 9.468311309814453
Total epoch: 11. epoch loss: 8.975594520568848
Total epoch: 12. epoch loss: 8.50129222869873
Total epoch: 13. epoch loss: 8.0460786819458
Total epoch: 14. epoch loss: 7.610493183135986
Total epoch: 15. epoch loss: 7.194679260253906
Total epoch: 16. epoch loss: 6.79836368560791
Total epoch: 17. epoch loss: 6.4209394454956055
Total epoch: 18. epoch loss: 6.061789035797119
Total epoch: 19. epoch loss: 5.720447063446045
Total epoch: 20. epoch loss: 5.396637439727783
Total epoch: 21. epoch loss: 5.090356349945068
Total epoch: 22. epoch loss: 4.801513195037842
Total epoch: 23. epoch loss: 4.52983283996582
Total epoch: 24. epoch loss: 4.274641513824463
Total epoch: 25. epoch loss: 4.034934043884277
Total epoch: 26. epoch loss: 3.809805154800415
Total epoch: 27. epoch loss: 3.598912239074707
Total epoch: 28. epoch loss: 3.401764154434204
Total epoch: 29. epoch loss: 3.217853307723999
Total epoch: 30. epoch loss: 3.0465848445892334
Total epoch: 31. epoch loss: 2.8872673511505127
Total epoch: 32. epoch loss: 2.739161729812622
Total epoch: 33. epoch loss: 2.6015381813049316
Total epoch: 34. epoch loss: 2.473665237426758
Total epoch: 35. epoch loss: 2.354938507080078
Total epoch: 36. epoch loss: 2.2448172569274902
Total epoch: 37. epoch loss: 2.142753839492798
Total epoch: 38. epoch loss: 2.0481762886047363
Total epoch: 39. epoch loss: 1.9604848623275757
Total epoch: 40. epoch loss: 1.8790929317474365
Total epoch: 41. epoch loss: 1.8034591674804688
Total epoch: 42. epoch loss: 1.7330979108810425
Total epoch: 43. epoch loss: 1.6676026582717896
Total epoch: 44. epoch loss: 1.606619119644165
Total epoch: 45. epoch loss: 1.5498218536376953
Total epoch: 46. epoch loss: 1.4968942403793335
Total epoch: 47. epoch loss: 1.4475293159484863
Total epoch: 48. epoch loss: 1.4014289379119873
Total epoch: 49. epoch loss: 1.3583272695541382
Total epoch: 50. epoch loss: 1.3179638385772705
Total epoch: 51. epoch loss: 1.280121922492981
Total epoch: 52. epoch loss: 1.24459707736969
Total epoch: 53. epoch loss: 1.2112009525299072
Total epoch: 54. epoch loss: 1.179771065711975
Total epoch: 55. epoch loss: 1.150128722190857
Total epoch: 56. epoch loss: 1.1221367120742798
Total epoch: 57. epoch loss: 1.095644235610962
Total epoch: 58. epoch loss: 1.0705498456954956
Total epoch: 59. epoch loss: 1.0467427968978882
Total epoch: 60. epoch loss: 1.0241398811340332
Total epoch: 61. epoch loss: 1.002651572227478
Total epoch: 62. epoch loss: 0.9822039008140564
Total epoch: 63. epoch loss: 0.9627249240875244
Total epoch: 64. epoch loss: 0.9441396594047546
Total epoch: 65. epoch loss: 0.926399290561676
Total epoch: 66. epoch loss: 0.9094376564025879
Total epoch: 67. epoch loss: 0.8932144641876221
Total epoch: 68. epoch loss: 0.8776749968528748
Total epoch: 69. epoch loss: 0.8627803325653076
Total epoch: 70. epoch loss: 0.8484877943992615
Total epoch: 71. epoch loss: 0.8347551226615906
Total epoch: 72. epoch loss: 0.8215492367744446
Total epoch: 73. epoch loss: 0.8088414669036865
Total epoch: 74. epoch loss: 0.7965995073318481
Total epoch: 75. epoch loss: 0.7847947478294373
Total epoch: 76. epoch loss: 0.7734111547470093
Total epoch: 77. epoch loss: 0.7624172568321228
Total epoch: 78. epoch loss: 0.751789927482605
Total epoch: 79. epoch loss: 0.7415189743041992
Total epoch: 80. epoch loss: 0.7315791845321655
Total epoch: 81. epoch loss: 0.7219526171684265
Total epoch: 82. epoch loss: 0.7126227617263794
Total epoch: 83. epoch loss: 0.7035772204399109
Total epoch: 84. epoch loss: 0.6947991251945496
Total epoch: 85. epoch loss: 0.6862753033638
Total epoch: 86. epoch loss: 0.677992582321167
Total epoch: 87. epoch loss: 0.6699413061141968
Total epoch: 88. epoch loss: 0.6621119379997253
Total epoch: 89. epoch loss: 0.6544923782348633
Total epoch: 90. epoch loss: 0.6470711827278137
Total epoch: 91. epoch loss: 0.6398406028747559
Total epoch: 92. epoch loss: 0.6328016519546509
Total epoch: 93. epoch loss: 0.6259282231330872
Total epoch: 94. epoch loss: 0.6192265748977661
Total epoch: 95. epoch loss: 0.6126856803894043
Total epoch: 96. epoch loss: 0.6063006520271301
Total epoch: 97. epoch loss: 0.6000592112541199
Total epoch: 98. epoch loss: 0.5939618945121765
Total epoch: 99. epoch loss: 0.588003933429718
Total epoch: 99. DecT loss: 0.588003933429718
Training time: 0.4808166027069092
APL_precision: 0.17142857142857143, APL_recall: 0.35294117647058826, APL_f1: 0.23076923076923078, APL_number: 170
CMT_precision: 0.3764705882352941, CMT_recall: 0.49230769230769234, CMT_f1: 0.42666666666666664, CMT_number: 195
DSC_precision: 0.40789473684210525, DSC_recall: 0.425629290617849, DSC_f1: 0.4165733482642777, DSC_number: 437
MAT_precision: 0.5287356321839081, MAT_recall: 0.6070381231671554, MAT_f1: 0.5651877133105803, MAT_number: 682
PRO_precision: 0.41545189504373176, PRO_recall: 0.36964980544747084, PRO_f1: 0.39121482498284144, PRO_number: 771
SMT_precision: 0.27472527472527475, SMT_recall: 0.43859649122807015, SMT_f1: 0.3378378378378379, SMT_number: 171
SPL_precision: 0.4166666666666667, SPL_recall: 0.6, SPL_f1: 0.49180327868852464, SPL_number: 75
overall_precision: 0.398832016489179, overall_recall: 0.4642143142742903, overall_f1: 0.4290465631929047, overall_accuracy: 0.8132370809806304
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]