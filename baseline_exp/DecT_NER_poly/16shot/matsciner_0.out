/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 02:31:16 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 02:31:17 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-61df0b13c7c2e0bc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1090.28it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/45 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 02:31:25 - INFO - __main__ - ***** Running training *****
06/01/2023 02:31:25 - INFO - __main__ -   Num examples = 45
06/01/2023 02:31:25 - INFO - __main__ -   Num Epochs = 100
06/01/2023 02:31:25 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 02:31:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 02:31:25 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 02:31:25 - INFO - __main__ -   Total optimization steps = 200
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.95321273803711
Total epoch: 1. epoch loss: 15.168526649475098
Total epoch: 2. epoch loss: 14.41402530670166
Total epoch: 3. epoch loss: 13.698726654052734
Total epoch: 4. epoch loss: 13.02952766418457
Total epoch: 5. epoch loss: 12.407976150512695
Total epoch: 6. epoch loss: 11.829400062561035
Total epoch: 7. epoch loss: 11.285590171813965
Total epoch: 8. epoch loss: 10.76856803894043
Total epoch: 9. epoch loss: 10.272651672363281
Total epoch: 10. epoch loss: 9.794511795043945
Total epoch: 11. epoch loss: 9.332716941833496
Total epoch: 12. epoch loss: 8.88709545135498
Total epoch: 13. epoch loss: 8.458107948303223
Total epoch: 14. epoch loss: 8.046480178833008
Total epoch: 15. epoch loss: 7.652746677398682
Total epoch: 16. epoch loss: 7.277061939239502
Total epoch: 17. epoch loss: 6.9190545082092285
Total epoch: 18. epoch loss: 6.577991962432861
Total epoch: 19. epoch loss: 6.252942085266113
Total epoch: 20. epoch loss: 5.943019866943359
Total epoch: 21. epoch loss: 5.647597789764404
Total epoch: 22. epoch loss: 5.366219520568848
Total epoch: 23. epoch loss: 5.098593235015869
Total epoch: 24. epoch loss: 4.8443708419799805
Total epoch: 25. epoch loss: 4.6030592918396
Total epoch: 26. epoch loss: 4.373963832855225
Total epoch: 27. epoch loss: 4.157408714294434
Total epoch: 28. epoch loss: 3.9532487392425537
Total epoch: 29. epoch loss: 3.760976791381836
Total epoch: 30. epoch loss: 3.580026626586914
Total epoch: 31. epoch loss: 3.4098215103149414
Total epoch: 32. epoch loss: 3.2498281002044678
Total epoch: 33. epoch loss: 3.0995376110076904
Total epoch: 34. epoch loss: 2.9584386348724365
Total epoch: 35. epoch loss: 2.825995445251465
Total epoch: 36. epoch loss: 2.70166277885437
Total epoch: 37. epoch loss: 2.58492374420166
Total epoch: 38. epoch loss: 2.475372314453125
Total epoch: 39. epoch loss: 2.372614860534668
Total epoch: 40. epoch loss: 2.276364803314209
Total epoch: 41. epoch loss: 2.186300277709961
Total epoch: 42. epoch loss: 2.1021289825439453
Total epoch: 43. epoch loss: 2.0234827995300293
Total epoch: 44. epoch loss: 1.949994683265686
Total epoch: 45. epoch loss: 1.8813115358352661
Total epoch: 46. epoch loss: 1.8170663118362427
Total epoch: 47. epoch loss: 1.756919026374817
Total epoch: 48. epoch loss: 1.7005985975265503
Total epoch: 49. epoch loss: 1.647803783416748
Total epoch: 50. epoch loss: 1.5982592105865479
Total epoch: 51. epoch loss: 1.5517346858978271
Total epoch: 52. epoch loss: 1.5079758167266846
Total epoch: 53. epoch loss: 1.4667739868164062
Total epoch: 54. epoch loss: 1.4279203414916992
Total epoch: 55. epoch loss: 1.3912336826324463
Total epoch: 56. epoch loss: 1.3565553426742554
Total epoch: 57. epoch loss: 1.3237520456314087
Total epoch: 58. epoch loss: 1.2926836013793945
Total epoch: 59. epoch loss: 1.2632358074188232
Total epoch: 60. epoch loss: 1.235302448272705
Total epoch: 61. epoch loss: 1.2087655067443848
Total epoch: 62. epoch loss: 1.1835274696350098
Total epoch: 63. epoch loss: 1.1595039367675781
Total epoch: 64. epoch loss: 1.1365910768508911
Total epoch: 65. epoch loss: 1.1147180795669556
Total epoch: 66. epoch loss: 1.0938180685043335
Total epoch: 67. epoch loss: 1.0738264322280884
Total epoch: 68. epoch loss: 1.0546973943710327
Total epoch: 69. epoch loss: 1.0363552570343018
Total epoch: 70. epoch loss: 1.018765926361084
Total epoch: 71. epoch loss: 1.0018856525421143
Total epoch: 72. epoch loss: 0.9856745004653931
Total epoch: 73. epoch loss: 0.9700822830200195
Total epoch: 74. epoch loss: 0.9550728797912598
Total epoch: 75. epoch loss: 0.9406205415725708
Total epoch: 76. epoch loss: 0.926692008972168
Total epoch: 77. epoch loss: 0.9132518172264099
Total epoch: 78. epoch loss: 0.900269091129303
Total epoch: 79. epoch loss: 0.8877345323562622
Total epoch: 80. epoch loss: 0.8756077289581299
Total epoch: 81. epoch loss: 0.8638749122619629
Total epoch: 82. epoch loss: 0.8525189161300659
Total epoch: 83. epoch loss: 0.8415097594261169
Total epoch: 84. epoch loss: 0.8308358192443848
Total epoch: 85. epoch loss: 0.8204784989356995
Total epoch: 86. epoch loss: 0.8104217052459717
Total epoch: 87. epoch loss: 0.8006504774093628
Total epoch: 88. epoch loss: 0.791155219078064
Total epoch: 89. epoch loss: 0.7819233536720276
Total epoch: 90. epoch loss: 0.7729361057281494
Total epoch: 91. epoch loss: 0.7641919255256653
Total epoch: 92. epoch loss: 0.7556652426719666
Total epoch: 93. epoch loss: 0.7473656535148621
Total epoch: 94. epoch loss: 0.7392698526382446
Total epoch: 95. epoch loss: 0.7313773036003113
Total epoch: 96. epoch loss: 0.7236723303794861
Total epoch: 97. epoch loss: 0.7161482572555542
Total epoch: 98. epoch loss: 0.7088010907173157
Total epoch: 99. epoch loss: 0.7016245722770691
Total epoch: 99. DecT loss: 0.7016245722770691
Training time: 0.49193835258483887
APL_precision: 0.2918149466192171, APL_recall: 0.4823529411764706, APL_f1: 0.3636363636363636, APL_number: 170
CMT_precision: 0.2702020202020202, CMT_recall: 0.5487179487179488, CMT_f1: 0.362098138747885, CMT_number: 195
DSC_precision: 0.46284501061571126, DSC_recall: 0.4988558352402746, DSC_f1: 0.4801762114537445, DSC_number: 437
MAT_precision: 0.4734848484848485, MAT_recall: 0.5498533724340176, MAT_f1: 0.5088195386702848, MAT_number: 682
PRO_precision: 0.37309292649098474, PRO_recall: 0.34889753566796367, PRO_f1: 0.36058981233243964, PRO_number: 771
SMT_precision: 0.2847682119205298, SMT_recall: 0.5029239766081871, SMT_f1: 0.36363636363636365, SMT_number: 171
SPL_precision: 0.3493975903614458, SPL_recall: 0.38666666666666666, SPL_f1: 0.3670886075949367, SPL_number: 75
overall_precision: 0.38279711096520025, overall_recall: 0.4662135145941623, overall_f1: 0.42040742743825493, overall_accuracy: 0.8062325780859124
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:02<?, ?it/s]