/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 14:50:35 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 14:50:36 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0b40478253d0b33b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1114.02it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/23 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 2482.34 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 14:50:46 - INFO - __main__ - ***** Running training *****
05/31/2023 14:50:46 - INFO - __main__ -   Num examples = 23
05/31/2023 14:50:46 - INFO - __main__ -   Num Epochs = 100
05/31/2023 14:50:46 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 14:50:46 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 14:50:46 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 14:50:46 - INFO - __main__ -   Total optimization steps = 600
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/600 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.570621490478516
Total epoch: 1. epoch loss: 16.172151565551758
Total epoch: 2. epoch loss: 15.776354789733887
Total epoch: 3. epoch loss: 15.380691528320312
Total epoch: 4. epoch loss: 14.985589027404785
Total epoch: 5. epoch loss: 14.591974258422852
Total epoch: 6. epoch loss: 14.200764656066895
Total epoch: 7. epoch loss: 13.812810897827148
Total epoch: 8. epoch loss: 13.42894458770752
Total epoch: 9. epoch loss: 13.049934387207031
Total epoch: 10. epoch loss: 12.676547050476074
Total epoch: 11. epoch loss: 12.3094482421875
Total epoch: 12. epoch loss: 11.949244499206543
Total epoch: 13. epoch loss: 11.596417427062988
Total epoch: 14. epoch loss: 11.251331329345703
Total epoch: 15. epoch loss: 10.914268493652344
Total epoch: 16. epoch loss: 10.585378646850586
Total epoch: 17. epoch loss: 10.264753341674805
Total epoch: 18. epoch loss: 9.952383995056152
Total epoch: 19. epoch loss: 9.648187637329102
Total epoch: 20. epoch loss: 9.352019309997559
Total epoch: 21. epoch loss: 9.063703536987305
Total epoch: 22. epoch loss: 8.782995223999023
Total epoch: 23. epoch loss: 8.509625434875488
Total epoch: 24. epoch loss: 8.243277549743652
Total epoch: 25. epoch loss: 7.983669281005859
Total epoch: 26. epoch loss: 7.730472087860107
Total epoch: 27. epoch loss: 7.483432769775391
Total epoch: 28. epoch loss: 7.2432026863098145
Total epoch: 29. epoch loss: 7.010210037231445
Total epoch: 30. epoch loss: 6.78467321395874
Total epoch: 31. epoch loss: 6.566779136657715
Total epoch: 32. epoch loss: 6.3565263748168945
Total epoch: 33. epoch loss: 6.153914928436279
Total epoch: 34. epoch loss: 5.958822727203369
Total epoch: 35. epoch loss: 5.771158695220947
Total epoch: 36. epoch loss: 5.590713977813721
Total epoch: 37. epoch loss: 5.417306900024414
Total epoch: 38. epoch loss: 5.250762462615967
Total epoch: 39. epoch loss: 5.090843200683594
Total epoch: 40. epoch loss: 4.937344551086426
Total epoch: 41. epoch loss: 4.790045261383057
Total epoch: 42. epoch loss: 4.648756980895996
Total epoch: 43. epoch loss: 4.513239860534668
Total epoch: 44. epoch loss: 4.383268356323242
Total epoch: 45. epoch loss: 4.258660316467285
Total epoch: 46. epoch loss: 4.139197826385498
Total epoch: 47. epoch loss: 4.0246734619140625
Total epoch: 48. epoch loss: 3.914889097213745
Total epoch: 49. epoch loss: 3.8096532821655273
Total epoch: 50. epoch loss: 3.7087574005126953
Total epoch: 51. epoch loss: 3.6120121479034424
Total epoch: 52. epoch loss: 3.519230842590332
Total epoch: 53. epoch loss: 3.4302289485931396
Total epoch: 54. epoch loss: 3.344825029373169
Total epoch: 55. epoch loss: 3.2628462314605713
Total epoch: 56. epoch loss: 3.1841044425964355
Total epoch: 57. epoch loss: 3.108471155166626
Total epoch: 58. epoch loss: 3.035773992538452
Total epoch: 59. epoch loss: 2.965877056121826
Total epoch: 60. epoch loss: 2.8986480236053467
Total epoch: 61. epoch loss: 2.8339178562164307
Total epoch: 62. epoch loss: 2.771627426147461
Total epoch: 63. epoch loss: 2.7116167545318604
Total epoch: 64. epoch loss: 2.653785467147827
Total epoch: 65. epoch loss: 2.598048686981201
Total epoch: 66. epoch loss: 2.5442938804626465
Total epoch: 67. epoch loss: 2.492431879043579
Total epoch: 68. epoch loss: 2.442379951477051
Total epoch: 69. epoch loss: 2.394054651260376
Total epoch: 70. epoch loss: 2.3473784923553467
Total epoch: 71. epoch loss: 2.3022713661193848
Total epoch: 72. epoch loss: 2.2586803436279297
Total epoch: 73. epoch loss: 2.2165281772613525
Total epoch: 74. epoch loss: 2.1757564544677734
Total epoch: 75. epoch loss: 2.136303424835205
Total epoch: 76. epoch loss: 2.098099946975708
Total epoch: 77. epoch loss: 2.0611252784729004
Total epoch: 78. epoch loss: 2.025296926498413
Total epoch: 79. epoch loss: 1.9905850887298584
Total epoch: 80. epoch loss: 1.9569402933120728
Total epoch: 81. epoch loss: 1.9243189096450806
Total epoch: 82. epoch loss: 1.8926727771759033
Total epoch: 83. epoch loss: 1.8619803190231323
Total epoch: 84. epoch loss: 1.8321805000305176
Total epoch: 85. epoch loss: 1.8032546043395996
Total epoch: 86. epoch loss: 1.7751706838607788
Total epoch: 87. epoch loss: 1.7478851079940796
Total epoch: 88. epoch loss: 1.7213727235794067
Total epoch: 89. epoch loss: 1.695605993270874
Total epoch: 90. epoch loss: 1.6705554723739624
Total epoch: 91. epoch loss: 1.6461951732635498
Total epoch: 92. epoch loss: 1.6225017309188843
Total epoch: 93. epoch loss: 1.599441409111023
Total epoch: 94. epoch loss: 1.5770046710968018
Total epoch: 95. epoch loss: 1.5551553964614868
Total epoch: 96. epoch loss: 1.5338736772537231
Total epoch: 97. epoch loss: 1.5131503343582153
Total epoch: 98. epoch loss: 1.4929676055908203
Total epoch: 99. epoch loss: 1.4732915163040161
Total epoch: 99. DecT loss: 1.4732915163040161
Training time: 0.5520684719085693
APL_precision: 0.11775362318840579, APL_recall: 0.38235294117647056, APL_f1: 0.18005540166204986, APL_number: 170
CMT_precision: 0.10673234811165845, CMT_recall: 0.3333333333333333, CMT_f1: 0.1616915422885572, CMT_number: 195
DSC_precision: 0.2890625, DSC_recall: 0.5080091533180778, DSC_f1: 0.36846473029045645, DSC_number: 437
MAT_precision: 0.5517826825127334, MAT_recall: 0.47653958944281527, MAT_f1: 0.5114083398898505, MAT_number: 682
PRO_precision: 0.2802653399668325, PRO_recall: 0.2191958495460441, PRO_f1: 0.24599708879184862, PRO_number: 771
SMT_precision: 0.1180327868852459, SMT_recall: 0.21052631578947367, SMT_f1: 0.1512605042016807, SMT_number: 171
SPL_precision: 0.10952380952380952, SPL_recall: 0.30666666666666664, SPL_f1: 0.1614035087719298, SPL_number: 75
overall_precision: 0.2488998899889989, overall_recall: 0.36185525789684125, overall_f1: 0.2949323773830862, overall_accuracy: 0.7372972200385908
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/600 [00:05<?, ?it/s]