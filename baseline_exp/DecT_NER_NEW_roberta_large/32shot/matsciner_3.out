/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 14:51:28 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 14:51:29 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-21be78dd80d291c5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 160
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 994.97it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/66 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 2377.50 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 14:51:39 - INFO - __main__ - ***** Running training *****
05/31/2023 14:51:39 - INFO - __main__ -   Num examples = 66
05/31/2023 14:51:39 - INFO - __main__ -   Num Epochs = 100
05/31/2023 14:51:39 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 14:51:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 14:51:39 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 14:51:39 - INFO - __main__ -   Total optimization steps = 1700
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/1700 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.553627014160156
Total epoch: 1. epoch loss: 16.1893310546875
Total epoch: 2. epoch loss: 15.827183723449707
Total epoch: 3. epoch loss: 15.465182304382324
Total epoch: 4. epoch loss: 15.10424518585205
Total epoch: 5. epoch loss: 14.745438575744629
Total epoch: 6. epoch loss: 14.39003849029541
Total epoch: 7. epoch loss: 14.039143562316895
Total epoch: 8. epoch loss: 13.693737983703613
Total epoch: 9. epoch loss: 13.354662895202637
Total epoch: 10. epoch loss: 13.022553443908691
Total epoch: 11. epoch loss: 12.697851181030273
Total epoch: 12. epoch loss: 12.380813598632812
Total epoch: 13. epoch loss: 12.07154655456543
Total epoch: 14. epoch loss: 11.770054817199707
Total epoch: 15. epoch loss: 11.476264953613281
Total epoch: 16. epoch loss: 11.190001487731934
Total epoch: 17. epoch loss: 10.911079406738281
Total epoch: 18. epoch loss: 10.639232635498047
Total epoch: 19. epoch loss: 10.374156951904297
Total epoch: 20. epoch loss: 10.11559009552002
Total epoch: 21. epoch loss: 9.863628387451172
Total epoch: 22. epoch loss: 9.618659973144531
Total epoch: 23. epoch loss: 9.380941390991211
Total epoch: 24. epoch loss: 9.150640487670898
Total epoch: 25. epoch loss: 8.927838325500488
Total epoch: 26. epoch loss: 8.712471961975098
Total epoch: 27. epoch loss: 8.50448226928711
Total epoch: 28. epoch loss: 8.303703308105469
Total epoch: 29. epoch loss: 8.10993766784668
Total epoch: 30. epoch loss: 7.922999858856201
Total epoch: 31. epoch loss: 7.742630958557129
Total epoch: 32. epoch loss: 7.568639278411865
Total epoch: 33. epoch loss: 7.400773048400879
Total epoch: 34. epoch loss: 7.238795280456543
Total epoch: 35. epoch loss: 7.082502365112305
Total epoch: 36. epoch loss: 6.931662082672119
Total epoch: 37. epoch loss: 6.786101818084717
Total epoch: 38. epoch loss: 6.645607948303223
Total epoch: 39. epoch loss: 6.510014533996582
Total epoch: 40. epoch loss: 6.379149436950684
Total epoch: 41. epoch loss: 6.252828121185303
Total epoch: 42. epoch loss: 6.130913257598877
Total epoch: 43. epoch loss: 6.013223171234131
Total epoch: 44. epoch loss: 5.899593830108643
Total epoch: 45. epoch loss: 5.7899017333984375
Total epoch: 46. epoch loss: 5.683979034423828
Total epoch: 47. epoch loss: 5.581682205200195
Total epoch: 48. epoch loss: 5.482846260070801
Total epoch: 49. epoch loss: 5.387337684631348
Total epoch: 50. epoch loss: 5.295027732849121
Total epoch: 51. epoch loss: 5.205758571624756
Total epoch: 52. epoch loss: 5.119393348693848
Total epoch: 53. epoch loss: 5.035804271697998
Total epoch: 54. epoch loss: 4.954869270324707
Total epoch: 55. epoch loss: 4.876464366912842
Total epoch: 56. epoch loss: 4.800468444824219
Total epoch: 57. epoch loss: 4.726788520812988
Total epoch: 58. epoch loss: 4.655304431915283
Total epoch: 59. epoch loss: 4.585921287536621
Total epoch: 60. epoch loss: 4.5185546875
Total epoch: 61. epoch loss: 4.453113555908203
Total epoch: 62. epoch loss: 4.38951301574707
Total epoch: 63. epoch loss: 4.3276824951171875
Total epoch: 64. epoch loss: 4.267547130584717
Total epoch: 65. epoch loss: 4.209038257598877
Total epoch: 66. epoch loss: 4.1520915031433105
Total epoch: 67. epoch loss: 4.096640110015869
Total epoch: 68. epoch loss: 4.0426483154296875
Total epoch: 69. epoch loss: 3.990018606185913
Total epoch: 70. epoch loss: 3.9387309551239014
Total epoch: 71. epoch loss: 3.888734817504883
Total epoch: 72. epoch loss: 3.8399770259857178
Total epoch: 73. epoch loss: 3.7923951148986816
Total epoch: 74. epoch loss: 3.745967388153076
Total epoch: 75. epoch loss: 3.700648546218872
Total epoch: 76. epoch loss: 3.6563844680786133
Total epoch: 77. epoch loss: 3.61315655708313
Total epoch: 78. epoch loss: 3.570911407470703
Total epoch: 79. epoch loss: 3.5296268463134766
Total epoch: 80. epoch loss: 3.4892683029174805
Total epoch: 81. epoch loss: 3.44980525970459
Total epoch: 82. epoch loss: 3.4112002849578857
Total epoch: 83. epoch loss: 3.373427629470825
Total epoch: 84. epoch loss: 3.336463689804077
Total epoch: 85. epoch loss: 3.3002781867980957
Total epoch: 86. epoch loss: 3.264854669570923
Total epoch: 87. epoch loss: 3.230156660079956
Total epoch: 88. epoch loss: 3.196174144744873
Total epoch: 89. epoch loss: 3.162876605987549
Total epoch: 90. epoch loss: 3.130242109298706
Total epoch: 91. epoch loss: 3.0982656478881836
Total epoch: 92. epoch loss: 3.066915273666382
Total epoch: 93. epoch loss: 3.0361733436584473
Total epoch: 94. epoch loss: 3.0060317516326904
Total epoch: 95. epoch loss: 2.976459264755249
Total epoch: 96. epoch loss: 2.9474411010742188
Total epoch: 97. epoch loss: 2.9189751148223877
Total epoch: 98. epoch loss: 2.8910365104675293
Total epoch: 99. epoch loss: 2.8636138439178467
Total epoch: 99. DecT loss: 2.8636138439178467
Training time: 0.7856967449188232
APL_precision: 0.16751269035532995, APL_recall: 0.38823529411764707, APL_f1: 0.23404255319148937, APL_number: 170
CMT_precision: 0.2734375, CMT_recall: 0.5384615384615384, CMT_f1: 0.3626943005181347, CMT_number: 195
DSC_precision: 0.33974358974358976, DSC_recall: 0.6064073226544623, DSC_f1: 0.43549712407559577, DSC_number: 437
MAT_precision: 0.4994652406417112, MAT_recall: 0.6847507331378299, MAT_f1: 0.577612863327149, MAT_number: 682
PRO_precision: 0.3474576271186441, PRO_recall: 0.4254215304798962, PRO_f1: 0.3825072886297376, PRO_number: 771
SMT_precision: 0.20638820638820637, SMT_recall: 0.49122807017543857, SMT_f1: 0.2906574394463668, SMT_number: 171
SPL_precision: 0.2413793103448276, SPL_recall: 0.37333333333333335, SPL_f1: 0.29319371727748694, SPL_number: 75
overall_precision: 0.33914141414141413, overall_recall: 0.536985205917633, overall_f1: 0.415725119950472, overall_accuracy: 0.7900378760808976
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/1700 [00:05<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 16:02:22 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 16:02:23 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-21be78dd80d291c5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 5
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 921.93it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/66 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 2018.88 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 16:02:34 - INFO - __main__ - ***** Running training *****
05/31/2023 16:02:34 - INFO - __main__ -   Num examples = 66
05/31/2023 16:02:34 - INFO - __main__ -   Num Epochs = 100
05/31/2023 16:02:34 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 16:02:34 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 16:02:34 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 16:02:34 - INFO - __main__ -   Total optimization steps = 1700
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/1700 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.597129821777344
Total epoch: 1. epoch loss: 16.270851135253906
Total epoch: 2. epoch loss: 15.947000503540039
Total epoch: 3. epoch loss: 15.62328052520752
Total epoch: 4. epoch loss: 15.299941062927246
Total epoch: 5. epoch loss: 14.977783203125
Total epoch: 6. epoch loss: 14.657588005065918
Total epoch: 7. epoch loss: 14.340251922607422
Total epoch: 8. epoch loss: 14.026593208312988
Total epoch: 9. epoch loss: 13.717354774475098
Total epoch: 10. epoch loss: 13.413174629211426
Total epoch: 11. epoch loss: 13.11452865600586
Total epoch: 12. epoch loss: 12.821738243103027
Total epoch: 13. epoch loss: 12.53502082824707
Total epoch: 14. epoch loss: 12.254453659057617
Total epoch: 15. epoch loss: 11.980058670043945
Total epoch: 16. epoch loss: 11.711816787719727
Total epoch: 17. epoch loss: 11.449630737304688
Total epoch: 18. epoch loss: 11.193371772766113
Total epoch: 19. epoch loss: 10.942880630493164
Total epoch: 20. epoch loss: 10.697919845581055
Total epoch: 21. epoch loss: 10.458304405212402
Total epoch: 22. epoch loss: 10.223780632019043
Total epoch: 23. epoch loss: 9.994552612304688
Total epoch: 24. epoch loss: 9.770883560180664
Total epoch: 25. epoch loss: 9.553011894226074
Total epoch: 26. epoch loss: 9.341100692749023
Total epoch: 27. epoch loss: 9.135229110717773
Total epoch: 28. epoch loss: 8.935393333435059
Total epoch: 29. epoch loss: 8.741569519042969
Total epoch: 30. epoch loss: 8.553679466247559
Total epoch: 31. epoch loss: 8.371613502502441
Total epoch: 32. epoch loss: 8.195277214050293
Total epoch: 33. epoch loss: 8.024510383605957
Total epoch: 34. epoch loss: 7.859206676483154
Total epoch: 35. epoch loss: 7.699202537536621
Total epoch: 36. epoch loss: 7.544330596923828
Total epoch: 37. epoch loss: 7.394465923309326
Total epoch: 38. epoch loss: 7.249416828155518
Total epoch: 39. epoch loss: 7.10899543762207
Total epoch: 40. epoch loss: 6.973097801208496
Total epoch: 41. epoch loss: 6.841526985168457
Total epoch: 42. epoch loss: 6.7141547203063965
Total epoch: 43. epoch loss: 6.590826034545898
Total epoch: 44. epoch loss: 6.471399784088135
Total epoch: 45. epoch loss: 6.3557515144348145
Total epoch: 46. epoch loss: 6.243709564208984
Total epoch: 47. epoch loss: 6.135188579559326
Total epoch: 48. epoch loss: 6.03004264831543
Total epoch: 49. epoch loss: 5.928151607513428
Total epoch: 50. epoch loss: 5.829409599304199
Total epoch: 51. epoch loss: 5.733719825744629
Total epoch: 52. epoch loss: 5.640934467315674
Total epoch: 53. epoch loss: 5.550988674163818
Total epoch: 54. epoch loss: 5.463757038116455
Total epoch: 55. epoch loss: 5.37915563583374
Total epoch: 56. epoch loss: 5.297056198120117
Total epoch: 57. epoch loss: 5.217367172241211
Total epoch: 58. epoch loss: 5.140005111694336
Total epoch: 59. epoch loss: 5.064846515655518
Total epoch: 60. epoch loss: 4.991822719573975
Total epoch: 61. epoch loss: 4.920856475830078
Total epoch: 62. epoch loss: 4.85182523727417
Total epoch: 63. epoch loss: 4.784676551818848
Total epoch: 64. epoch loss: 4.719330787658691
Total epoch: 65. epoch loss: 4.655708312988281
Total epoch: 66. epoch loss: 4.593740940093994
Total epoch: 67. epoch loss: 4.533359527587891
Total epoch: 68. epoch loss: 4.474517345428467
Total epoch: 69. epoch loss: 4.417154312133789
Total epoch: 70. epoch loss: 4.361205577850342
Total epoch: 71. epoch loss: 4.30662727355957
Total epoch: 72. epoch loss: 4.253368377685547
Total epoch: 73. epoch loss: 4.201378345489502
Total epoch: 74. epoch loss: 4.150622367858887
Total epoch: 75. epoch loss: 4.101050853729248
Total epoch: 76. epoch loss: 4.052620887756348
Total epoch: 77. epoch loss: 4.005300521850586
Total epoch: 78. epoch loss: 3.959038734436035
Total epoch: 79. epoch loss: 3.913811445236206
Total epoch: 80. epoch loss: 3.8695778846740723
Total epoch: 81. epoch loss: 3.826301097869873
Total epoch: 82. epoch loss: 3.7839579582214355
Total epoch: 83. epoch loss: 3.742513418197632
Total epoch: 84. epoch loss: 3.7019312381744385
Total epoch: 85. epoch loss: 3.662191867828369
Total epoch: 86. epoch loss: 3.623276472091675
Total epoch: 87. epoch loss: 3.5851519107818604
Total epoch: 88. epoch loss: 3.5477821826934814
Total epoch: 89. epoch loss: 3.511160135269165
Total epoch: 90. epoch loss: 3.475268602371216
Total epoch: 91. epoch loss: 3.440062999725342
Total epoch: 92. epoch loss: 3.405534029006958
Total epoch: 93. epoch loss: 3.3716773986816406
Total epoch: 94. epoch loss: 3.3384556770324707
Total epoch: 95. epoch loss: 3.3058536052703857
Total epoch: 96. epoch loss: 3.273852825164795
Total epoch: 97. epoch loss: 3.2424514293670654
Total epoch: 98. epoch loss: 3.211620330810547
Total epoch: 99. epoch loss: 3.18133282661438
Total epoch: 99. DecT loss: 3.18133282661438
Training time: 0.790715217590332
APL_precision: 0.1564102564102564, APL_recall: 0.3588235294117647, APL_f1: 0.2178571428571429, APL_number: 170
CMT_precision: 0.2698412698412698, CMT_recall: 0.5230769230769231, CMT_f1: 0.35602094240837695, CMT_number: 195
DSC_precision: 0.32957393483709274, DSC_recall: 0.6018306636155606, DSC_f1: 0.42591093117408907, DSC_number: 437
MAT_precision: 0.49148936170212765, MAT_recall: 0.6774193548387096, MAT_f1: 0.5696670776818742, MAT_number: 682
PRO_precision: 0.34177215189873417, PRO_recall: 0.42023346303501946, PRO_f1: 0.3769633507853403, PRO_number: 771
SMT_precision: 0.19148936170212766, SMT_recall: 0.47368421052631576, SMT_f1: 0.2727272727272727, SMT_number: 171
SPL_precision: 0.225, SPL_recall: 0.36, SPL_f1: 0.27692307692307694, SPL_number: 75
overall_precision: 0.33024768576432323, overall_recall: 0.5277888844462215, overall_f1: 0.4062788550323176, overall_accuracy: 0.7854641606517544
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/1700 [00:05<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 16:03:32 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 16:03:34 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-21be78dd80d291c5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1019.64it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/66 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3012.12 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 16:04:15 - INFO - __main__ - ***** Running training *****
05/31/2023 16:04:15 - INFO - __main__ -   Num examples = 66
05/31/2023 16:04:15 - INFO - __main__ -   Num Epochs = 100
05/31/2023 16:04:15 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 16:04:15 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 16:04:15 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 16:04:15 - INFO - __main__ -   Total optimization steps = 1700
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/1700 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.264930725097656
Total epoch: 1. epoch loss: 15.939024925231934
Total epoch: 2. epoch loss: 15.617205619812012
Total epoch: 3. epoch loss: 15.298714637756348
Total epoch: 4. epoch loss: 14.98244571685791
Total epoch: 5. epoch loss: 14.667655944824219
Total epoch: 6. epoch loss: 14.354493141174316
Total epoch: 7. epoch loss: 14.043905258178711
Total epoch: 8. epoch loss: 13.736863136291504
Total epoch: 9. epoch loss: 13.434215545654297
Total epoch: 10. epoch loss: 13.136642456054688
Total epoch: 11. epoch loss: 12.844642639160156
Total epoch: 12. epoch loss: 12.558518409729004
Total epoch: 13. epoch loss: 12.278478622436523
Total epoch: 14. epoch loss: 12.004579544067383
Total epoch: 15. epoch loss: 11.736807823181152
Total epoch: 16. epoch loss: 11.475133895874023
Total epoch: 17. epoch loss: 11.219388008117676
Total epoch: 18. epoch loss: 10.969474792480469
Total epoch: 19. epoch loss: 10.725191116333008
Total epoch: 20. epoch loss: 10.48631763458252
Total epoch: 21. epoch loss: 10.252635955810547
Total epoch: 22. epoch loss: 10.023914337158203
Total epoch: 23. epoch loss: 9.80022144317627
Total epoch: 24. epoch loss: 9.58184814453125
Total epoch: 25. epoch loss: 9.368977546691895
Total epoch: 26. epoch loss: 9.161789894104004
Total epoch: 27. epoch loss: 8.960421562194824
Total epoch: 28. epoch loss: 8.764874458312988
Total epoch: 29. epoch loss: 8.575201034545898
Total epoch: 30. epoch loss: 8.391371726989746
Total epoch: 31. epoch loss: 8.213289260864258
Total epoch: 32. epoch loss: 8.040811538696289
Total epoch: 33. epoch loss: 7.873824596405029
Total epoch: 34. epoch loss: 7.712165355682373
Total epoch: 35. epoch loss: 7.555656909942627
Total epoch: 36. epoch loss: 7.404151439666748
Total epoch: 37. epoch loss: 7.257504463195801
Total epoch: 38. epoch loss: 7.1155476570129395
Total epoch: 39. epoch loss: 6.978128433227539
Total epoch: 40. epoch loss: 6.845110893249512
Total epoch: 41. epoch loss: 6.716335296630859
Total epoch: 42. epoch loss: 6.591650485992432
Total epoch: 43. epoch loss: 6.470902442932129
Total epoch: 44. epoch loss: 6.353980541229248
Total epoch: 45. epoch loss: 6.240732669830322
Total epoch: 46. epoch loss: 6.131034851074219
Total epoch: 47. epoch loss: 6.024785995483398
Total epoch: 48. epoch loss: 5.92184591293335
Total epoch: 49. epoch loss: 5.82210111618042
Total epoch: 50. epoch loss: 5.725445747375488
Total epoch: 51. epoch loss: 5.631758689880371
Total epoch: 52. epoch loss: 5.54092264175415
Total epoch: 53. epoch loss: 5.452856063842773
Total epoch: 54. epoch loss: 5.367419242858887
Total epoch: 55. epoch loss: 5.284539222717285
Total epoch: 56. epoch loss: 5.2041120529174805
Total epoch: 57. epoch loss: 5.126030921936035
Total epoch: 58. epoch loss: 5.0502095222473145
Total epoch: 59. epoch loss: 4.976562976837158
Total epoch: 60. epoch loss: 4.904996395111084
Total epoch: 61. epoch loss: 4.835422039031982
Total epoch: 62. epoch loss: 4.7677483558654785
Total epoch: 63. epoch loss: 4.701932430267334
Total epoch: 64. epoch loss: 4.637858867645264
Total epoch: 65. epoch loss: 4.575484752655029
Total epoch: 66. epoch loss: 4.5147318840026855
Total epoch: 67. epoch loss: 4.45552921295166
Total epoch: 68. epoch loss: 4.397838115692139
Total epoch: 69. epoch loss: 4.341585159301758
Total epoch: 70. epoch loss: 4.286707878112793
Total epoch: 71. epoch loss: 4.233190536499023
Total epoch: 72. epoch loss: 4.180941104888916
Total epoch: 73. epoch loss: 4.129933834075928
Total epoch: 74. epoch loss: 4.08013916015625
Total epoch: 75. epoch loss: 4.031479358673096
Total epoch: 76. epoch loss: 3.9839372634887695
Total epoch: 77. epoch loss: 3.937471866607666
Total epoch: 78. epoch loss: 3.8920586109161377
Total epoch: 79. epoch loss: 3.8476459980010986
Total epoch: 80. epoch loss: 3.8041939735412598
Total epoch: 81. epoch loss: 3.7616848945617676
Total epoch: 82. epoch loss: 3.7200863361358643
Total epoch: 83. epoch loss: 3.6793718338012695
Total epoch: 84. epoch loss: 3.63950252532959
Total epoch: 85. epoch loss: 3.6004557609558105
Total epoch: 86. epoch loss: 3.56221342086792
Total epoch: 87. epoch loss: 3.524735450744629
Total epoch: 88. epoch loss: 3.48801589012146
Total epoch: 89. epoch loss: 3.4520211219787598
Total epoch: 90. epoch loss: 3.4167275428771973
Total epoch: 91. epoch loss: 3.3821237087249756
Total epoch: 92. epoch loss: 3.3481810092926025
Total epoch: 93. epoch loss: 3.3148844242095947
Total epoch: 94. epoch loss: 3.28220796585083
Total epoch: 95. epoch loss: 3.2501564025878906
Total epoch: 96. epoch loss: 3.218684196472168
Total epoch: 97. epoch loss: 3.187790870666504
Total epoch: 98. epoch loss: 3.157454252243042
Total epoch: 99. epoch loss: 3.127673387527466
Total epoch: 99. DecT loss: 3.127673387527466
Training time: 0.79215407371521
APL_precision: 0.15654205607476634, APL_recall: 0.3941176470588235, APL_f1: 0.22408026755852845, APL_number: 170
CMT_precision: 0.25, CMT_recall: 0.5333333333333333, CMT_f1: 0.3404255319148936, CMT_number: 195
DSC_precision: 0.38396624472573837, DSC_recall: 0.6247139588100686, DSC_f1: 0.475609756097561, DSC_number: 437
MAT_precision: 0.4754440961337513, MAT_recall: 0.6671554252199413, MAT_f1: 0.5552165954850519, MAT_number: 682
PRO_precision: 0.33059548254620125, PRO_recall: 0.4176394293125811, PRO_f1: 0.36905444126074505, PRO_number: 771
SMT_precision: 0.2200557103064067, SMT_recall: 0.4619883040935672, SMT_f1: 0.2981132075471698, SMT_number: 171
SPL_precision: 0.20863309352517986, SPL_recall: 0.38666666666666666, SPL_f1: 0.2710280373831776, SPL_number: 75
overall_precision: 0.3335843373493976, overall_recall: 0.5313874450219912, overall_f1: 0.40986892829606786, overall_accuracy: 0.7876795540627457
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/1700 [00:04<?, ?it/s]