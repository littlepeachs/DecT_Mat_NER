/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 14:51:27 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 14:51:29 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-145490b5457c9ad0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 160
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1093.83it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 2332.07 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 14:51:40 - INFO - __main__ - ***** Running training *****
05/31/2023 14:51:40 - INFO - __main__ -   Num examples = 72
05/31/2023 14:51:40 - INFO - __main__ -   Num Epochs = 100
05/31/2023 14:51:40 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 14:51:40 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 14:51:40 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 14:51:40 - INFO - __main__ -   Total optimization steps = 1800
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/1800 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.557693481445312
Total epoch: 1. epoch loss: 16.20598602294922
Total epoch: 2. epoch loss: 15.856956481933594
Total epoch: 3. epoch loss: 15.508413314819336
Total epoch: 4. epoch loss: 15.161176681518555
Total epoch: 5. epoch loss: 14.81646728515625
Total epoch: 6. epoch loss: 14.475300788879395
Total epoch: 7. epoch loss: 14.138901710510254
Total epoch: 8. epoch loss: 13.808409690856934
Total epoch: 9. epoch loss: 13.48466682434082
Total epoch: 10. epoch loss: 13.168179512023926
Total epoch: 11. epoch loss: 12.85921859741211
Total epoch: 12. epoch loss: 12.5578031539917
Total epoch: 13. epoch loss: 12.263872146606445
Total epoch: 14. epoch loss: 11.97719955444336
Total epoch: 15. epoch loss: 11.697562217712402
Total epoch: 16. epoch loss: 11.424661636352539
Total epoch: 17. epoch loss: 11.158166885375977
Total epoch: 18. epoch loss: 10.89781379699707
Total epoch: 19. epoch loss: 10.643302917480469
Total epoch: 20. epoch loss: 10.394466400146484
Total epoch: 21. epoch loss: 10.151703834533691
Total epoch: 22. epoch loss: 9.915389060974121
Total epoch: 23. epoch loss: 9.685744285583496
Total epoch: 24. epoch loss: 9.462971687316895
Total epoch: 25. epoch loss: 9.247111320495605
Total epoch: 26. epoch loss: 9.038178443908691
Total epoch: 27. epoch loss: 8.836085319519043
Total epoch: 28. epoch loss: 8.640719413757324
Total epoch: 29. epoch loss: 8.45190715789795
Total epoch: 30. epoch loss: 8.269445419311523
Total epoch: 31. epoch loss: 8.093133926391602
Total epoch: 32. epoch loss: 7.922788143157959
Total epoch: 33. epoch loss: 7.758149147033691
Total epoch: 34. epoch loss: 7.599003314971924
Total epoch: 35. epoch loss: 7.445133209228516
Total epoch: 36. epoch loss: 7.296347141265869
Total epoch: 37. epoch loss: 7.15244197845459
Total epoch: 38. epoch loss: 7.013247489929199
Total epoch: 39. epoch loss: 6.878604412078857
Total epoch: 40. epoch loss: 6.748363494873047
Total epoch: 41. epoch loss: 6.622345447540283
Total epoch: 42. epoch loss: 6.500410556793213
Total epoch: 43. epoch loss: 6.382418632507324
Total epoch: 44. epoch loss: 6.268214225769043
Total epoch: 45. epoch loss: 6.157658576965332
Total epoch: 46. epoch loss: 6.050614356994629
Total epoch: 47. epoch loss: 5.946945667266846
Total epoch: 48. epoch loss: 5.84652042388916
Total epoch: 49. epoch loss: 5.7492194175720215
Total epoch: 50. epoch loss: 5.654889106750488
Total epoch: 51. epoch loss: 5.5634284019470215
Total epoch: 52. epoch loss: 5.474686145782471
Total epoch: 53. epoch loss: 5.388585567474365
Total epoch: 54. epoch loss: 5.304998397827148
Total epoch: 55. epoch loss: 5.2238359451293945
Total epoch: 56. epoch loss: 5.144991874694824
Total epoch: 57. epoch loss: 5.068373680114746
Total epoch: 58. epoch loss: 4.9939117431640625
Total epoch: 59. epoch loss: 4.9215006828308105
Total epoch: 60. epoch loss: 4.851052761077881
Total epoch: 61. epoch loss: 4.782533645629883
Total epoch: 62. epoch loss: 4.715839862823486
Total epoch: 63. epoch loss: 4.6509108543396
Total epoch: 64. epoch loss: 4.587673664093018
Total epoch: 65. epoch loss: 4.526085376739502
Total epoch: 66. epoch loss: 4.466057300567627
Total epoch: 67. epoch loss: 4.407552242279053
Total epoch: 68. epoch loss: 4.350493431091309
Total epoch: 69. epoch loss: 4.294859886169434
Total epoch: 70. epoch loss: 4.240577697753906
Total epoch: 71. epoch loss: 4.187612533569336
Total epoch: 72. epoch loss: 4.1359124183654785
Total epoch: 73. epoch loss: 4.085421562194824
Total epoch: 74. epoch loss: 4.036130428314209
Total epoch: 75. epoch loss: 3.9879682064056396
Total epoch: 76. epoch loss: 3.940918445587158
Total epoch: 77. epoch loss: 3.89493989944458
Total epoch: 78. epoch loss: 3.849984884262085
Total epoch: 79. epoch loss: 3.8060302734375
Total epoch: 80. epoch loss: 3.763046979904175
Total epoch: 81. epoch loss: 3.720994472503662
Total epoch: 82. epoch loss: 3.6798572540283203
Total epoch: 83. epoch loss: 3.639596462249756
Total epoch: 84. epoch loss: 3.60017728805542
Total epoch: 85. epoch loss: 3.561591386795044
Total epoch: 86. epoch loss: 3.523801803588867
Total epoch: 87. epoch loss: 3.486797571182251
Total epoch: 88. epoch loss: 3.4505369663238525
Total epoch: 89. epoch loss: 3.415004014968872
Total epoch: 90. epoch loss: 3.380190372467041
Total epoch: 91. epoch loss: 3.346067190170288
Total epoch: 92. epoch loss: 3.312607526779175
Total epoch: 93. epoch loss: 3.27980375289917
Total epoch: 94. epoch loss: 3.2476272583007812
Total epoch: 95. epoch loss: 3.216069221496582
Total epoch: 96. epoch loss: 3.1851131916046143
Total epoch: 97. epoch loss: 3.154736042022705
Total epoch: 98. epoch loss: 3.124931812286377
Total epoch: 99. epoch loss: 3.095675468444824
Total epoch: 99. DecT loss: 3.095675468444824
Training time: 0.8098766803741455
APL_precision: 0.10943396226415095, APL_recall: 0.3411764705882353, APL_f1: 0.16571428571428573, APL_number: 170
CMT_precision: 0.2, CMT_recall: 0.5538461538461539, CMT_f1: 0.2938775510204082, CMT_number: 195
DSC_precision: 0.35435435435435436, DSC_recall: 0.540045766590389, DSC_f1: 0.42792384406165007, DSC_number: 437
MAT_precision: 0.5178970917225951, MAT_recall: 0.6788856304985337, MAT_f1: 0.5875634517766497, MAT_number: 682
PRO_precision: 0.368474923234391, PRO_recall: 0.4669260700389105, PRO_f1: 0.4118993135011442, PRO_number: 771
SMT_precision: 0.17380352644836272, SMT_recall: 0.40350877192982454, SMT_f1: 0.24295774647887322, SMT_number: 171
SPL_precision: 0.2079207920792079, SPL_recall: 0.28, SPL_f1: 0.23863636363636365, SPL_number: 75
overall_precision: 0.32034104750304504, overall_recall: 0.5257896841263494, overall_f1: 0.3981229185588859, overall_accuracy: 0.7812477667405131
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/1800 [00:05<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 16:02:22 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 16:02:22 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-145490b5457c9ad0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 5
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1037.04it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 2646.64 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 16:02:33 - INFO - __main__ - ***** Running training *****
05/31/2023 16:02:33 - INFO - __main__ -   Num examples = 72
05/31/2023 16:02:33 - INFO - __main__ -   Num Epochs = 100
05/31/2023 16:02:33 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 16:02:33 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 16:02:33 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 16:02:33 - INFO - __main__ -   Total optimization steps = 1800
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/1800 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.594640731811523
Total epoch: 1. epoch loss: 16.28082275390625
Total epoch: 2. epoch loss: 15.969063758850098
Total epoch: 3. epoch loss: 15.657181739807129
Total epoch: 4. epoch loss: 15.345922470092773
Total epoch: 5. epoch loss: 15.036128044128418
Total epoch: 6. epoch loss: 14.728440284729004
Total epoch: 7. epoch loss: 14.423490524291992
Total epoch: 8. epoch loss: 14.122251510620117
Total epoch: 9. epoch loss: 13.825563430786133
Total epoch: 10. epoch loss: 13.534077644348145
Total epoch: 11. epoch loss: 13.248254776000977
Total epoch: 12. epoch loss: 12.968366622924805
Total epoch: 13. epoch loss: 12.694539070129395
Total epoch: 14. epoch loss: 12.426767349243164
Total epoch: 15. epoch loss: 12.164946556091309
Total epoch: 16. epoch loss: 11.908931732177734
Total epoch: 17. epoch loss: 11.658550262451172
Total epoch: 18. epoch loss: 11.413610458374023
Total epoch: 19. epoch loss: 11.173850059509277
Total epoch: 20. epoch loss: 10.939042091369629
Total epoch: 21. epoch loss: 10.70896053314209
Total epoch: 22. epoch loss: 10.483603477478027
Total epoch: 23. epoch loss: 10.263229370117188
Total epoch: 24. epoch loss: 10.048063278198242
Total epoch: 25. epoch loss: 9.838302612304688
Total epoch: 26. epoch loss: 9.634055137634277
Total epoch: 27. epoch loss: 9.435406684875488
Total epoch: 28. epoch loss: 9.242369651794434
Total epoch: 29. epoch loss: 9.054891586303711
Total epoch: 30. epoch loss: 8.872908592224121
Total epoch: 31. epoch loss: 8.696332931518555
Total epoch: 32. epoch loss: 8.52503776550293
Total epoch: 33. epoch loss: 8.358914375305176
Total epoch: 34. epoch loss: 8.197843551635742
Total epoch: 35. epoch loss: 8.041680335998535
Total epoch: 36. epoch loss: 7.890289783477783
Total epoch: 37. epoch loss: 7.743527412414551
Total epoch: 38. epoch loss: 7.601228713989258
Total epoch: 39. epoch loss: 7.46326208114624
Total epoch: 40. epoch loss: 7.329474449157715
Total epoch: 41. epoch loss: 7.199706077575684
Total epoch: 42. epoch loss: 7.073845863342285
Total epoch: 43. epoch loss: 6.951739311218262
Total epoch: 44. epoch loss: 6.833268165588379
Total epoch: 45. epoch loss: 6.718293190002441
Total epoch: 46. epoch loss: 6.606709003448486
Total epoch: 47. epoch loss: 6.498383522033691
Total epoch: 48. epoch loss: 6.393198490142822
Total epoch: 49. epoch loss: 6.291058540344238
Total epoch: 50. epoch loss: 6.1918439865112305
Total epoch: 51. epoch loss: 6.095448017120361
Total epoch: 52. epoch loss: 6.001775741577148
Total epoch: 53. epoch loss: 5.910745620727539
Total epoch: 54. epoch loss: 5.822247505187988
Total epoch: 55. epoch loss: 5.736172676086426
Total epoch: 56. epoch loss: 5.6524457931518555
Total epoch: 57. epoch loss: 5.570991516113281
Total epoch: 58. epoch loss: 5.491715431213379
Total epoch: 59. epoch loss: 5.414545059204102
Total epoch: 60. epoch loss: 5.339402675628662
Total epoch: 61. epoch loss: 5.266206741333008
Total epoch: 62. epoch loss: 5.194876194000244
Total epoch: 63. epoch loss: 5.1253790855407715
Total epoch: 64. epoch loss: 5.0576171875
Total epoch: 65. epoch loss: 4.991547107696533
Total epoch: 66. epoch loss: 4.927099227905273
Total epoch: 67. epoch loss: 4.8642144203186035
Total epoch: 68. epoch loss: 4.802846908569336
Total epoch: 69. epoch loss: 4.74293851852417
Total epoch: 70. epoch loss: 4.684430122375488
Total epoch: 71. epoch loss: 4.627294540405273
Total epoch: 72. epoch loss: 4.571477890014648
Total epoch: 73. epoch loss: 4.516932487487793
Total epoch: 74. epoch loss: 4.463617324829102
Total epoch: 75. epoch loss: 4.411506652832031
Total epoch: 76. epoch loss: 4.360544204711914
Total epoch: 77. epoch loss: 4.3107099533081055
Total epoch: 78. epoch loss: 4.2619547843933105
Total epoch: 79. epoch loss: 4.214256763458252
Total epoch: 80. epoch loss: 4.167579174041748
Total epoch: 81. epoch loss: 4.121883869171143
Total epoch: 82. epoch loss: 4.077160358428955
Total epoch: 83. epoch loss: 4.0333476066589355
Total epoch: 84. epoch loss: 3.99043607711792
Total epoch: 85. epoch loss: 3.948402166366577
Total epoch: 86. epoch loss: 3.9072072505950928
Total epoch: 87. epoch loss: 3.8668415546417236
Total epoch: 88. epoch loss: 3.827280282974243
Total epoch: 89. epoch loss: 3.788484811782837
Total epoch: 90. epoch loss: 3.7504420280456543
Total epoch: 91. epoch loss: 3.7131361961364746
Total epoch: 92. epoch loss: 3.6765401363372803
Total epoch: 93. epoch loss: 3.640631675720215
Total epoch: 94. epoch loss: 3.605403423309326
Total epoch: 95. epoch loss: 3.5708346366882324
Total epoch: 96. epoch loss: 3.5368826389312744
Total epoch: 97. epoch loss: 3.503568649291992
Total epoch: 98. epoch loss: 3.4708595275878906
Total epoch: 99. epoch loss: 3.438727855682373
Total epoch: 99. DecT loss: 3.438727855682373
Training time: 0.7891223430633545
APL_precision: 0.102803738317757, APL_recall: 0.3235294117647059, APL_f1: 0.15602836879432624, APL_number: 170
CMT_precision: 0.20527306967984935, CMT_recall: 0.558974358974359, CMT_f1: 0.3002754820936639, CMT_number: 195
DSC_precision: 0.3509544787077827, DSC_recall: 0.5469107551487414, DSC_f1: 0.4275491949910555, DSC_number: 437
MAT_precision: 0.5049944506104328, MAT_recall: 0.6671554252199413, MAT_f1: 0.5748578648136449, MAT_number: 682
PRO_precision: 0.3660262891809909, PRO_recall: 0.4695201037613489, PRO_f1: 0.4113636363636364, PRO_number: 771
SMT_precision: 0.16341463414634147, SMT_recall: 0.391812865497076, SMT_f1: 0.23063683304647162, SMT_number: 171
SPL_precision: 0.22826086956521738, SPL_recall: 0.28, SPL_f1: 0.251497005988024, SPL_number: 75
overall_precision: 0.31601836192316984, overall_recall: 0.5229908036785286, overall_f1: 0.3939759036144578, overall_accuracy: 0.7794611591510041
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/1800 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 16:03:32 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 16:03:34 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-145490b5457c9ad0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1254.47it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/72 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 2931.25 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 16:04:15 - INFO - __main__ - ***** Running training *****
05/31/2023 16:04:15 - INFO - __main__ -   Num examples = 72
05/31/2023 16:04:15 - INFO - __main__ -   Num Epochs = 100
05/31/2023 16:04:15 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 16:04:15 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 16:04:15 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 16:04:15 - INFO - __main__ -   Total optimization steps = 1800
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/1800 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.27073860168457
Total epoch: 1. epoch loss: 15.955781936645508
Total epoch: 2. epoch loss: 15.645978927612305
Total epoch: 3. epoch loss: 15.339950561523438
Total epoch: 4. epoch loss: 15.035937309265137
Total epoch: 5. epoch loss: 14.733121871948242
Total epoch: 6. epoch loss: 14.431925773620605
Total epoch: 7. epoch loss: 14.133520126342773
Total epoch: 8. epoch loss: 13.838932037353516
Total epoch: 9. epoch loss: 13.54899787902832
Total epoch: 10. epoch loss: 13.264309883117676
Total epoch: 11. epoch loss: 12.985255241394043
Total epoch: 12. epoch loss: 12.712044715881348
Total epoch: 13. epoch loss: 12.444766998291016
Total epoch: 14. epoch loss: 12.183372497558594
Total epoch: 15. epoch loss: 11.927773475646973
Total epoch: 16. epoch loss: 11.67782974243164
Total epoch: 17. epoch loss: 11.433375358581543
Total epoch: 18. epoch loss: 11.19419002532959
Total epoch: 19. epoch loss: 10.960091590881348
Total epoch: 20. epoch loss: 10.730862617492676
Total epoch: 21. epoch loss: 10.506258964538574
Total epoch: 22. epoch loss: 10.286133766174316
Total epoch: 23. epoch loss: 10.070769309997559
Total epoch: 24. epoch loss: 9.860380172729492
Total epoch: 25. epoch loss: 9.655163764953613
Total epoch: 26. epoch loss: 9.455236434936523
Total epoch: 27. epoch loss: 9.260690689086914
Total epoch: 28. epoch loss: 9.071575164794922
Total epoch: 29. epoch loss: 8.887885093688965
Total epoch: 30. epoch loss: 8.709589004516602
Total epoch: 31. epoch loss: 8.536601066589355
Total epoch: 32. epoch loss: 8.368792533874512
Total epoch: 33. epoch loss: 8.206027030944824
Total epoch: 34. epoch loss: 8.048158645629883
Total epoch: 35. epoch loss: 7.895054817199707
Total epoch: 36. epoch loss: 7.746557712554932
Total epoch: 37. epoch loss: 7.60252046585083
Total epoch: 38. epoch loss: 7.462823390960693
Total epoch: 39. epoch loss: 7.327325820922852
Total epoch: 40. epoch loss: 7.1959099769592285
Total epoch: 41. epoch loss: 7.068412780761719
Total epoch: 42. epoch loss: 6.944721221923828
Total epoch: 43. epoch loss: 6.824702262878418
Total epoch: 44. epoch loss: 6.708225250244141
Total epoch: 45. epoch loss: 6.595186233520508
Total epoch: 46. epoch loss: 6.485459327697754
Total epoch: 47. epoch loss: 6.378921031951904
Total epoch: 48. epoch loss: 6.2754926681518555
Total epoch: 49. epoch loss: 6.175025463104248
Total epoch: 50. epoch loss: 6.07744026184082
Total epoch: 51. epoch loss: 5.982633590698242
Total epoch: 52. epoch loss: 5.890509605407715
Total epoch: 53. epoch loss: 5.800949573516846
Total epoch: 54. epoch loss: 5.713895320892334
Total epoch: 55. epoch loss: 5.629243850708008
Total epoch: 56. epoch loss: 5.546898365020752
Total epoch: 57. epoch loss: 5.46679162979126
Total epoch: 58. epoch loss: 5.388819217681885
Total epoch: 59. epoch loss: 5.312928676605225
Total epoch: 60. epoch loss: 5.239021301269531
Total epoch: 61. epoch loss: 5.1670427322387695
Total epoch: 62. epoch loss: 5.096907138824463
Total epoch: 63. epoch loss: 5.028566360473633
Total epoch: 64. epoch loss: 4.961950302124023
Total epoch: 65. epoch loss: 4.89696741104126
Total epoch: 66. epoch loss: 4.833600997924805
Total epoch: 67. epoch loss: 4.771759510040283
Total epoch: 68. epoch loss: 4.711438179016113
Total epoch: 69. epoch loss: 4.652534484863281
Total epoch: 70. epoch loss: 4.59501314163208
Total epoch: 71. epoch loss: 4.5388503074646
Total epoch: 72. epoch loss: 4.483980178833008
Total epoch: 73. epoch loss: 4.430379867553711
Total epoch: 74. epoch loss: 4.377985954284668
Total epoch: 75. epoch loss: 4.326777458190918
Total epoch: 76. epoch loss: 4.276698112487793
Total epoch: 77. epoch loss: 4.227737903594971
Total epoch: 78. epoch loss: 4.179839611053467
Total epoch: 79. epoch loss: 4.13298225402832
Total epoch: 80. epoch loss: 4.087121963500977
Total epoch: 81. epoch loss: 4.042238235473633
Total epoch: 82. epoch loss: 3.998297691345215
Total epoch: 83. epoch loss: 3.955274820327759
Total epoch: 84. epoch loss: 3.913132429122925
Total epoch: 85. epoch loss: 3.871863842010498
Total epoch: 86. epoch loss: 3.831414222717285
Total epoch: 87. epoch loss: 3.791775941848755
Total epoch: 88. epoch loss: 3.752936601638794
Total epoch: 89. epoch loss: 3.714845657348633
Total epoch: 90. epoch loss: 3.6775052547454834
Total epoch: 91. epoch loss: 3.640890121459961
Total epoch: 92. epoch loss: 3.6049726009368896
Total epoch: 93. epoch loss: 3.569730758666992
Total epoch: 94. epoch loss: 3.535149097442627
Total epoch: 95. epoch loss: 3.5012307167053223
Total epoch: 96. epoch loss: 3.4679300785064697
Total epoch: 97. epoch loss: 3.435239553451538
Total epoch: 98. epoch loss: 3.4031424522399902
Total epoch: 99. epoch loss: 3.3716304302215576
Total epoch: 99. DecT loss: 3.3716304302215576
Training time: 0.8147201538085938
APL_precision: 0.10175438596491228, APL_recall: 0.3411764705882353, APL_f1: 0.15675675675675677, APL_number: 170
CMT_precision: 0.1879432624113475, CMT_recall: 0.5435897435897435, CMT_f1: 0.27931488801054016, CMT_number: 195
DSC_precision: 0.39195979899497485, DSC_recall: 0.5354691075514875, DSC_f1: 0.45261121856866543, DSC_number: 437
MAT_precision: 0.5043668122270742, MAT_recall: 0.6774193548387096, MAT_f1: 0.5782227784730913, MAT_number: 682
PRO_precision: 0.3655378486055777, PRO_recall: 0.47600518806744485, PRO_f1: 0.41352112676056335, PRO_number: 771
SMT_precision: 0.15833333333333333, SMT_recall: 0.3333333333333333, SMT_f1: 0.21468926553672316, SMT_number: 171
SPL_precision: 0.20952380952380953, SPL_recall: 0.29333333333333333, SPL_f1: 0.24444444444444444, SPL_number: 75
overall_precision: 0.3172983479105928, overall_recall: 0.5221911235505797, overall_f1: 0.39474081910231223, overall_accuracy: 0.7796755520617451
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/1800 [00:05<?, ?it/s]