/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 14:51:28 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 14:51:29 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 160
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1045.18it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3286.94 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 14:51:38 - INFO - __main__ - ***** Running training *****
05/31/2023 14:51:38 - INFO - __main__ -   Num examples = 77
05/31/2023 14:51:38 - INFO - __main__ -   Num Epochs = 100
05/31/2023 14:51:38 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 14:51:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 14:51:38 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 14:51:38 - INFO - __main__ -   Total optimization steps = 2000
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/2000 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.578166961669922
Total epoch: 1. epoch loss: 16.225791931152344
Total epoch: 2. epoch loss: 15.874720573425293
Total epoch: 3. epoch loss: 15.52377700805664
Total epoch: 4. epoch loss: 15.174140930175781
Total epoch: 5. epoch loss: 14.826689720153809
Total epoch: 6. epoch loss: 14.482237815856934
Total epoch: 7. epoch loss: 14.142059326171875
Total epoch: 8. epoch loss: 13.807340621948242
Total epoch: 9. epoch loss: 13.4790678024292
Total epoch: 10. epoch loss: 13.158018112182617
Total epoch: 11. epoch loss: 12.844696044921875
Total epoch: 12. epoch loss: 12.53939437866211
Total epoch: 13. epoch loss: 12.242232322692871
Total epoch: 14. epoch loss: 11.953142166137695
Total epoch: 15. epoch loss: 11.67197036743164
Total epoch: 16. epoch loss: 11.398436546325684
Total epoch: 17. epoch loss: 11.132229804992676
Total epoch: 18. epoch loss: 10.873002052307129
Total epoch: 19. epoch loss: 10.620376586914062
Total epoch: 20. epoch loss: 10.374080657958984
Total epoch: 21. epoch loss: 10.134390830993652
Total epoch: 22. epoch loss: 9.901527404785156
Total epoch: 23. epoch loss: 9.675620079040527
Total epoch: 24. epoch loss: 9.456727027893066
Total epoch: 25. epoch loss: 9.244841575622559
Total epoch: 26. epoch loss: 9.039888381958008
Total epoch: 27. epoch loss: 8.84177017211914
Total epoch: 28. epoch loss: 8.650314331054688
Total epoch: 29. epoch loss: 8.465360641479492
Total epoch: 30. epoch loss: 8.28669261932373
Total epoch: 31. epoch loss: 8.114129066467285
Total epoch: 32. epoch loss: 7.947408676147461
Total epoch: 33. epoch loss: 7.786320209503174
Total epoch: 34. epoch loss: 7.6306304931640625
Total epoch: 35. epoch loss: 7.480112552642822
Total epoch: 36. epoch loss: 7.334568977355957
Total epoch: 37. epoch loss: 7.193788051605225
Total epoch: 38. epoch loss: 7.057621955871582
Total epoch: 39. epoch loss: 6.925857067108154
Total epoch: 40. epoch loss: 6.798351287841797
Total epoch: 41. epoch loss: 6.674942493438721
Total epoch: 42. epoch loss: 6.555475234985352
Total epoch: 43. epoch loss: 6.439812660217285
Total epoch: 44. epoch loss: 6.327826499938965
Total epoch: 45. epoch loss: 6.219363689422607
Total epoch: 46. epoch loss: 6.114308834075928
Total epoch: 47. epoch loss: 6.012541770935059
Total epoch: 48. epoch loss: 5.913928508758545
Total epoch: 49. epoch loss: 5.818349361419678
Total epoch: 50. epoch loss: 5.725684642791748
Total epoch: 51. epoch loss: 5.6358113288879395
Total epoch: 52. epoch loss: 5.548625469207764
Total epoch: 53. epoch loss: 5.4639997482299805
Total epoch: 54. epoch loss: 5.381860733032227
Total epoch: 55. epoch loss: 5.302073955535889
Total epoch: 56. epoch loss: 5.224563121795654
Total epoch: 57. epoch loss: 5.149233818054199
Total epoch: 58. epoch loss: 5.075986862182617
Total epoch: 59. epoch loss: 5.0047526359558105
Total epoch: 60. epoch loss: 4.935457229614258
Total epoch: 61. epoch loss: 4.868005752563477
Total epoch: 62. epoch loss: 4.802356719970703
Total epoch: 63. epoch loss: 4.738430500030518
Total epoch: 64. epoch loss: 4.676146030426025
Total epoch: 65. epoch loss: 4.615459442138672
Total epoch: 66. epoch loss: 4.556317329406738
Total epoch: 67. epoch loss: 4.498650550842285
Total epoch: 68. epoch loss: 4.442412853240967
Total epoch: 69. epoch loss: 4.387558937072754
Total epoch: 70. epoch loss: 4.334027290344238
Total epoch: 71. epoch loss: 4.281777381896973
Total epoch: 72. epoch loss: 4.230761528015137
Total epoch: 73. epoch loss: 4.180946350097656
Total epoch: 74. epoch loss: 4.132273197174072
Total epoch: 75. epoch loss: 4.084721088409424
Total epoch: 76. epoch loss: 4.038241863250732
Total epoch: 77. epoch loss: 3.9928107261657715
Total epoch: 78. epoch loss: 3.9483864307403564
Total epoch: 79. epoch loss: 3.90493106842041
Total epoch: 80. epoch loss: 3.862422227859497
Total epoch: 81. epoch loss: 3.820819139480591
Total epoch: 82. epoch loss: 3.7800981998443604
Total epoch: 83. epoch loss: 3.740236759185791
Total epoch: 84. epoch loss: 3.7012016773223877
Total epoch: 85. epoch loss: 3.662982702255249
Total epoch: 86. epoch loss: 3.6255271434783936
Total epoch: 87. epoch loss: 3.588841438293457
Total epoch: 88. epoch loss: 3.552873134613037
Total epoch: 89. epoch loss: 3.517629384994507
Total epoch: 90. epoch loss: 3.483060598373413
Total epoch: 91. epoch loss: 3.449178457260132
Total epoch: 92. epoch loss: 3.415947914123535
Total epoch: 93. epoch loss: 3.3833401203155518
Total epoch: 94. epoch loss: 3.3513553142547607
Total epoch: 95. epoch loss: 3.3199570178985596
Total epoch: 96. epoch loss: 3.2891530990600586
Total epoch: 97. epoch loss: 3.2589104175567627
Total epoch: 98. epoch loss: 3.2292189598083496
Total epoch: 99. epoch loss: 3.2000677585601807
Total epoch: 99. DecT loss: 3.2000677585601807
Training time: 0.8414344787597656
APL_precision: 0.1597444089456869, APL_recall: 0.29411764705882354, APL_f1: 0.20703933747412007, APL_number: 170
CMT_precision: 0.2003780718336484, CMT_recall: 0.5435897435897435, CMT_f1: 0.292817679558011, CMT_number: 195
DSC_precision: 0.42757417102966844, DSC_recall: 0.5606407322654462, DSC_f1: 0.48514851485148514, DSC_number: 437
MAT_precision: 0.5172413793103449, MAT_recall: 0.6378299120234604, MAT_f1: 0.5712409717662509, MAT_number: 682
PRO_precision: 0.295144157814871, PRO_recall: 0.5045395590142672, PRO_f1: 0.3724269985639062, PRO_number: 771
SMT_precision: 0.15051020408163265, SMT_recall: 0.34502923976608185, SMT_f1: 0.20959147424511546, SMT_number: 171
SPL_precision: 0.1564625850340136, SPL_recall: 0.30666666666666664, SPL_f1: 0.2072072072072072, SPL_number: 75
overall_precision: 0.3177729151470946, overall_recall: 0.5225909636145541, overall_f1: 0.3952222558209858, overall_accuracy: 0.775887943971986
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/2000 [00:04<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 16:02:21 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 16:02:24 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 5
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1047.01it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 2367.66 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 16:02:35 - INFO - __main__ - ***** Running training *****
05/31/2023 16:02:35 - INFO - __main__ -   Num examples = 77
05/31/2023 16:02:35 - INFO - __main__ -   Num Epochs = 100
05/31/2023 16:02:35 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 16:02:35 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 16:02:35 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 16:02:35 - INFO - __main__ -   Total optimization steps = 2000
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/2000 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.609432220458984
Total epoch: 1. epoch loss: 16.293188095092773
Total epoch: 2. epoch loss: 15.979737281799316
Total epoch: 3. epoch loss: 15.666460037231445
Total epoch: 4. epoch loss: 15.35350227355957
Total epoch: 5. epoch loss: 15.041723251342773
Total epoch: 6. epoch loss: 14.731861114501953
Total epoch: 7. epoch loss: 14.424635887145996
Total epoch: 8. epoch loss: 14.121017456054688
Total epoch: 9. epoch loss: 13.821914672851562
Total epoch: 10. epoch loss: 13.528076171875
Total epoch: 11. epoch loss: 13.240102767944336
Total epoch: 12. epoch loss: 12.95840072631836
Total epoch: 13. epoch loss: 12.68319320678711
Total epoch: 14. epoch loss: 12.414552688598633
Total epoch: 15. epoch loss: 12.152403831481934
Total epoch: 16. epoch loss: 11.896635055541992
Total epoch: 17. epoch loss: 11.647034645080566
Total epoch: 18. epoch loss: 11.403356552124023
Total epoch: 19. epoch loss: 11.165340423583984
Total epoch: 20. epoch loss: 10.932707786560059
Total epoch: 21. epoch loss: 10.705153465270996
Total epoch: 22. epoch loss: 10.482605934143066
Total epoch: 23. epoch loss: 10.265286445617676
Total epoch: 24. epoch loss: 10.053345680236816
Total epoch: 25. epoch loss: 9.84694766998291
Total epoch: 26. epoch loss: 9.64610767364502
Total epoch: 27. epoch loss: 9.450916290283203
Total epoch: 28. epoch loss: 9.261345863342285
Total epoch: 29. epoch loss: 9.077371597290039
Total epoch: 30. epoch loss: 8.898881912231445
Total epoch: 31. epoch loss: 8.725783348083496
Total epoch: 32. epoch loss: 8.55793571472168
Total epoch: 33. epoch loss: 8.395215034484863
Total epoch: 34. epoch loss: 8.23746109008789
Total epoch: 35. epoch loss: 8.084541320800781
Total epoch: 36. epoch loss: 7.936291694641113
Total epoch: 37. epoch loss: 7.792579650878906
Total epoch: 38. epoch loss: 7.653198719024658
Total epoch: 39. epoch loss: 7.518031597137451
Total epoch: 40. epoch loss: 7.386910438537598
Total epoch: 41. epoch loss: 7.259675979614258
Total epoch: 42. epoch loss: 7.1362223625183105
Total epoch: 43. epoch loss: 7.016386032104492
Total epoch: 44. epoch loss: 6.900061130523682
Total epoch: 45. epoch loss: 6.7871317863464355
Total epoch: 46. epoch loss: 6.677486896514893
Total epoch: 47. epoch loss: 6.570976257324219
Total epoch: 48. epoch loss: 6.4675374031066895
Total epoch: 49. epoch loss: 6.367038726806641
Total epoch: 50. epoch loss: 6.269400596618652
Total epoch: 51. epoch loss: 6.174495220184326
Total epoch: 52. epoch loss: 6.082258701324463
Total epoch: 53. epoch loss: 5.992574214935303
Total epoch: 54. epoch loss: 5.9053521156311035
Total epoch: 55. epoch loss: 5.820518970489502
Total epoch: 56. epoch loss: 5.7379655838012695
Total epoch: 57. epoch loss: 5.657629013061523
Total epoch: 58. epoch loss: 5.579405784606934
Total epoch: 59. epoch loss: 5.5032429695129395
Total epoch: 60. epoch loss: 5.429061412811279
Total epoch: 61. epoch loss: 5.356759071350098
Total epoch: 62. epoch loss: 5.286296844482422
Total epoch: 63. epoch loss: 5.217593193054199
Total epoch: 64. epoch loss: 5.150589942932129
Total epoch: 65. epoch loss: 5.085214138031006
Total epoch: 66. epoch loss: 5.021419525146484
Total epoch: 67. epoch loss: 4.959158420562744
Total epoch: 68. epoch loss: 4.898369312286377
Total epoch: 69. epoch loss: 4.838990688323975
Total epoch: 70. epoch loss: 4.7809977531433105
Total epoch: 71. epoch loss: 4.72433614730835
Total epoch: 72. epoch loss: 4.668956279754639
Total epoch: 73. epoch loss: 4.614830493927002
Total epoch: 74. epoch loss: 4.561923980712891
Total epoch: 75. epoch loss: 4.5101752281188965
Total epoch: 76. epoch loss: 4.459556579589844
Total epoch: 77. epoch loss: 4.410053730010986
Total epoch: 78. epoch loss: 4.361597537994385
Total epoch: 79. epoch loss: 4.314172744750977
Total epoch: 80. epoch loss: 4.267747402191162
Total epoch: 81. epoch loss: 4.222298622131348
Total epoch: 82. epoch loss: 4.177778720855713
Total epoch: 83. epoch loss: 4.134170055389404
Total epoch: 84. epoch loss: 4.091442584991455
Total epoch: 85. epoch loss: 4.049570560455322
Total epoch: 86. epoch loss: 4.00853967666626
Total epoch: 87. epoch loss: 3.9683053493499756
Total epoch: 88. epoch loss: 3.928854465484619
Total epoch: 89. epoch loss: 3.890178918838501
Total epoch: 90. epoch loss: 3.852229356765747
Total epoch: 91. epoch loss: 3.8150007724761963
Total epoch: 92. epoch loss: 3.778473377227783
Total epoch: 93. epoch loss: 3.7426249980926514
Total epoch: 94. epoch loss: 3.707444667816162
Total epoch: 95. epoch loss: 3.6729016304016113
Total epoch: 96. epoch loss: 3.6389877796173096
Total epoch: 97. epoch loss: 3.605680465698242
Total epoch: 98. epoch loss: 3.572971820831299
Total epoch: 99. epoch loss: 3.5408403873443604
Total epoch: 99. DecT loss: 3.5408403873443604
Training time: 0.8026127815246582
APL_precision: 0.16883116883116883, APL_recall: 0.3058823529411765, APL_f1: 0.2175732217573222, APL_number: 170
CMT_precision: 0.20226843100189035, CMT_recall: 0.5487179487179488, CMT_f1: 0.2955801104972376, CMT_number: 195
DSC_precision: 0.41737649063032367, DSC_recall: 0.5606407322654462, DSC_f1: 0.478515625, DSC_number: 437
MAT_precision: 0.5005847953216375, MAT_recall: 0.6275659824046921, MAT_f1: 0.5569290826284972, MAT_number: 682
PRO_precision: 0.2904942965779468, PRO_recall: 0.49546044098573283, PRO_f1: 0.3662511984659636, PRO_number: 771
SMT_precision: 0.14425427872860636, SMT_recall: 0.34502923976608185, SMT_f1: 0.20344827586206896, SMT_number: 171
SPL_precision: 0.16891891891891891, SPL_recall: 0.3333333333333333, SPL_f1: 0.22421524663677128, SPL_number: 75
overall_precision: 0.3126957359672368, overall_recall: 0.5189924030387845, overall_f1: 0.39025856885147325, overall_accuracy: 0.7732437647395126
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/2000 [00:05<?, ?it/s]/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 16:03:32 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 16:03:33 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:4, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1032.83it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading file vocab.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json
loading file merges.txt from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 2539.08 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 16:03:54 - INFO - __main__ - ***** Running training *****
05/31/2023 16:03:54 - INFO - __main__ -   Num examples = 77
05/31/2023 16:03:54 - INFO - __main__ -   Num Epochs = 100
05/31/2023 16:03:54 - INFO - __main__ -   Instantaneous batch size per device = 4
05/31/2023 16:03:54 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/31/2023 16:03:54 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 16:03:54 - INFO - __main__ -   Total optimization steps = 2000
tensor([3, 3, 3, 3, 3, 3, 3], device='cuda:0')
  0%|          | 0/2000 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.391191482543945
Total epoch: 1. epoch loss: 16.072696685791016
Total epoch: 2. epoch loss: 15.759228706359863
Total epoch: 3. epoch loss: 15.449579238891602
Total epoch: 4. epoch loss: 15.141995429992676
Total epoch: 5. epoch loss: 14.835495948791504
Total epoch: 6. epoch loss: 14.530466079711914
Total epoch: 7. epoch loss: 14.228048324584961
Total epoch: 8. epoch loss: 13.9293212890625
Total epoch: 9. epoch loss: 13.63520336151123
Total epoch: 10. epoch loss: 13.346388816833496
Total epoch: 11. epoch loss: 13.063400268554688
Total epoch: 12. epoch loss: 12.78658390045166
Total epoch: 13. epoch loss: 12.51611042022705
Total epoch: 14. epoch loss: 12.252037048339844
Total epoch: 15. epoch loss: 11.994318962097168
Total epoch: 16. epoch loss: 11.742814064025879
Total epoch: 17. epoch loss: 11.497347831726074
Total epoch: 18. epoch loss: 11.257678031921387
Total epoch: 19. epoch loss: 11.023561477661133
Total epoch: 20. epoch loss: 10.794718742370605
Total epoch: 21. epoch loss: 10.570868492126465
Total epoch: 22. epoch loss: 10.351754188537598
Total epoch: 23. epoch loss: 10.137609481811523
Total epoch: 24. epoch loss: 9.928629875183105
Total epoch: 25. epoch loss: 9.724963188171387
Total epoch: 26. epoch loss: 9.526679039001465
Total epoch: 27. epoch loss: 9.33384895324707
Total epoch: 28. epoch loss: 9.146474838256836
Total epoch: 29. epoch loss: 8.964558601379395
Total epoch: 30. epoch loss: 8.788030624389648
Total epoch: 31. epoch loss: 8.616829872131348
Total epoch: 32. epoch loss: 8.450831413269043
Total epoch: 33. epoch loss: 8.289875030517578
Total epoch: 34. epoch loss: 8.133818626403809
Total epoch: 35. epoch loss: 7.982496738433838
Total epoch: 36. epoch loss: 7.835753440856934
Total epoch: 37. epoch loss: 7.693431377410889
Total epoch: 38. epoch loss: 7.555408477783203
Total epoch: 39. epoch loss: 7.421515464782715
Total epoch: 40. epoch loss: 7.291621208190918
Total epoch: 41. epoch loss: 7.165604114532471
Total epoch: 42. epoch loss: 7.043309211730957
Total epoch: 43. epoch loss: 6.9246134757995605
Total epoch: 44. epoch loss: 6.809389114379883
Total epoch: 45. epoch loss: 6.69753360748291
Total epoch: 46. epoch loss: 6.58892822265625
Total epoch: 47. epoch loss: 6.48346471786499
Total epoch: 48. epoch loss: 6.381041526794434
Total epoch: 49. epoch loss: 6.28155517578125
Total epoch: 50. epoch loss: 6.184903144836426
Total epoch: 51. epoch loss: 6.090996742248535
Total epoch: 52. epoch loss: 5.999725341796875
Total epoch: 53. epoch loss: 5.911005020141602
Total epoch: 54. epoch loss: 5.824716091156006
Total epoch: 55. epoch loss: 5.74080228805542
Total epoch: 56. epoch loss: 5.659175395965576
Total epoch: 57. epoch loss: 5.579746723175049
Total epoch: 58. epoch loss: 5.502436637878418
Total epoch: 59. epoch loss: 5.427155494689941
Total epoch: 60. epoch loss: 5.353831768035889
Total epoch: 61. epoch loss: 5.282388210296631
Total epoch: 62. epoch loss: 5.212771892547607
Total epoch: 63. epoch loss: 5.144896984100342
Total epoch: 64. epoch loss: 5.0787129402160645
Total epoch: 65. epoch loss: 5.014153003692627
Total epoch: 66. epoch loss: 4.951159477233887
Total epoch: 67. epoch loss: 4.889678478240967
Total epoch: 68. epoch loss: 4.829665660858154
Total epoch: 69. epoch loss: 4.771050930023193
Total epoch: 70. epoch loss: 4.713802337646484
Total epoch: 71. epoch loss: 4.657871723175049
Total epoch: 72. epoch loss: 4.603221893310547
Total epoch: 73. epoch loss: 4.549803733825684
Total epoch: 74. epoch loss: 4.497570991516113
Total epoch: 75. epoch loss: 4.446508884429932
Total epoch: 76. epoch loss: 4.39656925201416
Total epoch: 77. epoch loss: 4.347705841064453
Total epoch: 78. epoch loss: 4.299894332885742
Total epoch: 79. epoch loss: 4.253098964691162
Total epoch: 80. epoch loss: 4.207287788391113
Total epoch: 81. epoch loss: 4.162434101104736
Total epoch: 82. epoch loss: 4.118516445159912
Total epoch: 83. epoch loss: 4.07548713684082
Total epoch: 84. epoch loss: 4.033340930938721
Total epoch: 85. epoch loss: 3.9920356273651123
Total epoch: 86. epoch loss: 3.951547145843506
Total epoch: 87. epoch loss: 3.911858081817627
Total epoch: 88. epoch loss: 3.8729422092437744
Total epoch: 89. epoch loss: 3.834780216217041
Total epoch: 90. epoch loss: 3.79734206199646
Total epoch: 91. epoch loss: 3.7606189250946045
Total epoch: 92. epoch loss: 3.724593162536621
Total epoch: 93. epoch loss: 3.6892282962799072
Total epoch: 94. epoch loss: 3.6545281410217285
Total epoch: 95. epoch loss: 3.62046217918396
Total epoch: 96. epoch loss: 3.5870063304901123
Total epoch: 97. epoch loss: 3.5541610717773438
Total epoch: 98. epoch loss: 3.5219039916992188
Total epoch: 99. epoch loss: 3.490200996398926
Total epoch: 99. DecT loss: 3.490200996398926
Training time: 0.8033092021942139
APL_precision: 0.15548780487804878, APL_recall: 0.3, APL_f1: 0.20481927710843376, APL_number: 170
CMT_precision: 0.19141323792486584, CMT_recall: 0.5487179487179488, CMT_f1: 0.2838196286472149, CMT_number: 195
DSC_precision: 0.4485294117647059, DSC_recall: 0.5583524027459954, DSC_f1: 0.49745158002038736, DSC_number: 437
MAT_precision: 0.4856815578465063, MAT_recall: 0.6217008797653959, MAT_f1: 0.5453376205787782, MAT_number: 682
PRO_precision: 0.27901785714285715, PRO_recall: 0.48638132295719844, PRO_f1: 0.35460992907801414, PRO_number: 771
SMT_precision: 0.16766467065868262, SMT_recall: 0.32748538011695905, SMT_f1: 0.22178217821782176, SMT_number: 171
SPL_precision: 0.1388888888888889, SPL_recall: 0.3333333333333333, SPL_f1: 0.19607843137254902, SPL_number: 75
overall_precision: 0.30802498798654493, overall_recall: 0.512594962015194, overall_f1: 0.38481164640552307, overall_accuracy: 0.7710283713285214
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 487, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/2000 [00:05<?, ?it/s]