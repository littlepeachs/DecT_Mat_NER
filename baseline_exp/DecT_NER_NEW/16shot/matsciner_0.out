/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:44:27 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:44:28 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-61df0b13c7c2e0bc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 977.01it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/45 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4528.56 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:44:36 - INFO - __main__ - ***** Running training *****
05/31/2023 13:44:36 - INFO - __main__ -   Num examples = 45
05/31/2023 13:44:36 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:44:36 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:44:36 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:44:36 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:44:36 - INFO - __main__ -   Total optimization steps = 200
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.753860473632812
Total epoch: 1. epoch loss: 14.854606628417969
Total epoch: 2. epoch loss: 13.980181694030762
Total epoch: 3. epoch loss: 13.133026123046875
Total epoch: 4. epoch loss: 12.315825462341309
Total epoch: 5. epoch loss: 11.53109073638916
Total epoch: 6. epoch loss: 10.780840873718262
Total epoch: 7. epoch loss: 10.066383361816406
Total epoch: 8. epoch loss: 9.388270378112793
Total epoch: 9. epoch loss: 8.7466402053833
Total epoch: 10. epoch loss: 8.141566276550293
Total epoch: 11. epoch loss: 7.573123455047607
Total epoch: 12. epoch loss: 7.041191101074219
Total epoch: 13. epoch loss: 6.545286655426025
Total epoch: 14. epoch loss: 6.084469795227051
Total epoch: 15. epoch loss: 5.657191276550293
Total epoch: 16. epoch loss: 5.2614898681640625
Total epoch: 17. epoch loss: 4.895034313201904
Total epoch: 18. epoch loss: 4.555405139923096
Total epoch: 19. epoch loss: 4.2403106689453125
Total epoch: 20. epoch loss: 3.947603702545166
Total epoch: 21. epoch loss: 3.6755311489105225
Total epoch: 22. epoch loss: 3.423677682876587
Total epoch: 23. epoch loss: 3.191345691680908
Total epoch: 24. epoch loss: 2.977569580078125
Total epoch: 25. epoch loss: 2.7812068462371826
Total epoch: 26. epoch loss: 2.601106882095337
Total epoch: 27. epoch loss: 2.4361510276794434
Total epoch: 28. epoch loss: 2.2853219509124756
Total epoch: 29. epoch loss: 2.1477036476135254
Total epoch: 30. epoch loss: 2.02244234085083
Total epoch: 31. epoch loss: 1.9086560010910034
Total epoch: 32. epoch loss: 1.8054344654083252
Total epoch: 33. epoch loss: 1.711851954460144
Total epoch: 34. epoch loss: 1.6269739866256714
Total epoch: 35. epoch loss: 1.549894094467163
Total epoch: 36. epoch loss: 1.4797756671905518
Total epoch: 37. epoch loss: 1.4158788919448853
Total epoch: 38. epoch loss: 1.3575297594070435
Total epoch: 39. epoch loss: 1.3041558265686035
Total epoch: 40. epoch loss: 1.2552367448806763
Total epoch: 41. epoch loss: 1.2103018760681152
Total epoch: 42. epoch loss: 1.168949842453003
Total epoch: 43. epoch loss: 1.1307908296585083
Total epoch: 44. epoch loss: 1.095506191253662
Total epoch: 45. epoch loss: 1.0627824068069458
Total epoch: 46. epoch loss: 1.0323675870895386
Total epoch: 47. epoch loss: 1.004023551940918
Total epoch: 48. epoch loss: 0.9775506258010864
Total epoch: 49. epoch loss: 0.9527644515037537
Total epoch: 50. epoch loss: 0.9295240640640259
Total epoch: 51. epoch loss: 0.907667875289917
Total epoch: 52. epoch loss: 0.887092649936676
Total epoch: 53. epoch loss: 0.8676744699478149
Total epoch: 54. epoch loss: 0.8493233919143677
Total epoch: 55. epoch loss: 0.8319477438926697
Total epoch: 56. epoch loss: 0.8154617547988892
Total epoch: 57. epoch loss: 0.7997970581054688
Total epoch: 58. epoch loss: 0.7848851680755615
Total epoch: 59. epoch loss: 0.7706676125526428
Total epoch: 60. epoch loss: 0.7570911645889282
Total epoch: 61. epoch loss: 0.7441039681434631
Total epoch: 62. epoch loss: 0.7316708564758301
Total epoch: 63. epoch loss: 0.7197469472885132
Total epoch: 64. epoch loss: 0.7083025574684143
Total epoch: 65. epoch loss: 0.6973012685775757
Total epoch: 66. epoch loss: 0.6867229342460632
Total epoch: 67. epoch loss: 0.6765385866165161
Total epoch: 68. epoch loss: 0.6667247414588928
Total epoch: 69. epoch loss: 0.6572556495666504
Total epoch: 70. epoch loss: 0.6481165885925293
Total epoch: 71. epoch loss: 0.6392841935157776
Total epoch: 72. epoch loss: 0.6307456493377686
Total epoch: 73. epoch loss: 0.62247633934021
Total epoch: 74. epoch loss: 0.6144713759422302
Total epoch: 75. epoch loss: 0.6067085266113281
Total epoch: 76. epoch loss: 0.5991760492324829
Total epoch: 77. epoch loss: 0.5918665528297424
Total epoch: 78. epoch loss: 0.5847622156143188
Total epoch: 79. epoch loss: 0.577855110168457
Total epoch: 80. epoch loss: 0.5711421966552734
Total epoch: 81. epoch loss: 0.5646045207977295
Total epoch: 82. epoch loss: 0.5582408308982849
Total epoch: 83. epoch loss: 0.5520409345626831
Total epoch: 84. epoch loss: 0.545997142791748
Total epoch: 85. epoch loss: 0.5401065945625305
Total epoch: 86. epoch loss: 0.5343600511550903
Total epoch: 87. epoch loss: 0.5287484526634216
Total epoch: 88. epoch loss: 0.523270308971405
Total epoch: 89. epoch loss: 0.5179219841957092
Total epoch: 90. epoch loss: 0.5126919150352478
Total epoch: 91. epoch loss: 0.5075848698616028
Total epoch: 92. epoch loss: 0.5025840997695923
Total epoch: 93. epoch loss: 0.49769529700279236
Total epoch: 94. epoch loss: 0.4929113984107971
Total epoch: 95. epoch loss: 0.4882265627384186
Total epoch: 96. epoch loss: 0.4836389422416687
Total epoch: 97. epoch loss: 0.4791475236415863
Total epoch: 98. epoch loss: 0.4747433066368103
Total epoch: 99. epoch loss: 0.4704287648200989
Total epoch: 99. DecT loss: 0.4704287648200989
Training time: 0.5862233638763428
APL_precision: 0.3346303501945525, APL_recall: 0.5058823529411764, APL_f1: 0.4028103044496487, APL_number: 170
CMT_precision: 0.26865671641791045, CMT_recall: 0.46153846153846156, CMT_f1: 0.33962264150943394, CMT_number: 195
DSC_precision: 0.5168195718654435, DSC_recall: 0.38672768878718533, DSC_f1: 0.4424083769633508, DSC_number: 437
MAT_precision: 0.5387700534759359, MAT_recall: 0.5909090909090909, MAT_f1: 0.5636363636363636, MAT_number: 682
PRO_precision: 0.32127351664254705, PRO_recall: 0.28793774319066145, PRO_f1: 0.3036935704514364, PRO_number: 771
SMT_precision: 0.284, SMT_recall: 0.4152046783625731, SMT_f1: 0.33729216152019, SMT_number: 171
SPL_precision: 0.3191489361702128, SPL_recall: 0.4, SPL_f1: 0.3550295857988166, SPL_number: 75
overall_precision: 0.3963730569948187, overall_recall: 0.42822870851659334, overall_f1: 0.4116855660196041, overall_accuracy: 0.8051604602959045
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]