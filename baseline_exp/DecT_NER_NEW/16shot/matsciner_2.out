/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:44:27 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:44:28 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-545026288f6cba7d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1052.13it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/46 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4789.31 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:44:35 - INFO - __main__ - ***** Running training *****
05/31/2023 13:44:35 - INFO - __main__ -   Num examples = 46
05/31/2023 13:44:35 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:44:35 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:44:35 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:44:35 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:44:35 - INFO - __main__ -   Total optimization steps = 200
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.868157386779785
Total epoch: 1. epoch loss: 15.007963180541992
Total epoch: 2. epoch loss: 14.166925430297852
Total epoch: 3. epoch loss: 13.346761703491211
Total epoch: 4. epoch loss: 12.550064086914062
Total epoch: 5. epoch loss: 11.77977466583252
Total epoch: 6. epoch loss: 11.038568496704102
Total epoch: 7. epoch loss: 10.328394889831543
Total epoch: 8. epoch loss: 9.650379180908203
Total epoch: 9. epoch loss: 9.004993438720703
Total epoch: 10. epoch loss: 8.39254093170166
Total epoch: 11. epoch loss: 7.81329870223999
Total epoch: 12. epoch loss: 7.267614364624023
Total epoch: 13. epoch loss: 6.755568981170654
Total epoch: 14. epoch loss: 6.276871681213379
Total epoch: 15. epoch loss: 5.8306803703308105
Total epoch: 16. epoch loss: 5.415762424468994
Total epoch: 17. epoch loss: 5.030529975891113
Total epoch: 18. epoch loss: 4.67328405380249
Total epoch: 19. epoch loss: 4.342235088348389
Total epoch: 20. epoch loss: 4.03559684753418
Total epoch: 21. epoch loss: 3.7516233921051025
Total epoch: 22. epoch loss: 3.4894392490386963
Total epoch: 23. epoch loss: 3.2483623027801514
Total epoch: 24. epoch loss: 3.027451276779175
Total epoch: 25. epoch loss: 2.8255600929260254
Total epoch: 26. epoch loss: 2.6414999961853027
Total epoch: 27. epoch loss: 2.4740347862243652
Total epoch: 28. epoch loss: 2.321927547454834
Total epoch: 29. epoch loss: 2.183960437774658
Total epoch: 30. epoch loss: 2.058873176574707
Total epoch: 31. epoch loss: 1.945477843284607
Total epoch: 32. epoch loss: 1.8426045179367065
Total epoch: 33. epoch loss: 1.7491302490234375
Total epoch: 34. epoch loss: 1.6640567779541016
Total epoch: 35. epoch loss: 1.5864613056182861
Total epoch: 36. epoch loss: 1.5155378580093384
Total epoch: 37. epoch loss: 1.4505623579025269
Total epoch: 38. epoch loss: 1.390950322151184
Total epoch: 39. epoch loss: 1.3361425399780273
Total epoch: 40. epoch loss: 1.2856791019439697
Total epoch: 41. epoch loss: 1.2391446828842163
Total epoch: 42. epoch loss: 1.1961818933486938
Total epoch: 43. epoch loss: 1.1564388275146484
Total epoch: 44. epoch loss: 1.119633436203003
Total epoch: 45. epoch loss: 1.0854836702346802
Total epoch: 46. epoch loss: 1.0537490844726562
Total epoch: 47. epoch loss: 1.024211049079895
Total epoch: 48. epoch loss: 0.9966559410095215
Total epoch: 49. epoch loss: 0.9709165692329407
Total epoch: 50. epoch loss: 0.9468223452568054
Total epoch: 51. epoch loss: 0.924214780330658
Total epoch: 52. epoch loss: 0.902963399887085
Total epoch: 53. epoch loss: 0.8829487562179565
Total epoch: 54. epoch loss: 0.8640512228012085
Total epoch: 55. epoch loss: 0.8461838960647583
Total epoch: 56. epoch loss: 0.8292440176010132
Total epoch: 57. epoch loss: 0.8131586909294128
Total epoch: 58. epoch loss: 0.7978495359420776
Total epoch: 59. epoch loss: 0.7832679748535156
Total epoch: 60. epoch loss: 0.7693471312522888
Total epoch: 61. epoch loss: 0.7560417652130127
Total epoch: 62. epoch loss: 0.7433016300201416
Total epoch: 63. epoch loss: 0.7310921549797058
Total epoch: 64. epoch loss: 0.7193765044212341
Total epoch: 65. epoch loss: 0.7081225514411926
Total epoch: 66. epoch loss: 0.6972984671592712
Total epoch: 67. epoch loss: 0.6868706345558167
Total epoch: 68. epoch loss: 0.676823616027832
Total epoch: 69. epoch loss: 0.6671301126480103
Total epoch: 70. epoch loss: 0.6577715277671814
Total epoch: 71. epoch loss: 0.6487234830856323
Total epoch: 72. epoch loss: 0.6399732828140259
Total epoch: 73. epoch loss: 0.6314966082572937
Total epoch: 74. epoch loss: 0.6232861280441284
Total epoch: 75. epoch loss: 0.6153286695480347
Total epoch: 76. epoch loss: 0.6076019406318665
Total epoch: 77. epoch loss: 0.6001020073890686
Total epoch: 78. epoch loss: 0.5928162336349487
Total epoch: 79. epoch loss: 0.5857312083244324
Total epoch: 80. epoch loss: 0.5788337588310242
Total epoch: 81. epoch loss: 0.572128415107727
Total epoch: 82. epoch loss: 0.5655924677848816
Total epoch: 83. epoch loss: 0.5592248439788818
Total epoch: 84. epoch loss: 0.5530220866203308
Total epoch: 85. epoch loss: 0.5469666719436646
Total epoch: 86. epoch loss: 0.5410621166229248
Total epoch: 87. epoch loss: 0.5352942943572998
Total epoch: 88. epoch loss: 0.5296627879142761
Total epoch: 89. epoch loss: 0.5241605639457703
Total epoch: 90. epoch loss: 0.5187839269638062
Total epoch: 91. epoch loss: 0.513525128364563
Total epoch: 92. epoch loss: 0.5083801746368408
Total epoch: 93. epoch loss: 0.503351092338562
Total epoch: 94. epoch loss: 0.498426228761673
Total epoch: 95. epoch loss: 0.49360015988349915
Total epoch: 96. epoch loss: 0.4888766407966614
Total epoch: 97. epoch loss: 0.48424798250198364
Total epoch: 98. epoch loss: 0.47971311211586
Total epoch: 99. epoch loss: 0.47526633739471436
Total epoch: 99. DecT loss: 0.47526633739471436
Training time: 0.5120341777801514
APL_precision: 0.3105022831050228, APL_recall: 0.4, APL_f1: 0.3496143958868895, APL_number: 170
CMT_precision: 0.2103825136612022, CMT_recall: 0.39487179487179486, CMT_f1: 0.2745098039215686, CMT_number: 195
DSC_precision: 0.40053763440860213, DSC_recall: 0.34096109839816935, DSC_f1: 0.3683559950556242, DSC_number: 437
MAT_precision: 0.588477366255144, MAT_recall: 0.6290322580645161, MAT_f1: 0.6080793763288448, MAT_number: 682
PRO_precision: 0.3989071038251366, PRO_recall: 0.3787289234760052, PRO_f1: 0.38855622089155023, PRO_number: 771
SMT_precision: 0.2967479674796748, SMT_recall: 0.4269005847953216, SMT_f1: 0.3501199040767386, SMT_number: 171
SPL_precision: 0.3645833333333333, SPL_recall: 0.4666666666666667, SPL_f1: 0.4093567251461988, SPL_number: 75
overall_precision: 0.4068840579710145, overall_recall: 0.4490203918432627, overall_f1: 0.4269150351644175, overall_accuracy: 0.8122364377099563
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:04<?, ?it/s]