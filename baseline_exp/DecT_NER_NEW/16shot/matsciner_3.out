/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:44:27 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:44:28 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-2a29c2bd2933348e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 976.56it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/36 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3411.91 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:44:34 - INFO - __main__ - ***** Running training *****
05/31/2023 13:44:34 - INFO - __main__ -   Num examples = 36
05/31/2023 13:44:34 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:44:34 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:44:34 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:44:34 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:44:34 - INFO - __main__ -   Total optimization steps = 200
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.770254135131836
Total epoch: 1. epoch loss: 14.853806495666504
Total epoch: 2. epoch loss: 13.959290504455566
Total epoch: 3. epoch loss: 13.087118148803711
Total epoch: 4. epoch loss: 12.23880386352539
Total epoch: 5. epoch loss: 11.416644096374512
Total epoch: 6. epoch loss: 10.623356819152832
Total epoch: 7. epoch loss: 9.86169719696045
Total epoch: 8. epoch loss: 9.13410758972168
Total epoch: 9. epoch loss: 8.442625999450684
Total epoch: 10. epoch loss: 7.7888898849487305
Total epoch: 11. epoch loss: 7.174272537231445
Total epoch: 12. epoch loss: 6.5996479988098145
Total epoch: 13. epoch loss: 6.065317153930664
Total epoch: 14. epoch loss: 5.570924758911133
Total epoch: 15. epoch loss: 5.115357398986816
Total epoch: 16. epoch loss: 4.696957111358643
Total epoch: 17. epoch loss: 4.313566207885742
Total epoch: 18. epoch loss: 3.9626731872558594
Total epoch: 19. epoch loss: 3.641634702682495
Total epoch: 20. epoch loss: 3.3478431701660156
Total epoch: 21. epoch loss: 3.0790464878082275
Total epoch: 22. epoch loss: 2.8343746662139893
Total epoch: 23. epoch loss: 2.6125853061676025
Total epoch: 24. epoch loss: 2.4122586250305176
Total epoch: 25. epoch loss: 2.2317655086517334
Total epoch: 26. epoch loss: 2.0695550441741943
Total epoch: 27. epoch loss: 1.9240010976791382
Total epoch: 28. epoch loss: 1.7936005592346191
Total epoch: 29. epoch loss: 1.6768956184387207
Total epoch: 30. epoch loss: 1.5725138187408447
Total epoch: 31. epoch loss: 1.4791598320007324
Total epoch: 32. epoch loss: 1.3955893516540527
Total epoch: 33. epoch loss: 1.3206719160079956
Total epoch: 34. epoch loss: 1.2533537149429321
Total epoch: 35. epoch loss: 1.192713975906372
Total epoch: 36. epoch loss: 1.1379345655441284
Total epoch: 37. epoch loss: 1.0882904529571533
Total epoch: 38. epoch loss: 1.0431731939315796
Total epoch: 39. epoch loss: 1.0020451545715332
Total epoch: 40. epoch loss: 0.9644320607185364
Total epoch: 41. epoch loss: 0.9299420118331909
Total epoch: 42. epoch loss: 0.8982202410697937
Total epoch: 43. epoch loss: 0.8689515590667725
Total epoch: 44. epoch loss: 0.8418775200843811
Total epoch: 45. epoch loss: 0.8167607188224792
Total epoch: 46. epoch loss: 0.7934070825576782
Total epoch: 47. epoch loss: 0.7716329097747803
Total epoch: 48. epoch loss: 0.7512885332107544
Total epoch: 49. epoch loss: 0.7322425842285156
Total epoch: 50. epoch loss: 0.7143759727478027
Total epoch: 51. epoch loss: 0.6975788474082947
Total epoch: 52. epoch loss: 0.6817545890808105
Total epoch: 53. epoch loss: 0.6668214797973633
Total epoch: 54. epoch loss: 0.6526990532875061
Total epoch: 55. epoch loss: 0.6393243670463562
Total epoch: 56. epoch loss: 0.6266308426856995
Total epoch: 57. epoch loss: 0.6145697236061096
Total epoch: 58. epoch loss: 0.6030851602554321
Total epoch: 59. epoch loss: 0.5921363830566406
Total epoch: 60. epoch loss: 0.5816817283630371
Total epoch: 61. epoch loss: 0.5716874599456787
Total epoch: 62. epoch loss: 0.5621191263198853
Total epoch: 63. epoch loss: 0.5529499053955078
Total epoch: 64. epoch loss: 0.5441538691520691
Total epoch: 65. epoch loss: 0.5357027649879456
Total epoch: 66. epoch loss: 0.527579128742218
Total epoch: 67. epoch loss: 0.5197620987892151
Total epoch: 68. epoch loss: 0.5122304558753967
Total epoch: 69. epoch loss: 0.5049717426300049
Total epoch: 70. epoch loss: 0.4979664385318756
Total epoch: 71. epoch loss: 0.4911983013153076
Total epoch: 72. epoch loss: 0.48465755581855774
Total epoch: 73. epoch loss: 0.47833091020584106
Total epoch: 74. epoch loss: 0.4722065031528473
Total epoch: 75. epoch loss: 0.46627169847488403
Total epoch: 76. epoch loss: 0.4605189561843872
Total epoch: 77. epoch loss: 0.45493456721305847
Total epoch: 78. epoch loss: 0.4495130181312561
Total epoch: 79. epoch loss: 0.4442470371723175
Total epoch: 80. epoch loss: 0.43912896513938904
Total epoch: 81. epoch loss: 0.43414977192878723
Total epoch: 82. epoch loss: 0.42930474877357483
Total epoch: 83. epoch loss: 0.424587219953537
Total epoch: 84. epoch loss: 0.41999226808547974
Total epoch: 85. epoch loss: 0.41551119089126587
Total epoch: 86. epoch loss: 0.41114065051078796
Total epoch: 87. epoch loss: 0.4068762958049774
Total epoch: 88. epoch loss: 0.4027169346809387
Total epoch: 89. epoch loss: 0.3986530303955078
Total epoch: 90. epoch loss: 0.39468303322792053
Total epoch: 91. epoch loss: 0.39080357551574707
Total epoch: 92. epoch loss: 0.3870120644569397
Total epoch: 93. epoch loss: 0.3833010494709015
Total epoch: 94. epoch loss: 0.37967169284820557
Total epoch: 95. epoch loss: 0.37611913681030273
Total epoch: 96. epoch loss: 0.3726412057876587
Total epoch: 97. epoch loss: 0.3692324757575989
Total epoch: 98. epoch loss: 0.36589497327804565
Total epoch: 99. epoch loss: 0.3626229763031006
Total epoch: 99. DecT loss: 0.3626229763031006
Training time: 0.5573704242706299
APL_precision: 0.2188449848024316, APL_recall: 0.4235294117647059, APL_f1: 0.28857715430861725, APL_number: 170
CMT_precision: 0.45989304812834225, CMT_recall: 0.441025641025641, CMT_f1: 0.450261780104712, CMT_number: 195
DSC_precision: 0.4434389140271493, DSC_recall: 0.448512585812357, DSC_f1: 0.4459613196814562, DSC_number: 437
MAT_precision: 0.6240126382306477, MAT_recall: 0.5791788856304986, MAT_f1: 0.6007604562737642, MAT_number: 682
PRO_precision: 0.32653061224489793, PRO_recall: 0.2697795071335927, PRO_f1: 0.2954545454545454, PRO_number: 771
SMT_precision: 0.2509090909090909, SMT_recall: 0.40350877192982454, SMT_f1: 0.3094170403587444, SMT_number: 171
SPL_precision: 0.33653846153846156, SPL_recall: 0.4666666666666667, SPL_f1: 0.3910614525139665, SPL_number: 75
overall_precision: 0.4069812044495589, overall_recall: 0.42423030787684923, overall_f1: 0.41542678151918555, overall_accuracy: 0.8128797083839611
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:03<?, ?it/s]