/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:43:38 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:43:39 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-9dfab3336ef43c32/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:4, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 32
model_logits_weight: 20
num_train_epochs: 35
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1104.20it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/13 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5228.61 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:43:45 - INFO - __main__ - ***** Running training *****
05/31/2023 13:43:45 - INFO - __main__ -   Num examples = 13
05/31/2023 13:43:45 - INFO - __main__ -   Num Epochs = 35
05/31/2023 13:43:45 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:43:45 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:43:45 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:43:45 - INFO - __main__ -   Total optimization steps = 35
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 14.109230995178223
Total epoch: 1. epoch loss: 13.495826721191406
Total epoch: 2. epoch loss: 12.894896507263184
Total epoch: 3. epoch loss: 12.30633544921875
Total epoch: 4. epoch loss: 11.730124473571777
Total epoch: 5. epoch loss: 11.166410446166992
Total epoch: 6. epoch loss: 10.6156587600708
Total epoch: 7. epoch loss: 10.078692436218262
Total epoch: 8. epoch loss: 9.556376457214355
Total epoch: 9. epoch loss: 9.049726486206055
Total epoch: 10. epoch loss: 8.559629440307617
Total epoch: 11. epoch loss: 8.086931228637695
Total epoch: 12. epoch loss: 7.632177829742432
Total epoch: 13. epoch loss: 7.195772171020508
Total epoch: 14. epoch loss: 6.7777605056762695
Total epoch: 15. epoch loss: 6.378122329711914
Total epoch: 16. epoch loss: 5.996553421020508
Total epoch: 17. epoch loss: 5.632664680480957
Total epoch: 18. epoch loss: 5.286001205444336
Total epoch: 19. epoch loss: 4.956010341644287
Total epoch: 20. epoch loss: 4.642279624938965
Total epoch: 21. epoch loss: 4.344357013702393
Total epoch: 22. epoch loss: 4.061790466308594
Total epoch: 23. epoch loss: 3.7942380905151367
Total epoch: 24. epoch loss: 3.5413570404052734
Total epoch: 25. epoch loss: 3.30281662940979
Total epoch: 26. epoch loss: 3.0783402919769287
Total epoch: 27. epoch loss: 2.8675081729888916
Total epoch: 28. epoch loss: 2.6699397563934326
Total epoch: 29. epoch loss: 2.4852070808410645
Total epoch: 30. epoch loss: 2.312779664993286
Total epoch: 31. epoch loss: 2.152066230773926
Total epoch: 32. epoch loss: 2.002490520477295
Total epoch: 33. epoch loss: 1.8636749982833862
Total epoch: 34. epoch loss: 1.7355055809020996
Total epoch: 34. DecT loss: 1.7355055809020996
Training time: 0.20322203636169434
APL_precision: 0.23737373737373738, APL_recall: 0.27647058823529413, APL_f1: 0.2554347826086957, APL_number: 170
CMT_precision: 0.21180555555555555, CMT_recall: 0.3128205128205128, CMT_f1: 0.2525879917184265, CMT_number: 195
DSC_precision: 0.3918918918918919, DSC_recall: 0.19908466819221968, DSC_f1: 0.26403641881638845, DSC_number: 437
MAT_precision: 0.5026737967914439, MAT_recall: 0.5513196480938416, MAT_f1: 0.5258741258741259, MAT_number: 682
PRO_precision: 0.32047477744807124, PRO_recall: 0.14007782101167315, PRO_f1: 0.19494584837545126, PRO_number: 771
SMT_precision: 0.20408163265306123, SMT_recall: 0.05847953216374269, SMT_f1: 0.09090909090909091, SMT_number: 171
SPL_precision: 0.2619047619047619, SPL_recall: 0.14666666666666667, SPL_f1: 0.18803418803418803, SPL_number: 75
overall_precision: 0.37154989384288745, overall_recall: 0.2798880447820872, overall_f1: 0.31927023945267957, overall_accuracy: 0.7574154813808878
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:02<?, ?it/s]