Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
14 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10898303985595703
loss:2.234545946121216
training_time:0.07043337821960449
loss:0.8906660676002502
training_time:0.06255507469177246
loss:0.681269645690918
training_time:0.06185579299926758
loss:0.47248491644859314
{'APL': 0.0, 'CMT': 5.076142131979695, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 5.555555555555556, 'macro_f1': 0.015188139553621787, 'micro_f1': 0.0060111635895234005}
training_time:0.056606292724609375
loss:0.27227991819381714
{'APL': 3.4482758620689653, 'CMT': 24.16918429003021, 'DSC': 1.937046004842615, 'MAT': 0.6389776357827477, 'PRO': 7.737397420867527, 'SMT': 9.876543209876544, 'SPL': 26.804123711340207, 'macro_f1': 0.10658792590686973, 'micro_f1': 0.07756024096385541}
training_time:0.07134819030761719
loss:0.19192613661289215
{'APL': 12.154696132596683, 'CMT': 21.453287197231834, 'DSC': 1.4527845036319613, 'MAT': 3.442879499217528, 'PRO': 4.662576687116564, 'SMT': 12.121212121212121, 'SPL': 25.26315789473684, 'macro_f1': 0.11507227719391931, 'micro_f1': 0.07470157874470544}
training_time:0.07226896286010742
loss:0.09286311268806458
training_time:0.06135272979736328
loss:0.046642038971185684
{'APL': 29.441624365482234, 'CMT': 32.6530612244898, 'DSC': 1.4388489208633095, 'MAT': 9.09090909090909, 'PRO': 4.161248374512353, 'SMT': 18.713450292397663, 'SPL': 25.882352941176467, 'macro_f1': 0.1734021360140442, 'micro_f1': 0.1180100269957578}
training_time:0.07239246368408203
loss:0.0201927088201046
{'APL': 34.862385321100916, 'CMT': 45.6973293768546, 'DSC': 5.011389521640091, 'MAT': 14.847161572052403, 'PRO': 7.798742138364781, 'SMT': 20.33898305084746, 'SPL': 26.96629213483146, 'macro_f1': 0.22217469016527389, 'micro_f1': 0.17359591539022615}
training_time:0.08232235908508301
loss:0.012974361889064312
{'APL': 38.333333333333336, 'CMT': 51.098901098901095, 'DSC': 7.126948775055679, 'MAT': 22.972972972972972, 'PRO': 13.333333333333334, 'SMT': 25.0, 'SPL': 33.684210526315795, 'macro_f1': 0.273642428628446, 'micro_f1': 0.23058336209872282}
training_time:0.07071971893310547
loss:0.010798449628055096
{'APL': 39.39393939393939, 'CMT': 51.30890052356021, 'DSC': 7.4074074074074066, 'MAT': 31.880448318804483, 'PRO': 17.136150234741784, 'SMT': 27.55102040816326, 'SPL': 33.33333333333333, 'macro_f1': 0.2971588565999284, 'micro_f1': 0.26933158584534733}
training_time:0.06596159934997559
loss:0.005267176777124405
{'APL': 40.28776978417267, 'CMT': 51.04166666666667, 'DSC': 9.623430962343097, 'MAT': 36.038186157517906, 'PRO': 19.54022988505747, 'SMT': 27.450980392156865, 'SPL': 32.6530612244898, 'macro_f1': 0.3094790358177207, 'micro_f1': 0.29015873015873017}
training_time:0.07169485092163086
loss:0.003904023440554738
{'APL': 41.13475177304964, 'CMT': 51.41388174807198, 'DSC': 11.08829568788501, 'MAT': 41.14285714285714, 'PRO': 20.88535754824063, 'SMT': 27.450980392156865, 'SPL': 32.32323232323232, 'macro_f1': 0.32205622373641934, 'micro_f1': 0.3114703139571029}
training_time:0.07230687141418457
loss:0.0027703389059752226
{'APL': 41.13475177304964, 'CMT': 52.219321148825074, 'DSC': 9.426229508196721, 'MAT': 42.792792792792795, 'PRO': 21.493212669683263, 'SMT': 27.184466019417474, 'SPL': 34.34343434343434, 'macro_f1': 0.3265631546505705, 'micro_f1': 0.31640866873065016}
training_time:0.07242894172668457
loss:0.00232766754925251
training_time:0.061544179916381836
loss:0.0016319577116519213
training_time:0.05923175811767578
loss:0.0012036558473482728
training_time:0.05930352210998535
loss:0.0011194885009899735
training_time:0.06402468681335449
loss:0.0007718864362686872
training_time:0.05906224250793457
loss:0.0007137295906431973
training_time:0.06058073043823242
loss:0.0006067143986001611
training_time:0.05947065353393555
loss:0.0007200677064247429
training_time:0.057641029357910156
loss:0.03588669374585152
training_time:0.06354236602783203
loss:0.0005335749010555446
training_time:0.06568455696105957
loss:0.0005043298588134348
training_time:0.06242203712463379
loss:0.0011296776356175542
training_time:0.05958843231201172
loss:0.0352783203125
training_time:0.06726264953613281
loss:0.0005346841644495726
training_time:0.0566105842590332
loss:0.00046596460742875934
training_time:0.057929277420043945
loss:0.0005474666249938309
{'APL': 40.26402640264026, 'CMT': 50.724637681159415, 'DSC': 8.812260536398467, 'MAT': 43.22647362978284, 'PRO': 20.851063829787233, 'SMT': 27.64227642276423, 'SPL': 35.84905660377359, 'macro_f1': 0.3248139930090086, 'micro_f1': 0.31389365351629506}
