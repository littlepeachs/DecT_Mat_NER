Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
13 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.11866140365600586
loss:2.220811367034912
training_time:0.0671839714050293
loss:0.8955954909324646
training_time:0.05896711349487305
loss:0.6883220672607422
training_time:0.05869007110595703
loss:0.3919800817966461
{'APL': 15.789473684210527, 'CMT': 16.83168316831683, 'DSC': 2.3255813953488373, 'MAT': 5.230769230769231, 'PRO': 6.666666666666667, 'SMT': 4.9261083743842375, 'SPL': 22.988505747126435, 'macro_f1': 0.10679826895260394, 'micro_f1': 0.08488063660477453}
training_time:0.07378530502319336
loss:0.25932881236076355
{'APL': 20.8, 'CMT': 18.237082066869302, 'DSC': 10.021786492374728, 'MAT': 10.432190760059614, 'PRO': 7.59493670886076, 'SMT': 2.3121387283236996, 'SPL': 26.666666666666668, 'macro_f1': 0.13723543060450683, 'micro_f1': 0.11334037310806055}
training_time:0.056403160095214844
loss:0.12256006896495819
{'APL': 25.112107623318387, 'CMT': 15.333333333333332, 'DSC': 14.196242171189976, 'MAT': 21.86234817813765, 'PRO': 7.424593967517403, 'SMT': 7.142857142857142, 'SPL': 32.608695652173914, 'macro_f1': 0.17668596866932543, 'micro_f1': 0.15287958115183245}
training_time:0.053444862365722656
loss:0.0650177150964737
{'APL': 38.56209150326797, 'CMT': 33.86243386243387, 'DSC': 27.835051546391753, 'MAT': 39.65141612200435, 'PRO': 18.640776699029125, 'SMT': 20.5761316872428, 'SPL': 46.15384615384615, 'macro_f1': 0.32183106796316574, 'micro_f1': 0.2988248461108002}
training_time:0.06571626663208008
loss:0.031328048557043076
{'APL': 39.57219251336899, 'CMT': 37.40259740259741, 'DSC': 32.16080402010051, 'MAT': 40.57017543859649, 'PRO': 22.825070159027128, 'SMT': 25.306122448979597, 'SPL': 49.586776859504134, 'macro_f1': 0.35346248406024894, 'micro_f1': 0.3294625978935997}
training_time:0.0590519905090332
loss:0.014947418123483658
training_time:0.058090925216674805
loss:0.009699348360300064
training_time:0.06550264358520508
loss:0.005274879280477762
training_time:0.0655364990234375
loss:0.0034473873674869537
training_time:0.06165647506713867
loss:0.002292315009981394
training_time:0.0636444091796875
loss:0.0016875205328688025
training_time:0.05933785438537598
loss:0.0012794694630429149
training_time:0.06403851509094238
loss:0.0012721592793241143
training_time:0.05971217155456543
loss:0.0008880788227543235
training_time:0.05873584747314453
loss:0.0008404208347201347
training_time:0.062357187271118164
loss:0.0006339244428090751
training_time:0.057025909423828125
loss:0.0005805733962915838
training_time:0.0549771785736084
loss:0.0006531935650855303
training_time:0.059448957443237305
loss:0.00047515201731584966
training_time:0.07673192024230957
loss:0.00043789276969619095
training_time:0.06121420860290527
loss:0.026193175464868546
training_time:0.06255269050598145
loss:0.00044591439655050635
training_time:0.05907297134399414
loss:0.0004637169186025858
training_time:0.06720900535583496
loss:0.00037330033956095576
{'APL': 33.33333333333333, 'CMT': 50.61425061425061, 'DSC': 36.90685413005273, 'MAT': 38.592508513053346, 'PRO': 21.435594886922317, 'SMT': 24.347826086956523, 'SPL': 43.103448275862064, 'macro_f1': 0.3547625940577585, 'micro_f1': 0.3351322180248246}
training_time:0.06540775299072266
loss:0.017501328140497208
{'APL': 33.68421052631579, 'CMT': 50.74626865671642, 'DSC': 36.84210526315789, 'MAT': 39.1845979614949, 'PRO': 21.104536489151872, 'SMT': 24.137931034482758, 'SPL': 43.103448275862064, 'macro_f1': 0.35543299743883106, 'micro_f1': 0.3358613217768147}
training_time:0.0650937557220459
loss:0.00040332929347641766
{'APL': 33.760683760683754, 'CMT': 50.74626865671642, 'DSC': 37.06293706293707, 'MAT': 39.59276018099548, 'PRO': 20.969337289812067, 'SMT': 24.45414847161572, 'SPL': 44.44444444444445, 'macro_f1': 0.3586151140960071, 'micro_f1': 0.33776812381210974}
training_time:0.06577634811401367
loss:0.00040382094448432326
{'APL': 34.06813627254509, 'CMT': 50.69124423963134, 'DSC': 37.908496732026144, 'MAT': 38.99895724713243, 'PRO': 20.715630885122412, 'SMT': 25.54744525547445, 'SPL': 46.03174603174603, 'macro_f1': 0.362802366662397, 'micro_f1': 0.3388804841149773}
