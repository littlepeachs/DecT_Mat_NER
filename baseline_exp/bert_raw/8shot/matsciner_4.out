Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
21 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.12130451202392578
loss:2.1778059005737305
training_time:0.15579771995544434
loss:1.2523994445800781
training_time:0.058442115783691406
loss:0.8844709992408752
{'APL': 0.0, 'CMT': 2.7906976744186047, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.00398671096345515, 'micro_f1': 0.00253592561284869}
training_time:0.07171344757080078
loss:0.6234897375106812
{'APL': 0.0, 'CMT': 2.3622047244094486, 'DSC': 0.0, 'MAT': 0.3215434083601286, 'PRO': 3.6452004860267313, 'SMT': 1.1560693641618498, 'SPL': 8.21917808219178, 'macro_f1': 0.022434565807357053, 'micro_f1': 0.018297533810660304}
training_time:0.07370209693908691
loss:0.3948417007923126
{'APL': 2.6200873362445414, 'CMT': 18.536585365853657, 'DSC': 1.923076923076923, 'MAT': 5.555555555555555, 'PRO': 24.423569598633648, 'SMT': 15.141955835962145, 'SPL': 38.333333333333336, 'macro_f1': 0.1521916627837997, 'micro_f1': 0.15282392026578073}
training_time:0.06576919555664062
loss:0.22982199490070343
{'APL': 11.194029850746269, 'CMT': 28.703703703703702, 'DSC': 3.6613272311212817, 'MAT': 25.296442687747035, 'PRO': 24.675324675324674, 'SMT': 17.14285714285714, 'SPL': 54.285714285714285, 'macro_f1': 0.23565628511030629, 'micro_f1': 0.22006287510717348}
training_time:0.07652711868286133
loss:0.1488073319196701
{'APL': 19.047619047619047, 'CMT': 35.83535108958838, 'DSC': 8.56531049250535, 'MAT': 42.437431991294886, 'PRO': 27.516158818097875, 'SMT': 23.728813559322035, 'SPL': 51.5625, 'macro_f1': 0.2981331214263251, 'micro_f1': 0.29763560500695413}
training_time:0.06326842308044434
loss:0.08463548868894577
{'APL': 34.22818791946309, 'CMT': 48.24561403508772, 'DSC': 20.105820105820104, 'MAT': 54.03899721448469, 'PRO': 39.865996649916255, 'SMT': 27.956989247311824, 'SPL': 52.941176470588225, 'macro_f1': 0.3962611166323884, 'micro_f1': 0.404483663248271}
training_time:0.06447196006774902
loss:0.03836596757173538
{'APL': 39.08794788273616, 'CMT': 50.917431192660544, 'DSC': 24.545454545454547, 'MAT': 56.414922656960876, 'PRO': 43.78600823045267, 'SMT': 29.29936305732484, 'SPL': 48.888888888888886, 'macro_f1': 0.4184857377921121, 'micro_f1': 0.43025676613462877}
training_time:0.07221484184265137
loss:0.026707204058766365
{'APL': 37.85714285714286, 'CMT': 56.42317380352645, 'DSC': 24.71590909090909, 'MAT': 54.11985018726592, 'PRO': 43.898305084745765, 'SMT': 32.304038004750595, 'SPL': 50.74626865671641, 'macro_f1': 0.4286638395500815, 'micro_f1': 0.4311663479923518}
training_time:0.07791686058044434
loss:0.018074188381433487
{'APL': 41.35338345864662, 'CMT': 55.81395348837208, 'DSC': 26.3764404609475, 'MAT': 52.99625468164793, 'PRO': 47.793505412156534, 'SMT': 35.15439429928742, 'SPL': 50.0, 'macro_f1': 0.4421256168586543, 'micro_f1': 0.4431924882629108}
training_time:0.06742715835571289
loss:0.008941158652305603
{'APL': 43.678160919540225, 'CMT': 54.83028720626631, 'DSC': 27.370948379351734, 'MAT': 52.34521575984991, 'PRO': 49.793899422918386, 'SMT': 38.141809290953546, 'SPL': 49.635036496350374, 'macro_f1': 0.45113622496461503, 'micro_f1': 0.4504881450488145}
training_time:0.09328341484069824
loss:0.005630180239677429
{'APL': 44.1860465116279, 'CMT': 56.08465608465608, 'DSC': 27.981651376146786, 'MAT': 52.64150943396226, 'PRO': 50.40916530278232, 'SMT': 39.01234567901234, 'SPL': 50.0, 'macro_f1': 0.4575933919831253, 'micro_f1': 0.4548603093973678}
training_time:0.07377958297729492
loss:0.005391913000494242
training_time:0.07964777946472168
loss:0.0029807824175804853
training_time:0.05515766143798828
loss:0.00313433725386858
training_time:0.06062769889831543
loss:0.0017612611409276724
training_time:0.06070423126220703
loss:0.0016048562247306108
training_time:0.06444430351257324
loss:0.0013716798275709152
training_time:0.061363935470581055
loss:0.0013602719409391284
training_time:0.06191277503967285
loss:0.0019045104272663593
training_time:0.06076765060424805
loss:0.018428325653076172
training_time:0.0620884895324707
loss:0.0007376970024779439
training_time:0.0633852481842041
loss:0.0007424268405884504
{'APL': 41.1764705882353, 'CMT': 54.83870967741935, 'DSC': 27.7027027027027, 'MAT': 55.84415584415584, 'PRO': 46.58901830282862, 'SMT': 41.4985590778098, 'SPL': 56.05095541401274, 'macro_f1': 0.4624293880102347, 'micro_f1': 0.4535263895375992}
training_time:0.07297730445861816
loss:0.009317051619291306
training_time:0.07242631912231445
loss:0.0007302977610379457
training_time:0.0590357780456543
loss:0.0006544283824041486
training_time:0.0724802017211914
loss:0.000696235045325011
training_time:0.10103297233581543
loss:0.000698913587257266
training_time:0.07444429397583008
loss:0.0006966445944271982
{'APL': 39.04382470119522, 'CMT': 54.32098765432099, 'DSC': 28.270042194092827, 'MAT': 56.31399317406144, 'PRO': 45.83987441130298, 'SMT': 45.56962025316456, 'SPL': 57.48502994011976, 'macro_f1': 0.4669191033260825, 'micro_f1': 0.45663486556808325}
