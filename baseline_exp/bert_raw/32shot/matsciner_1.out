Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
64 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.5096988677978516
loss:1.2171543836593628
training_time:0.24543070793151855
loss:0.653211772441864
{'APL': 20.074349442379184, 'CMT': 25.984251968503933, 'DSC': 0.0, 'MAT': 17.43869209809264, 'PRO': 29.15782024062279, 'SMT': 21.978021978021978, 'SPL': 0.0, 'macro_f1': 0.1637616224680293, 'micro_f1': 0.20952896589063347}
training_time:0.23920798301696777
loss:0.3695961833000183
{'APL': 41.02564102564102, 'CMT': 61.455525606468996, 'DSC': 32.83082077051927, 'MAT': 59.07859078590787, 'PRO': 40.16949152542372, 'SMT': 42.33983286908078, 'SPL': 20.0, 'macro_f1': 0.4241427179757738, 'micro_f1': 0.46033300685602346}
training_time:0.2465512752532959
loss:0.21059532463550568
{'APL': 50.783699059561116, 'CMT': 68.73239436619718, 'DSC': 50.457516339869265, 'MAT': 67.2311600338696, 'PRO': 55.85725368502715, 'SMT': 56.66666666666668, 'SPL': 52.727272727272734, 'macro_f1': 0.5749370898263768, 'micro_f1': 0.5864352591915962}
training_time:0.23358535766601562
loss:0.06308162957429886
{'APL': 50.0, 'CMT': 68.18181818181817, 'DSC': 54.56852791878172, 'MAT': 67.71929824561404, 'PRO': 58.64197530864198, 'SMT': 65.16129032258064, 'SPL': 64.12213740458014, 'macro_f1': 0.6119929248314525, 'micro_f1': 0.6110725040537409}
training_time:0.2607591152191162
loss:0.014137943275272846
{'APL': 46.884272997032646, 'CMT': 71.03825136612024, 'DSC': 60.25492468134415, 'MAT': 69.6160267111853, 'PRO': 61.946902654867266, 'SMT': 64.61538461538461, 'SPL': 66.19718309859155, 'macro_f1': 0.6293613516064654, 'micro_f1': 0.6357096141268803}
training_time:0.2390141487121582
loss:0.00916960183531046
{'APL': 50.46728971962617, 'CMT': 70.25641025641025, 'DSC': 61.29753914988814, 'MAT': 71.54340836012862, 'PRO': 61.19969627942293, 'SMT': 65.2694610778443, 'SPL': 65.24822695035462, 'macro_f1': 0.6361171882766786, 'micro_f1': 0.6442577030812325}
training_time:0.2535712718963623
loss:0.0026662200689315796
training_time:0.22892284393310547
loss:0.0019692368805408478
training_time:0.22374534606933594
loss:0.004547696094959974
training_time:0.23973560333251953
loss:0.0027949458453804255
training_time:0.23942947387695312
loss:0.0007768260547891259
{'APL': 55.17241379310346, 'CMT': 73.71273712737127, 'DSC': 61.368653421633546, 'MAT': 70.28704422032584, 'PRO': 63.54716981132075, 'SMT': 66.66666666666666, 'SPL': 62.16216216216216, 'macro_f1': 0.6470240674322625, 'micro_f1': 0.6544592847910384}
training_time:0.2375624179840088
loss:0.0004072057199664414
training_time:0.23425507545471191
loss:0.0006384537555277348
{'APL': 57.668711656441715, 'CMT': 72.22222222222223, 'DSC': 64.74820143884892, 'MAT': 72.41647241647242, 'PRO': 62.904439428141465, 'SMT': 66.2420382165605, 'SPL': 60.40268456375839, 'macro_f1': 0.652292528489208, 'micro_f1': 0.6640574037834311}
training_time:0.24434185028076172
loss:0.0003537047014106065
training_time:0.24050021171569824
loss:0.0003341145347803831
training_time:0.2253098487854004
loss:0.008335872553288937
training_time:0.23148560523986816
loss:0.0007772152312099934
training_time:0.23810195922851562
loss:0.00030138445436023176
training_time:0.22155070304870605
loss:0.0011989031918346882
training_time:0.2290053367614746
loss:0.0004054537566844374
training_time:0.22972559928894043
loss:0.00042293776641599834
training_time:0.2461397647857666
loss:0.0004990405286662281
training_time:0.24483013153076172
loss:0.0010337383719161153
training_time:0.23209714889526367
loss:0.001400084001943469
training_time:0.2383260726928711
loss:0.003932527266442776
training_time:0.2283787727355957
loss:0.00035312114050611854
training_time:0.2381439208984375
loss:0.0002633906260598451
training_time:0.23964142799377441
loss:0.0003880950971506536
training_time:0.24695825576782227
loss:0.00032836158061400056
{'APL': 59.02578796561605, 'CMT': 72.02072538860104, 'DSC': 64.87695749440715, 'MAT': 73.21937321937322, 'PRO': 63.31444759206799, 'SMT': 68.85245901639345, 'SPL': 60.75949367088608, 'macro_f1': 0.6600989204962072, 'micro_f1': 0.6709599517005433}
