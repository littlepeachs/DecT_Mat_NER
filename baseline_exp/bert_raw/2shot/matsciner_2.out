Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
7 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.1406710147857666
loss:2.1782219409942627
training_time:0.07882928848266602
loss:0.7521656155586243
training_time:0.06665873527526855
loss:0.671373724937439
training_time:0.06689739227294922
loss:0.456341415643692
{'APL': 0.0, 'CMT': 18.45386533665835, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.026362664766654786, 'micro_f1': 0.029214370311883137}
training_time:0.0794062614440918
loss:0.3532232940196991
training_time:0.06120610237121582
loss:0.21344266831874847
training_time:0.06161212921142578
loss:0.15218585729599
{'APL': 11.111111111111112, 'CMT': 3.389830508474576, 'DSC': 0.0, 'MAT': 1.9169329073482428, 'PRO': 0.0, 'SMT': 3.614457831325301, 'SPL': 2.8169014084507045, 'macro_f1': 0.03264176252387134, 'micro_f1': 0.019842910293509715}
training_time:0.06473183631896973
loss:0.08453786373138428
{'APL': 20.84942084942085, 'CMT': 5.533596837944664, 'DSC': 11.81818181818182, 'MAT': 5.598755832037325, 'PRO': 0.0, 'SMT': 7.729468599033816, 'SPL': 7.894736842105264, 'macro_f1': 0.08489165825531965, 'micro_f1': 0.06817311374952126}
training_time:0.06064581871032715
loss:0.03548223897814751
{'APL': 23.30827067669173, 'CMT': 5.7377049180327875, 'DSC': 23.892100192678225, 'MAT': 13.733905579399142, 'PRO': 0.0, 'SMT': 7.339449541284402, 'SPL': 9.411764705882353, 'macro_f1': 0.11917599373424091, 'micro_f1': 0.11577424023154846}
training_time:0.06603789329528809
loss:0.03186997026205063
training_time:0.06198620796203613
loss:0.015888873487710953
training_time:0.06147599220275879
loss:0.0172504261136055
{'APL': 21.238938053097346, 'CMT': 7.5, 'DSC': 31.563421828908556, 'MAT': 17.5, 'PRO': 0.0, 'SMT': 7.766990291262137, 'SPL': 8.988764044943819, 'macro_f1': 0.13508302031173122, 'micro_f1': 0.14863463532665053}
training_time:0.06178426742553711
loss:0.007047672290354967
{'APL': 20.168067226890756, 'CMT': 10.196078431372548, 'DSC': 33.65253077975376, 'MAT': 22.902796271637815, 'PRO': 0.0, 'SMT': 7.725321888412018, 'SPL': 9.09090909090909, 'macro_f1': 0.14819386241282284, 'micro_f1': 0.1705630556470201}
training_time:0.06171536445617676
loss:0.004765448626130819
{'APL': 19.841269841269842, 'CMT': 11.363636363636363, 'DSC': 35.204081632653065, 'MAT': 25.78125, 'PRO': 0.0, 'SMT': 7.782101167315175, 'SPL': 8.51063829787234, 'macro_f1': 0.1549756818610668, 'micro_f1': 0.18388625592417063}
training_time:0.06654524803161621
loss:0.0048815165646374226
{'APL': 19.841269841269842, 'CMT': 11.940298507462686, 'DSC': 34.93234932349324, 'MAT': 26.09819121447029, 'PRO': 0.2656042496679947, 'SMT': 7.380073800738007, 'SPL': 8.51063829787234, 'macro_f1': 0.1556691789071063, 'micro_f1': 0.18542635658914727}
training_time:0.07706427574157715
loss:0.0028807173948735
{'APL': 20.155038759689923, 'CMT': 12.40875912408759, 'DSC': 35.26570048309179, 'MAT': 26.030927835051543, 'PRO': 0.5291005291005291, 'SMT': 7.246376811594201, 'SPL': 8.421052631578949, 'macro_f1': 0.15722422310599218, 'micro_f1': 0.18755746245786087}
training_time:0.06405186653137207
loss:0.0034473054111003876
training_time:0.07608294486999512
loss:0.002336871810257435
{'APL': 19.696969696969695, 'CMT': 13.380281690140844, 'DSC': 36.077481840193705, 'MAT': 26.666666666666668, 'PRO': 0.5249343832020997, 'SMT': 7.773851590106007, 'SPL': 6.382978723404255, 'macro_f1': 0.15786166370097607, 'micro_f1': 0.19070756149407836}
training_time:0.05994296073913574
loss:0.0017037810757756233
training_time:0.059018611907958984
loss:0.001250332803465426
{'APL': 19.84732824427481, 'CMT': 15.492957746478872, 'DSC': 36.69950738916256, 'MAT': 25.930680359435172, 'PRO': 0.5249343832020997, 'SMT': 7.829181494661921, 'SPL': 6.382978723404255, 'macro_f1': 0.161010811915171, 'micro_f1': 0.1918142944410507}
training_time:0.06048083305358887
loss:0.0013509922428056598
training_time:0.06159329414367676
loss:0.0012347082374617457
training_time:0.06184887886047363
loss:0.0010321546578779817
training_time:0.06208467483520508
loss:0.0010675295488908887
training_time:0.06434297561645508
loss:0.001042477786540985
training_time:0.06157279014587402
loss:0.0010483359219506383
training_time:0.062181949615478516
loss:0.0007698729168623686
training_time:0.06470274925231934
loss:0.0009176786406897008
training_time:0.06525468826293945
loss:0.0008923322893679142
training_time:0.06021547317504883
loss:0.0007719613495282829
{'APL': 18.63799283154122, 'CMT': 14.332247557003258, 'DSC': 36.44646924829157, 'MAT': 26.260257913247365, 'PRO': 0.49813200498131993, 'SMT': 8.641975308641975, 'SPL': 6.0606060606060606, 'macro_f1': 0.15839668703473253, 'micro_f1': 0.19136325148179506}
