Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
6 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.20024681091308594
loss:2.292529344558716
training_time:0.06544089317321777
loss:0.9586035013198853
training_time:0.06037569046020508
loss:0.5910921096801758
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 4.415584415584416, 'SMT': 1.0526315789473684, 'SPL': 0.0, 'macro_f1': 0.0078117371350454054, 'micro_f1': 0.014510278113663845}
training_time:0.07478690147399902
loss:0.29193541407585144
{'APL': 2.2988505747126435, 'CMT': 0.0, 'DSC': 4.47427293064877, 'MAT': 8.641975308641975, 'PRO': 11.54299175500589, 'SMT': 3.041825095057034, 'SPL': 14.432989690721648, 'macro_f1': 0.06347557907826852, 'micro_f1': 0.07326804497642367}
training_time:0.057843923568725586
loss:0.11709865182638168
{'APL': 8.24742268041237, 'CMT': 1.0256410256410258, 'DSC': 9.657947686116701, 'MAT': 23.803009575923394, 'PRO': 11.98589894242068, 'SMT': 3.791469194312796, 'SPL': 31.48148148148148, 'macro_f1': 0.12856124369472635, 'micro_f1': 0.13592233009708737}
training_time:0.07728195190429688
loss:0.0672888457775116
{'APL': 10.294117647058822, 'CMT': 2.061855670103093, 'DSC': 14.079422382671481, 'MAT': 34.77188655980271, 'PRO': 11.636363636363637, 'SMT': 4.324324324324325, 'SPL': 31.775700934579444, 'macro_f1': 0.15563381593557646, 'micro_f1': 0.17978290366350066}
training_time:0.07791376113891602
loss:0.02222760207951069
{'APL': 12.318840579710146, 'CMT': 1.0, 'DSC': 15.533980582524274, 'MAT': 41.52249134948098, 'PRO': 12.009803921568626, 'SMT': 6.486486486486487, 'SPL': 30.630630630630627, 'macro_f1': 0.1707174765005731, 'micro_f1': 0.20696387894565568}
training_time:0.07628822326660156
loss:0.014171949587762356
{'APL': 16.363636363636367, 'CMT': 6.511627906976745, 'DSC': 20.609579100145137, 'MAT': 49.89858012170385, 'PRO': 15.31322505800464, 'SMT': 5.76923076923077, 'SPL': 29.00763358778626, 'macro_f1': 0.2049621612964054, 'micro_f1': 0.25840397544577604}
training_time:0.061127662658691406
loss:0.008731285110116005
{'APL': 16.41337386018237, 'CMT': 8.071748878923767, 'DSC': 23.78223495702006, 'MAT': 52.7536231884058, 'PRO': 16.363636363636363, 'SMT': 6.451612903225806, 'SPL': 31.25, 'macro_f1': 0.2215517573591345, 'micro_f1': 0.27977207977207974}
training_time:0.07000350952148438
loss:0.003689089324325323
{'APL': 16.816816816816818, 'CMT': 8.438818565400844, 'DSC': 25.144508670520228, 'MAT': 54.54545454545454, 'PRO': 17.627118644067796, 'SMT': 6.306306306306307, 'SPL': 31.74603174603175, 'macro_f1': 0.22946436470656895, 'micro_f1': 0.2940848214285714}
training_time:0.0840752124786377
loss:0.0023049318697303534
{'APL': 17.81609195402299, 'CMT': 12.244897959183675, 'DSC': 25.07374631268437, 'MAT': 55.111111111111114, 'PRO': 18.426966292134832, 'SMT': 6.306306306306307, 'SPL': 28.57142857142857, 'macro_f1': 0.23364364072410265, 'micro_f1': 0.30159603742432584}
training_time:0.06939125061035156
loss:0.0020224428735673428
training_time:0.06177949905395508
loss:0.001413268386386335
{'APL': 16.452442159383033, 'CMT': 12.167300380228136, 'DSC': 26.37037037037037, 'MAT': 55.178416013925144, 'PRO': 19.517543859649127, 'SMT': 9.049773755656108, 'SPL': 26.984126984126984, 'macro_f1': 0.23674281931905558, 'micro_f1': 0.30522088353413657}
training_time:0.07653450965881348
loss:0.0011797735933214426
{'APL': 16.873449131513645, 'CMT': 11.851851851851851, 'DSC': 26.825633383010434, 'MAT': 55.680832610581085, 'PRO': 19.693654266958426, 'SMT': 11.711711711711711, 'SPL': 25.396825396825395, 'macro_f1': 0.24004851193207505, 'micro_f1': 0.3085927108273477}
training_time:0.08035588264465332
loss:0.0008059396641328931
training_time:0.05956554412841797
loss:0.0006312915356829762
training_time:0.06762123107910156
loss:0.0004989408189430833
training_time:0.06710076332092285
loss:0.0004532013554126024
training_time:0.06413531303405762
loss:0.0004782162432093173
training_time:0.06419110298156738
loss:0.0004622905398719013
training_time:0.059294700622558594
loss:0.000356487522367388
training_time:0.0636134147644043
loss:0.000326149893226102
training_time:0.06013965606689453
loss:0.00033001447445712984
training_time:0.06141161918640137
loss:0.0003288032894488424
training_time:0.05927562713623047
loss:0.0003371328057255596
training_time:0.06412315368652344
loss:0.0003295634232927114
training_time:0.06392407417297363
loss:0.0003157634928356856
training_time:0.07042479515075684
loss:0.0002940476988442242
training_time:0.06278610229492188
loss:0.0003043452452402562
training_time:0.06402039527893066
loss:0.00028189955628477037
{'APL': 16.018306636155604, 'CMT': 11.37123745819398, 'DSC': 26.666666666666668, 'MAT': 55.8659217877095, 'PRO': 19.375, 'SMT': 14.339622641509434, 'SPL': 28.148148148148145, 'macro_f1': 0.24540700476911906, 'micro_f1': 0.30916687146719096}
