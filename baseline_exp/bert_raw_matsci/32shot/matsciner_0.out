Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
77 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.2599928379058838
loss:1.0054223537445068
training_time:0.20947718620300293
loss:0.5573288798332214
training_time:0.21901154518127441
loss:0.48377856612205505
training_time:0.22198724746704102
loss:0.11029957979917526
training_time:0.21013212203979492
loss:0.049370378255844116
training_time:0.21961069107055664
loss:0.022189483046531677
training_time:0.20973634719848633
loss:0.027305202558636665
training_time:0.21167945861816406
loss:0.03566458076238632
training_time:0.2191615104675293
loss:0.0035862019285559654
training_time:0.21043896675109863
loss:0.003146351547911763
training_time:0.21261811256408691
loss:0.022018548101186752
training_time:0.20810222625732422
loss:0.0011866209097206593
training_time:0.21817398071289062
loss:0.0008197613642551005
training_time:0.20934224128723145
loss:0.001454581506550312
training_time:0.2142033576965332
loss:0.0009390779305249453
training_time:0.20949435234069824
loss:0.013377252966165543
training_time:0.212310791015625
loss:0.0008656354038976133
training_time:0.2099771499633789
loss:0.00043852045200765133
training_time:0.21397113800048828
loss:0.0004058518388774246
training_time:0.20955204963684082
loss:0.00037018946022726595
training_time:0.21213459968566895
loss:0.00032256124541163445
training_time:0.2101728916168213
loss:0.00022007149527780712
training_time:0.21573591232299805
loss:0.0008953111828304827
training_time:0.21006274223327637
loss:0.0002551233337726444
training_time:0.21235442161560059
loss:0.0002572305384092033
training_time:0.21001267433166504
loss:0.0002267236850457266
training_time:0.21662139892578125
loss:0.00021974813716951758
training_time:0.21084332466125488
loss:0.0002318038750672713
training_time:0.21590971946716309
loss:0.0002792549494188279
training_time:0.2089555263519287
loss:0.00022782098676543683
{'APL': 64.26426426426426, 'CMT': 62.21198156682027, 'DSC': 63.17044100119189, 'MAT': 73.95315826827537, 'PRO': 48.69305451829724, 'SMT': 60.66838046272493, 'SPL': 55.4054054054054, 'macro_f1': 0.611952407838542, 'micro_f1': 0.618687384992844}
