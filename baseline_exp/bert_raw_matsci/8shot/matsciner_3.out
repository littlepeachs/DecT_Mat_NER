Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
23 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.1158759593963623
loss:1.8435285091400146
training_time:0.08237814903259277
loss:0.9796286821365356
training_time:0.05649709701538086
loss:0.6239370107650757
training_time:0.05471920967102051
loss:0.4423823356628418
training_time:0.05266237258911133
loss:0.2997289299964905
training_time:0.05249142646789551
loss:0.2289118766784668
training_time:0.05220818519592285
loss:0.11439229547977448
training_time:0.0522761344909668
loss:0.08297117799520493
training_time:0.05265045166015625
loss:0.031239990144968033
training_time:0.05259871482849121
loss:0.027621854096651077
training_time:0.052300214767456055
loss:0.017494600266218185
training_time:0.05243277549743652
loss:0.009906943887472153
training_time:0.051784515380859375
loss:0.00670595420524478
training_time:0.051763057708740234
loss:0.003649211721494794
training_time:0.051694631576538086
loss:0.002803994109854102
training_time:0.05180859565734863
loss:0.001993804005905986
training_time:0.05160117149353027
loss:0.0017195658292621374
training_time:0.051657676696777344
loss:0.001456181867979467
training_time:0.05465507507324219
loss:0.0011104035656899214
training_time:0.05167198181152344
loss:0.0011158069828525186
training_time:0.05547046661376953
loss:0.0010540507500991225
training_time:0.05145859718322754
loss:0.0035767052322626114
training_time:0.05162692070007324
loss:0.0006933653494343162
training_time:0.05157661437988281
loss:0.0006784072611480951
training_time:0.05174088478088379
loss:0.0006299095111899078
training_time:0.05165886878967285
loss:0.0006458496209233999
training_time:0.051779985427856445
loss:0.0005846323329024017
training_time:0.051636457443237305
loss:0.0005769053241237998
training_time:0.0518338680267334
loss:0.0005423367256298661
training_time:0.05198073387145996
loss:0.000586557318456471
{'APL': 44.776119402985074, 'CMT': 51.90476190476191, 'DSC': 41.479524438573314, 'MAT': 65.94360086767897, 'PRO': 42.39401496259351, 'SMT': 33.021806853582554, 'SPL': 54.08163265306123, 'macro_f1': 0.476573515833195, 'micro_f1': 0.5003158559696779}
