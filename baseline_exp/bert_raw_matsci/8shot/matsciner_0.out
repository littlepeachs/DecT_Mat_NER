Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
22 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10712456703186035
loss:1.9702011346817017
training_time:0.05968451499938965
loss:1.126128077507019
training_time:0.055007219314575195
loss:0.7102917432785034
training_time:0.053559303283691406
loss:0.5785015821456909
training_time:0.05060005187988281
loss:0.3646541237831116
training_time:0.050324440002441406
loss:0.21525709331035614
training_time:0.05026865005493164
loss:0.12529747188091278
training_time:0.050441741943359375
loss:0.06732206046581268
training_time:0.050467491149902344
loss:0.03676877170801163
training_time:0.050345659255981445
loss:0.023430434986948967
training_time:0.05045461654663086
loss:0.014619209803640842
training_time:0.05066037178039551
loss:0.007800032384693623
training_time:0.05053305625915527
loss:0.0071179429069161415
training_time:0.05056953430175781
loss:0.005945607554167509
training_time:0.050508975982666016
loss:0.0032491381280124187
training_time:0.05062723159790039
loss:0.002872644690796733
training_time:0.05065727233886719
loss:0.0037739183753728867
training_time:0.0505523681640625
loss:0.0016130502335727215
training_time:0.05059385299682617
loss:0.0018719201907515526
training_time:0.05063962936401367
loss:0.0014148590853437781
training_time:0.05070304870605469
loss:0.001232922193594277
training_time:0.05052757263183594
loss:0.0010519768111407757
training_time:0.05049252510070801
loss:0.0010869147954508662
training_time:0.05057048797607422
loss:0.0010979310609400272
training_time:0.05063176155090332
loss:0.0007930331630632281
training_time:0.05049943923950195
loss:0.000952617556322366
training_time:0.050592660903930664
loss:0.0009788594907149673
training_time:0.05070185661315918
loss:0.0007672067731618881
training_time:0.05095529556274414
loss:0.0007545633125118911
training_time:0.05287456512451172
loss:0.0006791746127419174
{'APL': 37.254901960784316, 'CMT': 40.06849315068492, 'DSC': 38.62559241706161, 'MAT': 63.09611151870873, 'PRO': 18.091451292246518, 'SMT': 44.50127877237852, 'SPL': 50.241545893719795, 'macro_f1': 0.41697053572226345, 'micro_f1': 0.42306891526129503}
