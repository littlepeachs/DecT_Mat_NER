Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
24 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10482335090637207
loss:1.9772930145263672
training_time:0.06216621398925781
loss:0.9628186225891113
training_time:0.05072164535522461
loss:0.899002194404602
training_time:0.048387765884399414
loss:0.6684432625770569
training_time:0.04835629463195801
loss:0.5243172645568848
training_time:0.04849100112915039
loss:0.37575578689575195
training_time:0.04836702346801758
loss:0.24394690990447998
training_time:0.04838705062866211
loss:0.16281834244728088
training_time:0.04833388328552246
loss:0.09836900979280472
training_time:0.04828476905822754
loss:0.06175864487886429
training_time:0.04829144477844238
loss:0.03229581192135811
training_time:0.048383474349975586
loss:0.024381237104535103
training_time:0.04843640327453613
loss:0.012843175791203976
training_time:0.048700571060180664
loss:0.008024578914046288
training_time:0.04830193519592285
loss:0.0067132022231817245
training_time:0.04800271987915039
loss:0.004790185950696468
training_time:0.047902822494506836
loss:0.0037053076084703207
training_time:0.047905921936035156
loss:0.0030499775893986225
training_time:0.04787135124206543
loss:0.0019462511409074068
training_time:0.04788088798522949
loss:0.0018499933648854494
training_time:0.04785013198852539
loss:0.0016992787132039666
training_time:0.04785013198852539
loss:0.0012454864336177707
training_time:0.04783892631530762
loss:0.0012292375322431326
training_time:0.047896623611450195
loss:0.0010899800108745694
training_time:0.0478670597076416
loss:0.0009821095736697316
training_time:0.04786062240600586
loss:0.0018720944644883275
training_time:0.047823429107666016
loss:0.0010785481426864862
training_time:0.04787182807922363
loss:0.0008806785335764289
training_time:0.04782867431640625
loss:0.0009323920821771026
training_time:0.04789018630981445
loss:0.0008543454459868371
{'APL': 56.49202733485193, 'CMT': 62.96296296296296, 'DSC': 44.70314318975553, 'MAT': 56.80272108843536, 'PRO': 45.11517077045274, 'SMT': 53.41246290801187, 'SPL': 39.02439024390244, 'macro_f1': 0.5121612549976755, 'micro_f1': 0.5109301328761251}
