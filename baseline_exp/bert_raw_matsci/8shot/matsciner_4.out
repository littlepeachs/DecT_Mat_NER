Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
21 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10319113731384277
loss:2.428863525390625
training_time:0.06289243698120117
loss:1.2628973722457886
training_time:0.05671262741088867
loss:2.0386264324188232
training_time:0.052889347076416016
loss:3.5528764724731445
training_time:0.05214381217956543
loss:0.9944835305213928
training_time:0.052049875259399414
loss:0.9964752197265625
training_time:0.051987409591674805
loss:0.9550659656524658
training_time:0.05199837684631348
loss:0.9052585959434509
training_time:0.05203366279602051
loss:0.8210856318473816
training_time:0.0520477294921875
loss:0.750524640083313
training_time:0.0522003173828125
loss:0.5989411473274231
training_time:0.05239224433898926
loss:0.535291850566864
training_time:0.05210375785827637
loss:0.42406609654426575
training_time:0.0518498420715332
loss:0.3236032724380493
training_time:0.05174994468688965
loss:0.2543322443962097
training_time:0.05177426338195801
loss:0.1863744556903839
training_time:0.05178475379943848
loss:0.13605435192584991
training_time:0.05183005332946777
loss:0.09156214445829391
training_time:0.05167078971862793
loss:0.06865303218364716
training_time:0.051692962646484375
loss:0.04860871285200119
training_time:0.0516657829284668
loss:0.039584800601005554
training_time:0.051682233810424805
loss:0.03211437165737152
training_time:0.05164361000061035
loss:0.026716677471995354
training_time:0.051680803298950195
loss:0.019283581525087357
training_time:0.05162787437438965
loss:0.016959834843873978
training_time:0.05165505409240723
loss:0.024285363033413887
training_time:0.05163860321044922
loss:0.010556875728070736
training_time:0.051740407943725586
loss:0.011208724230527878
training_time:0.05161309242248535
loss:0.010048717260360718
training_time:0.05163741111755371
loss:0.009494554251432419
{'APL': 42.3162583518931, 'CMT': 28.220858895705526, 'DSC': 19.52755905511811, 'MAT': 52.13815789473685, 'PRO': 24.841341795104263, 'SMT': 41.73913043478261, 'SPL': 50.2994011976048, 'macro_f1': 0.3701181537499218, 'micro_f1': 0.36058128973660303}
