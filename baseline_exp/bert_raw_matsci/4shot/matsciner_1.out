Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
13 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.12487912178039551
loss:1.950297474861145
training_time:0.045064449310302734
loss:1.2627524137496948
training_time:0.04724860191345215
loss:0.9571411609649658
training_time:0.04036235809326172
loss:0.8300153017044067
training_time:0.04624509811401367
loss:0.7254801988601685
training_time:0.03907966613769531
loss:0.5773329138755798
training_time:0.04869818687438965
loss:0.3947221040725708
training_time:0.03905439376831055
loss:0.2638787031173706
training_time:0.04949021339416504
loss:0.16429951786994934
training_time:0.04121994972229004
loss:0.10126657038927078
training_time:0.04048776626586914
loss:0.054043401032686234
training_time:0.04588723182678223
loss:0.028680631890892982
training_time:0.03923439979553223
loss:0.014780900441110134
training_time:0.04875922203063965
loss:0.01006008218973875
training_time:0.039049625396728516
loss:0.009211286902427673
training_time:0.04605698585510254
loss:0.004564453847706318
training_time:0.040064096450805664
loss:0.004915714729577303
training_time:0.046875715255737305
loss:0.002791991923004389
training_time:0.04017448425292969
loss:0.002433916088193655
training_time:0.047373294830322266
loss:0.002078279620036483
training_time:0.03902888298034668
loss:0.0019440738251432776
training_time:0.048673152923583984
loss:0.0013940087519586086
training_time:0.03900742530822754
loss:0.0011570027563720942
training_time:0.049407958984375
loss:0.0011344896629452705
training_time:0.04208064079284668
loss:0.0010566761484369636
training_time:0.04564476013183594
loss:0.0009950876701623201
training_time:0.03889274597167969
loss:0.0009868256747722626
training_time:0.047971487045288086
loss:0.0008300471818074584
training_time:0.05040144920349121
loss:0.0009154776344075799
training_time:0.03992724418640137
loss:0.001037951442413032
{'APL': 44.987775061124694, 'CMT': 45.70230607966457, 'DSC': 31.21951219512195, 'MAT': 31.25, 'PRO': 26.57869934024505, 'SMT': 20.938628158844764, 'SPL': 54.088050314465406, 'macro_f1': 0.363949958784952, 'micro_f1': 0.33384694401643555}
