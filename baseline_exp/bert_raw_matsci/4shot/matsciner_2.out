Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
14 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.11527538299560547
loss:2.6224794387817383
training_time:0.04738116264343262
loss:0.797609806060791
training_time:0.04270148277282715
loss:0.5194545984268188
training_time:0.042566537857055664
loss:0.3993610143661499
training_time:0.043549537658691406
loss:0.21025696396827698
training_time:0.04388904571533203
loss:0.14832629263401031
training_time:0.0446927547454834
loss:0.06566009670495987
training_time:0.04250359535217285
loss:0.04302484169602394
training_time:0.042414188385009766
loss:0.018491405993700027
training_time:0.04348039627075195
loss:0.011134670116007328
training_time:0.04247331619262695
loss:0.006879595573991537
training_time:0.04257798194885254
loss:0.005538607481867075
training_time:0.04369497299194336
loss:0.0030751891899853945
training_time:0.04240894317626953
loss:0.002606218447908759
training_time:0.04242730140686035
loss:0.0017274918500334024
training_time:0.04324936866760254
loss:0.0020857611671090126
training_time:0.042525291442871094
loss:0.001101940986700356
training_time:0.0424046516418457
loss:0.0009094172273762524
training_time:0.043634653091430664
loss:0.0008816648623906076
training_time:0.04256749153137207
loss:0.0007607681909576058
training_time:0.042448997497558594
loss:0.0006812171777710319
training_time:0.04353594779968262
loss:0.0006603394285775721
training_time:0.04240536689758301
loss:0.0005502236308529973
training_time:0.0426335334777832
loss:0.0006049331277608871
training_time:0.043775320053100586
loss:0.0006476994603872299
training_time:0.04266762733459473
loss:0.0006058249273337424
training_time:0.04226112365722656
loss:0.009027628228068352
training_time:0.04252362251281738
loss:0.0004981847596354783
training_time:0.04347586631774902
loss:0.00047905155224725604
training_time:0.042566776275634766
loss:0.0004859568434767425
{'APL': 43.962848297213625, 'CMT': 48.38012958963282, 'DSC': 9.282700421940927, 'MAT': 34.24807903402854, 'PRO': 5.5222088835534215, 'SMT': 24.427480916030532, 'SPL': 33.07086614173229, 'macro_f1': 0.28413473326304595, 'micro_f1': 0.25758915414087824}
