Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
14 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10451459884643555
loss:1.9930514097213745
training_time:0.05199718475341797
loss:0.6847013831138611
training_time:0.04918670654296875
loss:0.47641095519065857
training_time:0.04560375213623047
loss:0.34077101945877075
training_time:0.0556640625
loss:0.2443135678768158
training_time:0.04315495491027832
loss:0.15386195480823517
training_time:0.04944300651550293
loss:0.0938851609826088
training_time:0.04293680191040039
loss:0.03970948979258537
training_time:0.050591230392456055
loss:0.025345994159579277
training_time:0.04309415817260742
loss:0.013479798100888729
training_time:0.051080942153930664
loss:0.010802902281284332
training_time:0.04410505294799805
loss:0.004649405833333731
training_time:0.04961895942687988
loss:0.003974010702222586
training_time:0.04531288146972656
loss:0.0031653123442083597
training_time:0.048998117446899414
loss:0.0021085506305098534
training_time:0.0429224967956543
loss:0.003912109881639481
training_time:0.05069613456726074
loss:0.009233333170413971
training_time:0.04301118850708008
loss:0.004255346953868866
training_time:0.04960918426513672
loss:0.0012232312001287937
training_time:0.04306387901306152
loss:0.0009019188000820577
training_time:0.0534062385559082
loss:0.0007513553719036281
training_time:0.043039798736572266
loss:0.000741279567591846
training_time:0.04934835433959961
loss:0.0017259034793823957
training_time:0.04480934143066406
loss:0.0010085246758535504
training_time:0.04960274696350098
loss:0.0008973033400252461
training_time:0.04531288146972656
loss:0.0006270476151257753
training_time:0.04974818229675293
loss:0.0005498273530974984
training_time:0.045145273208618164
loss:0.0007506680558435619
training_time:0.051048994064331055
loss:0.0006992120761424303
training_time:0.043097734451293945
loss:0.000531956204213202
{'APL': 14.61187214611872, 'CMT': 38.28715365239294, 'DSC': 4.792332268370607, 'MAT': 22.043628013777266, 'PRO': 3.781979977753059, 'SMT': 22.222222222222225, 'SPL': 46.95652173913042, 'macro_f1': 0.2181367285996646, 'micro_f1': 0.16292798110979928}
