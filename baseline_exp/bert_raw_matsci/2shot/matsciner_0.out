Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
8 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.11728167533874512
loss:2.172980785369873
training_time:0.043083906173706055
loss:0.6902977228164673
training_time:0.05049443244934082
loss:2.0146214962005615
training_time:0.04230093955993652
loss:0.7019636034965515
training_time:0.0484466552734375
loss:0.6797302961349487
training_time:0.04188942909240723
loss:0.585892915725708
training_time:0.046918392181396484
loss:0.5983544588088989
training_time:0.04068183898925781
loss:0.48717135190963745
training_time:0.047824859619140625
loss:0.3846547305583954
training_time:0.04192948341369629
loss:0.2690572738647461
training_time:0.0489504337310791
loss:0.22174201905727386
training_time:0.041936635971069336
loss:0.11825356632471085
training_time:0.05052924156188965
loss:0.08120602369308472
training_time:0.04076552391052246
loss:0.08140554279088974
training_time:0.04849958419799805
loss:0.05061504617333412
training_time:0.04070425033569336
loss:0.0304286889731884
training_time:0.048468589782714844
loss:0.028729859739542007
training_time:0.04073786735534668
loss:0.01838749274611473
training_time:0.04877042770385742
loss:0.015808744356036186
training_time:0.0408940315246582
loss:0.010829913429915905
training_time:0.04870343208312988
loss:0.0051878890953958035
training_time:0.04067254066467285
loss:0.005359496455639601
training_time:0.05071520805358887
loss:0.0038706299383193254
training_time:0.04084420204162598
loss:0.003239649347960949
training_time:0.04806852340698242
loss:0.005413266830146313
training_time:0.04066348075866699
loss:0.002993714762851596
training_time:0.04801797866821289
loss:0.003299645148217678
training_time:0.04076194763183594
loss:0.0026475449558347464
training_time:0.04848337173461914
loss:0.0026693292893469334
training_time:0.040643930435180664
loss:0.0024188810493797064
{'APL': 25.396825396825403, 'CMT': 22.857142857142858, 'DSC': 9.074410163339383, 'MAT': 36.40081799591002, 'PRO': 1.6355140186915889, 'SMT': 2.2598870056497176, 'SPL': 35.53299492385786, 'macro_f1': 0.1902251319448812, 'micro_f1': 0.19214224261542873}
