Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
6 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10183262825012207
loss:2.2431843280792236
training_time:0.05481386184692383
loss:0.8728214502334595
training_time:0.041898250579833984
loss:0.544079601764679
training_time:0.0478665828704834
loss:0.29999715089797974
training_time:0.04130291938781738
loss:0.11784595996141434
training_time:0.048453569412231445
loss:0.037548936903476715
training_time:0.04185843467712402
loss:0.0150767145678401
training_time:0.049021244049072266
loss:0.037764597684144974
training_time:0.04201555252075195
loss:0.004504842218011618
training_time:0.04990506172180176
loss:0.04947027564048767
training_time:0.04105424880981445
loss:0.0022307182662189007
training_time:0.048486948013305664
loss:0.0022528108675032854
training_time:0.041858673095703125
loss:0.0022834795527160168
training_time:0.049430131912231445
loss:0.001313400105573237
training_time:0.041970252990722656
loss:0.0010108242277055979
training_time:0.04889369010925293
loss:0.012377475388348103
training_time:0.04155611991882324
loss:0.0009157515014521778
training_time:0.048584699630737305
loss:0.0008013533079065382
training_time:0.04076361656188965
loss:0.0007065461250022054
training_time:0.048703908920288086
loss:0.0006635513855144382
training_time:0.04239201545715332
loss:0.0005804779357276857
training_time:0.04864788055419922
loss:0.0005414175102487206
training_time:0.04072737693786621
loss:0.0005938812391832471
training_time:0.04884934425354004
loss:0.0005425167037174106
training_time:0.04278111457824707
loss:0.0005354542518034577
training_time:0.04839754104614258
loss:0.000566770788282156
training_time:0.04065871238708496
loss:0.00044550112215802073
training_time:0.05104827880859375
loss:0.0004916117177344859
training_time:0.04326510429382324
loss:0.00038477889029309154
training_time:0.04844236373901367
loss:0.00045585486805066466
{'APL': 17.333333333333336, 'CMT': 15.757575757575756, 'DSC': 19.331742243436754, 'MAT': 44.44444444444445, 'PRO': 12.514220705346988, 'SMT': 12.16216216216216, 'SPL': 24.778761061946902, 'macro_f1': 0.2090317710117805, 'micro_f1': 0.2374043787918755}
