Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
6 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10206055641174316
loss:2.2366292476654053
training_time:0.04497408866882324
loss:0.8006473183631897
training_time:0.05022382736206055
loss:0.47802379727363586
training_time:0.040799617767333984
loss:0.2895222008228302
training_time:0.049474239349365234
loss:0.11135049164295197
training_time:0.04066586494445801
loss:0.04602517932653427
training_time:0.05213427543640137
loss:0.018924089148640633
training_time:0.042620182037353516
loss:0.008354614488780499
training_time:0.04752755165100098
loss:0.005835527088493109
training_time:0.04163646697998047
loss:0.004138731863349676
training_time:0.04980778694152832
loss:0.002663664985448122
training_time:0.04430341720581055
loss:0.0052087148651480675
training_time:0.04976391792297363
loss:0.0013687735190615058
training_time:0.04125356674194336
loss:0.0012187509564682841
training_time:0.04815268516540527
loss:0.0011508739553391933
training_time:0.04051637649536133
loss:0.07635509222745895
training_time:0.04810810089111328
loss:0.0019107508705928922
training_time:0.04069066047668457
loss:0.006447776686400175
training_time:0.04819178581237793
loss:0.0008634676923975348
training_time:0.03958916664123535
loss:0.0008590286015532911
training_time:0.049533843994140625
loss:0.0011786514660343528
training_time:0.04185295104980469
loss:0.0017078372184187174
training_time:0.04995322227478027
loss:0.0018808457534760237
training_time:0.04980325698852539
loss:0.002403559861704707
training_time:0.047593116760253906
loss:0.0013330001384019852
training_time:0.04764962196350098
loss:0.0021287943236529827
training_time:0.046694278717041016
loss:0.0011590856593102217
training_time:0.0428922176361084
loss:0.001192983123473823
training_time:0.03977370262145996
loss:0.0009130220278166234
training_time:0.042215824127197266
loss:0.0008394060423597693
{'APL': 9.221902017291066, 'CMT': 18.23529411764706, 'DSC': 11.515151515151516, 'MAT': 33.70535714285714, 'PRO': 1.166861143523921, 'SMT': 12.785388127853883, 'SPL': 17.567567567567565, 'macro_f1': 0.14885360233127448, 'micro_f1': 0.15460051918084797}
