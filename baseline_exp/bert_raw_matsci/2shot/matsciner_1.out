Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
9 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.12338018417358398
loss:2.07546067237854
training_time:0.04450678825378418
loss:0.5794886946678162
training_time:0.04804873466491699
loss:0.6174646615982056
training_time:0.0533294677734375
loss:0.4228808879852295
training_time:0.04134535789489746
loss:0.3205997347831726
training_time:0.04866218566894531
loss:0.213950052857399
training_time:0.04820561408996582
loss:0.13550175726413727
training_time:0.0496821403503418
loss:0.08625591546297073
training_time:0.046872615814208984
loss:0.05209839716553688
training_time:0.04873299598693848
loss:0.03098265640437603
training_time:0.05041027069091797
loss:0.015934785827994347
training_time:0.04338669776916504
loss:0.01285011786967516
training_time:0.049248695373535156
loss:0.008194361813366413
training_time:0.04130721092224121
loss:0.008086864836513996
training_time:0.04865598678588867
loss:0.01569225639104843
training_time:0.04216957092285156
loss:0.004340148065239191
training_time:0.04776906967163086
loss:0.022123657166957855
training_time:0.042296409606933594
loss:0.0015454301610589027
training_time:0.049477338790893555
loss:0.001978011569008231
training_time:0.042385101318359375
loss:0.0018942903261631727
training_time:0.04755830764770508
loss:0.005575847811996937
training_time:0.04268908500671387
loss:0.002160280244424939
training_time:0.05010223388671875
loss:0.00192305410746485
training_time:0.04372715950012207
loss:0.0014410174917429686
training_time:0.04311537742614746
loss:0.0013081709621474147
training_time:0.05147671699523926
loss:0.0010229586623609066
training_time:0.04435086250305176
loss:0.0011019686935469508
training_time:0.05123782157897949
loss:0.001080607995390892
training_time:0.04296064376831055
loss:0.0009476306731812656
training_time:0.055428504943847656
loss:0.0009617360192351043
{'APL': 10.344827586206897, 'CMT': 13.852813852813853, 'DSC': 3.1111111111111107, 'MAT': 39.42505133470226, 'PRO': 10.561797752808989, 'SMT': 4.9504950495049505, 'SPL': 23.157894736842106, 'macro_f1': 0.15057713060570024, 'micro_f1': 0.1871008939974457}
