Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
7 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.11419558525085449
loss:2.5648562908172607
training_time:0.04519057273864746
loss:0.6509436964988708
training_time:0.049890995025634766
loss:0.40762391686439514
training_time:0.04231905937194824
loss:0.2187211960554123
training_time:0.04990196228027344
loss:0.10189880430698395
training_time:0.0422968864440918
loss:0.047203175723552704
training_time:0.04860377311706543
loss:0.016205517575144768
training_time:0.04297375679016113
loss:0.015644369646906853
training_time:0.04893207550048828
loss:0.056939564645290375
training_time:0.042185068130493164
loss:0.003922682721167803
training_time:0.0484919548034668
loss:0.003426379757001996
training_time:0.04133248329162598
loss:0.0022553077433258295
training_time:0.04919600486755371
loss:0.002261514076963067
training_time:0.0439143180847168
loss:0.0019647576846182346
training_time:0.04988455772399902
loss:0.003805949119850993
training_time:0.04352879524230957
loss:0.0014224486658349633
training_time:0.05040407180786133
loss:0.0012061750749126077
training_time:0.04264378547668457
loss:0.0011858177604153752
training_time:0.05043745040893555
loss:0.0011338811600580812
training_time:0.040427446365356445
loss:0.0009901936864480376
training_time:0.04780292510986328
loss:0.0012743792030960321
training_time:0.040045976638793945
loss:0.0007691981154493988
training_time:0.04859352111816406
loss:0.0007538718637079
training_time:0.040972232818603516
loss:0.0006585010560229421
training_time:0.04541778564453125
loss:0.0005656619323417544
training_time:0.04926347732543945
loss:0.0010567944264039397
training_time:0.039791107177734375
loss:0.000659327837638557
training_time:0.04769134521484375
loss:0.0013188979355618358
training_time:0.03988003730773926
loss:0.0006195204332470894
training_time:0.04802060127258301
loss:0.0005458345985971391
{'APL': 21.73913043478261, 'CMT': 29.411764705882355, 'DSC': 30.091743119266056, 'MAT': 7.065217391304349, 'PRO': 0.0, 'SMT': 13.249211356466878, 'SPL': 5.217391304347826, 'macro_f1': 0.15253494044578583, 'micro_f1': 0.13738524849635958}
