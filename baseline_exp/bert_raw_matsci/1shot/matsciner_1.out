Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
5 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.09564900398254395
loss:2.088646173477173
training_time:0.045348167419433594
loss:0.5661499500274658
training_time:0.04765009880065918
loss:0.30274492502212524
training_time:0.04540753364562988
loss:0.17204523086547852
training_time:0.041821956634521484
loss:0.06518720835447311
training_time:0.043370962142944336
loss:0.03637212514877319
training_time:0.04802584648132324
loss:0.0091138556599617
training_time:0.04469585418701172
loss:0.006000889465212822
training_time:0.04849076271057129
loss:0.0028803690802305937
training_time:0.04949808120727539
loss:0.0023263520561158657
training_time:0.04828453063964844
loss:0.0014275697758421302
training_time:0.05056023597717285
loss:0.0013968679122626781
training_time:0.043189048767089844
loss:0.001117596635594964
training_time:0.04223370552062988
loss:0.001016982481814921
training_time:0.04115605354309082
loss:0.0007361119496636093
training_time:0.04145383834838867
loss:0.0005300500779412687
training_time:0.04250741004943848
loss:0.0005494182696565986
training_time:0.0412907600402832
loss:0.00042874421342276037
training_time:0.041413307189941406
loss:0.000513873528689146
training_time:0.04447150230407715
loss:0.0004744606849271804
training_time:0.04334068298339844
loss:0.00038418459007516503
training_time:0.04216170310974121
loss:0.00031295992084778845
training_time:0.042200326919555664
loss:0.0003872885135933757
training_time:0.042474985122680664
loss:0.0003340977418702096
training_time:0.04284477233886719
loss:0.0002136313123628497
training_time:0.0419001579284668
loss:0.0002649393572937697
training_time:0.04108166694641113
loss:0.00022765505127608776
training_time:0.04352140426635742
loss:0.00020324173965491354
training_time:0.0422978401184082
loss:0.12447849661111832
training_time:0.04462242126464844
loss:0.0002266974770464003
{'APL': 18.803418803418808, 'CMT': 0.9523809523809524, 'DSC': 0.0, 'MAT': 13.829787234042554, 'PRO': 1.2422360248447206, 'SMT': 0.0, 'SPL': 10.0, 'macro_f1': 0.06403974716383862, 'micro_f1': 0.06111313204801746}
