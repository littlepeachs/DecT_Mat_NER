Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
6 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.10886716842651367
loss:1.8518608808517456
training_time:0.04398012161254883
loss:0.4276350140571594
training_time:0.04914522171020508
loss:1.5593453645706177
training_time:0.04428601264953613
loss:0.4533739387989044
training_time:0.049373626708984375
loss:0.34578630328178406
training_time:0.051291465759277344
loss:0.2821294069290161
training_time:0.04257345199584961
loss:0.22802765667438507
training_time:0.048773765563964844
loss:0.1518019437789917
training_time:0.041559696197509766
loss:0.09633473306894302
training_time:0.049477577209472656
loss:0.05993486940860748
training_time:0.0414280891418457
loss:0.03888416290283203
training_time:0.05064558982849121
loss:0.023672381415963173
training_time:0.04177689552307129
loss:0.011567838490009308
training_time:0.0486605167388916
loss:0.013241981156170368
training_time:0.04332900047302246
loss:0.00576278381049633
training_time:0.04805183410644531
loss:0.003976228181272745
training_time:0.03802371025085449
loss:0.002927378285676241
training_time:0.04879355430603027
loss:0.0033526087645441294
training_time:0.053072452545166016
loss:0.0024532161187380552
training_time:0.04205513000488281
loss:0.0017818338237702847
training_time:0.048114776611328125
loss:0.0017753042047843337
training_time:0.04167914390563965
loss:0.0016684611327946186
training_time:0.046935319900512695
loss:0.001518954406492412
training_time:0.047255754470825195
loss:0.0016853194683790207
training_time:0.04089522361755371
loss:0.0011634387774392962
training_time:0.04902935028076172
loss:0.001075409585610032
training_time:0.04187965393066406
loss:0.0012434660457074642
training_time:0.048356056213378906
loss:0.0009912109235301614
training_time:0.03994870185852051
loss:0.0008764296653680503
training_time:0.04865241050720215
loss:0.0008205745834857225
{'APL': 0.0, 'CMT': 0.6472491909385114, 'DSC': 6.073752711496745, 'MAT': 39.075630252100844, 'PRO': 1.715686274509804, 'SMT': 2.2857142857142856, 'SPL': 27.09677419354839, 'macro_f1': 0.10984972415472655, 'micro_f1': 0.152073732718894}
