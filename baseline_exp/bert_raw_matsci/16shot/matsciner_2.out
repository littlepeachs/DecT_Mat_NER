Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
46 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.16982007026672363
loss:1.0161404609680176
training_time:0.13774943351745605
loss:0.8868058919906616
training_time:0.11763763427734375
loss:0.7165489196777344
training_time:0.11980676651000977
loss:0.5876653790473938
training_time:0.11777853965759277
loss:0.3664758503437042
training_time:0.11811280250549316
loss:0.19628764688968658
training_time:0.11767029762268066
loss:0.09338262677192688
training_time:0.11860060691833496
loss:0.053239256143569946
training_time:0.11755490303039551
loss:0.03618926554918289
training_time:0.1186375617980957
loss:0.014947532676160336
training_time:0.11755752563476562
loss:0.009093570522964
training_time:0.11850953102111816
loss:0.00855167955160141
training_time:0.11767005920410156
loss:0.01364817563444376
training_time:0.11855673789978027
loss:0.02080291137099266
training_time:0.117156982421875
loss:0.0014997811522334814
training_time:0.11860799789428711
loss:0.03215055540204048
training_time:0.11718273162841797
loss:0.011153188534080982
training_time:0.11801552772521973
loss:0.0012450558133423328
training_time:0.11738038063049316
loss:0.0009147966047748923
training_time:0.11772036552429199
loss:0.0004644712316803634
training_time:0.11839652061462402
loss:0.000509411096572876
training_time:0.11773371696472168
loss:0.014590062201023102
training_time:0.1181643009185791
loss:0.0008915482321754098
training_time:0.1176149845123291
loss:0.0006890353979542851
training_time:0.1187283992767334
loss:0.0012105291243642569
training_time:0.11764693260192871
loss:0.0008969818591140211
training_time:0.11904597282409668
loss:0.0004194294160697609
training_time:0.11758828163146973
loss:0.002109681023284793
training_time:0.11867880821228027
loss:0.0011882048565894365
training_time:0.11765265464782715
loss:0.00032655071117915213
{'APL': 49.61636828644501, 'CMT': 67.12328767123287, 'DSC': 60.82004555808657, 'MAT': 71.02040816326532, 'PRO': 47.95050270688322, 'SMT': 55.494505494505496, 'SPL': 48.93617021276596, 'macro_f1': 0.5728018401331206, 'micro_f1': 0.5933890880127438}
