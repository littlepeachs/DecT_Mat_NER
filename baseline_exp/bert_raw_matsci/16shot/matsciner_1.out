Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
41 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
training_time:0.19423341751098633
loss:1.9764771461486816
training_time:0.12607979774475098
loss:0.6416324973106384
training_time:0.12518715858459473
loss:0.5440447330474854
training_time:0.12470436096191406
loss:0.2750959098339081
training_time:0.12408208847045898
loss:0.09711211919784546
training_time:0.12493276596069336
loss:0.05234457179903984
training_time:0.1257495880126953
loss:0.020053120329976082
training_time:0.1250169277191162
loss:0.014289558865129948
training_time:0.1244347095489502
loss:0.03711894154548645
training_time:0.12406063079833984
loss:0.04232615604996681
training_time:0.1255948543548584
loss:0.033483412116765976
training_time:0.12572407722473145
loss:0.005805228371173143
training_time:0.12485671043395996
loss:0.034262172877788544
training_time:0.12503576278686523
loss:0.028481775894761086
training_time:0.12415099143981934
loss:0.012937845662236214
training_time:0.12559962272644043
loss:0.004243600182235241
training_time:0.12624144554138184
loss:0.0016674373764544725
training_time:0.12486481666564941
loss:0.015094800852239132
training_time:0.12437820434570312
loss:0.0016682284185662866
training_time:0.12418174743652344
loss:0.000650937610771507
training_time:0.12615060806274414
loss:0.0278587955981493
training_time:0.12551593780517578
loss:0.0006870926008559763
training_time:0.12498116493225098
loss:0.0011207426432520151
training_time:0.12441325187683105
loss:0.0012159168254584074
training_time:0.12432646751403809
loss:0.000969046785030514
training_time:0.12605714797973633
loss:0.0012863078154623508
training_time:0.12550592422485352
loss:0.0006478738505393267
training_time:0.1249690055847168
loss:0.0005030354368500412
training_time:0.1243753433227539
loss:0.00036970077781006694
training_time:0.12430000305175781
loss:0.0015533164842054248
{'APL': 39.743589743589745, 'CMT': 67.35751295336787, 'DSC': 52.368421052631575, 'MAT': 63.76068376068377, 'PRO': 38.62660944206009, 'SMT': 55.307262569832396, 'SPL': 59.84251968503938, 'macro_f1': 0.5385808560102927, 'micro_f1': 0.5218764095624718}
