/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:44 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:45 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1027.01it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:46 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-29dd68c625901780.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3700.00 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:51 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:51 - INFO - __main__ -   Num examples = 77
06/01/2023 14:48:51 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:51 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:51 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:51 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:51 - INFO - __main__ -   Total optimization steps = 300
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 17.76374053955078
Total epoch: 1. epoch loss: 16.942241668701172
Total epoch: 2. epoch loss: 16.1961669921875
Total epoch: 3. epoch loss: 15.549363136291504
Total epoch: 4. epoch loss: 15.000908851623535
Total epoch: 5. epoch loss: 14.534772872924805
Total epoch: 6. epoch loss: 14.130337715148926
Total epoch: 7. epoch loss: 13.768596649169922
Total epoch: 8. epoch loss: 13.435635566711426
Total epoch: 9. epoch loss: 13.122665405273438
Total epoch: 10. epoch loss: 12.824710845947266
Total epoch: 11. epoch loss: 12.539534568786621
Total epoch: 12. epoch loss: 12.266711235046387
Total epoch: 13. epoch loss: 12.006889343261719
Total epoch: 14. epoch loss: 11.7612886428833
Total epoch: 15. epoch loss: 11.531367301940918
Total epoch: 16. epoch loss: 11.318516731262207
Total epoch: 17. epoch loss: 11.123641967773438
Total epoch: 18. epoch loss: 10.946645736694336
Total epoch: 19. epoch loss: 10.785749435424805
Total epoch: 20. epoch loss: 10.637855529785156
Total epoch: 21. epoch loss: 10.499641418457031
Total epoch: 22. epoch loss: 10.36851978302002
Total epoch: 23. epoch loss: 10.24330997467041
Total epoch: 24. epoch loss: 10.124271392822266
Total epoch: 25. epoch loss: 10.012641906738281
Total epoch: 26. epoch loss: 9.909791946411133
Total epoch: 27. epoch loss: 9.815178871154785
Total epoch: 28. epoch loss: 9.723982810974121
Total epoch: 29. epoch loss: 9.63524055480957
Total epoch: 30. epoch loss: 9.5512056350708
Total epoch: 31. epoch loss: 9.4722318649292
Total epoch: 32. epoch loss: 9.39770793914795
Total epoch: 33. epoch loss: 9.326900482177734
Total epoch: 34. epoch loss: 9.259020805358887
Total epoch: 35. epoch loss: 9.193365097045898
Total epoch: 36. epoch loss: 9.129890441894531
Total epoch: 37. epoch loss: 9.069327354431152
Total epoch: 38. epoch loss: 9.012404441833496
Total epoch: 39. epoch loss: 8.958666801452637
Total epoch: 40. epoch loss: 8.906767845153809
Total epoch: 41. epoch loss: 8.855798721313477
Total epoch: 42. epoch loss: 8.805970191955566
Total epoch: 43. epoch loss: 8.757919311523438
Total epoch: 44. epoch loss: 8.711867332458496
Total epoch: 45. epoch loss: 8.66763687133789
Total epoch: 46. epoch loss: 8.625015258789062
Total epoch: 47. epoch loss: 8.583731651306152
Total epoch: 48. epoch loss: 8.5435209274292
Total epoch: 49. epoch loss: 8.50465202331543
Total epoch: 50. epoch loss: 8.467217445373535
Total epoch: 51. epoch loss: 8.430840492248535
Total epoch: 52. epoch loss: 8.395835876464844
Total epoch: 53. epoch loss: 8.362156867980957
Total epoch: 54. epoch loss: 8.32910442352295
Total epoch: 55. epoch loss: 8.296587944030762
Total epoch: 56. epoch loss: 8.265122413635254
Total epoch: 57. epoch loss: 8.234796524047852
Total epoch: 58. epoch loss: 8.205310821533203
Total epoch: 59. epoch loss: 8.176445007324219
Total epoch: 60. epoch loss: 8.148259162902832
Total epoch: 61. epoch loss: 8.120844841003418
Total epoch: 62. epoch loss: 8.094002723693848
Total epoch: 63. epoch loss: 8.067761421203613
Total epoch: 64. epoch loss: 8.042231559753418
Total epoch: 65. epoch loss: 8.01722526550293
Total epoch: 66. epoch loss: 7.992939472198486
Total epoch: 67. epoch loss: 7.969202995300293
Total epoch: 68. epoch loss: 7.946264266967773
Total epoch: 69. epoch loss: 7.923944473266602
Total epoch: 70. epoch loss: 7.902204990386963
Total epoch: 71. epoch loss: 7.880418300628662
Total epoch: 72. epoch loss: 7.859133243560791
Total epoch: 73. epoch loss: 7.838794231414795
Total epoch: 74. epoch loss: 7.8183159828186035
Total epoch: 75. epoch loss: 7.798456192016602
Total epoch: 76. epoch loss: 7.7793378829956055
Total epoch: 77. epoch loss: 7.759977340698242
Total epoch: 78. epoch loss: 7.741620063781738
Total epoch: 79. epoch loss: 7.723813533782959
Total epoch: 80. epoch loss: 7.705410480499268
Total epoch: 81. epoch loss: 7.688048362731934
Total epoch: 82. epoch loss: 7.6714582443237305
Total epoch: 83. epoch loss: 7.654299259185791
Total epoch: 84. epoch loss: 7.637515068054199
Total epoch: 85. epoch loss: 7.621805191040039
Total epoch: 86. epoch loss: 7.604615688323975
Total epoch: 87. epoch loss: 7.589546203613281
Total epoch: 88. epoch loss: 7.57464075088501
Total epoch: 89. epoch loss: 7.558715343475342
Total epoch: 90. epoch loss: 7.546191692352295
Total epoch: 91. epoch loss: 7.528713226318359
Total epoch: 92. epoch loss: 7.520291805267334
Total epoch: 93. epoch loss: 7.502128601074219
Total epoch: 94. epoch loss: 7.499175071716309
Total epoch: 95. epoch loss: 7.487206935882568
Total epoch: 96. epoch loss: 7.4664411544799805
Total epoch: 97. epoch loss: 7.461559295654297
Total epoch: 98. epoch loss: 7.4517083168029785
Total epoch: 99. epoch loss: 7.432397365570068
Total epoch: 99. DecT loss: 7.432397365570068
Training time: 0.5998342037200928
APL_precision: 0.2486910994764398, APL_recall: 0.5588235294117647, APL_f1: 0.34420289855072467, APL_number: 170
CMT_precision: 0.3087818696883853, CMT_recall: 0.558974358974359, CMT_f1: 0.3978102189781022, CMT_number: 195
DSC_precision: 0.5714285714285714, DSC_recall: 0.5491990846681922, DSC_f1: 0.5600933488914819, DSC_number: 437
MAT_precision: 0.6015180265654649, MAT_recall: 0.46480938416422285, MAT_f1: 0.5244003308519437, MAT_number: 682
PRO_precision: 0.45005370569280345, PRO_recall: 0.543450064850843, PRO_f1: 0.49236192714453586, PRO_number: 771
SMT_precision: 0.39204545454545453, SMT_recall: 0.40350877192982454, SMT_f1: 0.39769452449567716, SMT_number: 171
SPL_precision: 0.4918032786885246, SPL_recall: 0.4, SPL_f1: 0.4411764705882353, SPL_number: 75
overall_precision: 0.4487719298245614, overall_recall: 0.5113954418232707, overall_f1: 0.4780414875724164, overall_accuracy: 0.8220284468586949
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]