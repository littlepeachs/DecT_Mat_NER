/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:44 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:45 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-145490b5457c9ad0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1110.78it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:46 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-145490b5457c9ad0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7e770c668affbbc4.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3949.77 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:51 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:51 - INFO - __main__ -   Num examples = 72
06/01/2023 14:48:51 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:51 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:51 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:51 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:51 - INFO - __main__ -   Total optimization steps = 300
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.86465072631836
Total epoch: 1. epoch loss: 16.050159454345703
Total epoch: 2. epoch loss: 15.327302932739258
Total epoch: 3. epoch loss: 14.716885566711426
Total epoch: 4. epoch loss: 14.208922386169434
Total epoch: 5. epoch loss: 13.779679298400879
Total epoch: 6. epoch loss: 13.405447959899902
Total epoch: 7. epoch loss: 13.0675630569458
Total epoch: 8. epoch loss: 12.753329277038574
Total epoch: 9. epoch loss: 12.455122947692871
Total epoch: 10. epoch loss: 12.1690092086792
Total epoch: 11. epoch loss: 11.893647193908691
Total epoch: 12. epoch loss: 11.629380226135254
Total epoch: 13. epoch loss: 11.377534866333008
Total epoch: 14. epoch loss: 11.139726638793945
Total epoch: 15. epoch loss: 10.917407989501953
Total epoch: 16. epoch loss: 10.711431503295898
Total epoch: 17. epoch loss: 10.522065162658691
Total epoch: 18. epoch loss: 10.348962783813477
Total epoch: 19. epoch loss: 10.19091510772705
Total epoch: 20. epoch loss: 10.045771598815918
Total epoch: 21. epoch loss: 9.910831451416016
Total epoch: 22. epoch loss: 9.783525466918945
Total epoch: 23. epoch loss: 9.66211986541748
Total epoch: 24. epoch loss: 9.545928955078125
Total epoch: 25. epoch loss: 9.434955596923828
Total epoch: 26. epoch loss: 9.32935905456543
Total epoch: 27. epoch loss: 9.230600357055664
Total epoch: 28. epoch loss: 9.139565467834473
Total epoch: 29. epoch loss: 9.056081771850586
Total epoch: 30. epoch loss: 8.979391098022461
Total epoch: 31. epoch loss: 8.907360076904297
Total epoch: 32. epoch loss: 8.837115287780762
Total epoch: 33. epoch loss: 8.767729759216309
Total epoch: 34. epoch loss: 8.700343132019043
Total epoch: 35. epoch loss: 8.63650894165039
Total epoch: 36. epoch loss: 8.576822280883789
Total epoch: 37. epoch loss: 8.520753860473633
Total epoch: 38. epoch loss: 8.467334747314453
Total epoch: 39. epoch loss: 8.415778160095215
Total epoch: 40. epoch loss: 8.365478515625
Total epoch: 41. epoch loss: 8.316121101379395
Total epoch: 42. epoch loss: 8.267953872680664
Total epoch: 43. epoch loss: 8.221427917480469
Total epoch: 44. epoch loss: 8.176915168762207
Total epoch: 45. epoch loss: 8.134550094604492
Total epoch: 46. epoch loss: 8.093825340270996
Total epoch: 47. epoch loss: 8.05399227142334
Total epoch: 48. epoch loss: 8.01504135131836
Total epoch: 49. epoch loss: 7.977149486541748
Total epoch: 50. epoch loss: 7.9400835037231445
Total epoch: 51. epoch loss: 7.904041767120361
Total epoch: 52. epoch loss: 7.86942195892334
Total epoch: 53. epoch loss: 7.836048126220703
Total epoch: 54. epoch loss: 7.803439140319824
Total epoch: 55. epoch loss: 7.771259307861328
Total epoch: 56. epoch loss: 7.7394866943359375
Total epoch: 57. epoch loss: 7.708558559417725
Total epoch: 58. epoch loss: 7.679049015045166
Total epoch: 59. epoch loss: 7.6502766609191895
Total epoch: 60. epoch loss: 7.621825218200684
Total epoch: 61. epoch loss: 7.594021797180176
Total epoch: 62. epoch loss: 7.567081928253174
Total epoch: 63. epoch loss: 7.541035175323486
Total epoch: 64. epoch loss: 7.515852928161621
Total epoch: 65. epoch loss: 7.491133689880371
Total epoch: 66. epoch loss: 7.466738700866699
Total epoch: 67. epoch loss: 7.4427490234375
Total epoch: 68. epoch loss: 7.41944694519043
Total epoch: 69. epoch loss: 7.396629333496094
Total epoch: 70. epoch loss: 7.374639511108398
Total epoch: 71. epoch loss: 7.353147983551025
Total epoch: 72. epoch loss: 7.3320465087890625
Total epoch: 73. epoch loss: 7.311093807220459
Total epoch: 74. epoch loss: 7.290765285491943
Total epoch: 75. epoch loss: 7.271472454071045
Total epoch: 76. epoch loss: 7.2525739669799805
Total epoch: 77. epoch loss: 7.233421802520752
Total epoch: 78. epoch loss: 7.214523792266846
Total epoch: 79. epoch loss: 7.196375370025635
Total epoch: 80. epoch loss: 7.178968906402588
Total epoch: 81. epoch loss: 7.162209987640381
Total epoch: 82. epoch loss: 7.144143581390381
Total epoch: 83. epoch loss: 7.128345966339111
Total epoch: 84. epoch loss: 7.111248016357422
Total epoch: 85. epoch loss: 7.096970558166504
Total epoch: 86. epoch loss: 7.079039096832275
Total epoch: 87. epoch loss: 7.067525386810303
Total epoch: 88. epoch loss: 7.049453258514404
Total epoch: 89. epoch loss: 7.040615558624268
Total epoch: 90. epoch loss: 7.023781776428223
Total epoch: 91. epoch loss: 7.014580726623535
Total epoch: 92. epoch loss: 7.001420497894287
Total epoch: 93. epoch loss: 6.98569393157959
Total epoch: 94. epoch loss: 6.974172115325928
Total epoch: 95. epoch loss: 6.961129665374756
Total epoch: 96. epoch loss: 6.94846248626709
Total epoch: 97. epoch loss: 6.937020778656006
Total epoch: 98. epoch loss: 6.924056529998779
Total epoch: 99. epoch loss: 6.914079189300537
Total epoch: 99. DecT loss: 6.914079189300537
Training time: 0.572166919708252
APL_precision: 0.19507186858316222, APL_recall: 0.5588235294117647, APL_f1: 0.289193302891933, APL_number: 170
CMT_precision: 0.2829736211031175, CMT_recall: 0.6051282051282051, CMT_f1: 0.38562091503267976, CMT_number: 195
DSC_precision: 0.44299065420560746, DSC_recall: 0.5423340961098398, DSC_f1: 0.4876543209876543, DSC_number: 437
MAT_precision: 0.683495145631068, MAT_recall: 0.5161290322580645, MAT_f1: 0.5881370091896408, MAT_number: 682
PRO_precision: 0.5100942126514132, PRO_recall: 0.4915693904020752, PRO_f1: 0.5006605019815059, PRO_number: 771
SMT_precision: 0.3615819209039548, SMT_recall: 0.3742690058479532, SMT_f1: 0.367816091954023, SMT_number: 171
SPL_precision: 0.5319148936170213, SPL_recall: 0.3333333333333333, SPL_f1: 0.4098360655737705, SPL_number: 75
overall_precision: 0.43478260869565216, overall_recall: 0.507796881247501, overall_f1: 0.46846182220582805, overall_accuracy: 0.8096633550139375
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:02<?, ?it/s]