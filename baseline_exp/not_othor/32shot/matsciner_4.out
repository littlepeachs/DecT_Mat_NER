/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:44 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:45 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1154.66it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:47 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-504cb67db8976d98/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0164a92bba32f475.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:51 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:51 - INFO - __main__ -   Num examples = 72
06/01/2023 14:48:51 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:51 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:51 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:51 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:51 - INFO - __main__ -   Total optimization steps = 300
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 17.4029598236084
Total epoch: 1. epoch loss: 16.628326416015625
Total epoch: 2. epoch loss: 15.915217399597168
Total epoch: 3. epoch loss: 15.285307884216309
Total epoch: 4. epoch loss: 14.741729736328125
Total epoch: 5. epoch loss: 14.271169662475586
Total epoch: 6. epoch loss: 13.854368209838867
Total epoch: 7. epoch loss: 13.475224494934082
Total epoch: 8. epoch loss: 13.123053550720215
Total epoch: 9. epoch loss: 12.791869163513184
Total epoch: 10. epoch loss: 12.479240417480469
Total epoch: 11. epoch loss: 12.185304641723633
Total epoch: 12. epoch loss: 11.91117000579834
Total epoch: 13. epoch loss: 11.657857894897461
Total epoch: 14. epoch loss: 11.425536155700684
Total epoch: 15. epoch loss: 11.21343994140625
Total epoch: 16. epoch loss: 11.020331382751465
Total epoch: 17. epoch loss: 10.844853401184082
Total epoch: 18. epoch loss: 10.684879302978516
Total epoch: 19. epoch loss: 10.537343978881836
Total epoch: 20. epoch loss: 10.399232864379883
Total epoch: 21. epoch loss: 10.268573760986328
Total epoch: 22. epoch loss: 10.144561767578125
Total epoch: 23. epoch loss: 10.027180671691895
Total epoch: 24. epoch loss: 9.916585922241211
Total epoch: 25. epoch loss: 9.812602996826172
Total epoch: 26. epoch loss: 9.715940475463867
Total epoch: 27. epoch loss: 9.625826835632324
Total epoch: 28. epoch loss: 9.540548324584961
Total epoch: 29. epoch loss: 9.45860767364502
Total epoch: 30. epoch loss: 9.379590034484863
Total epoch: 31. epoch loss: 9.304071426391602
Total epoch: 32. epoch loss: 9.232819557189941
Total epoch: 33. epoch loss: 9.16588020324707
Total epoch: 34. epoch loss: 9.102384567260742
Total epoch: 35. epoch loss: 9.04116439819336
Total epoch: 36. epoch loss: 8.981629371643066
Total epoch: 37. epoch loss: 8.924107551574707
Total epoch: 38. epoch loss: 8.869297981262207
Total epoch: 39. epoch loss: 8.817428588867188
Total epoch: 40. epoch loss: 8.76799488067627
Total epoch: 41. epoch loss: 8.720230102539062
Total epoch: 42. epoch loss: 8.673699378967285
Total epoch: 43. epoch loss: 8.628445625305176
Total epoch: 44. epoch loss: 8.584670066833496
Total epoch: 45. epoch loss: 8.542442321777344
Total epoch: 46. epoch loss: 8.501575469970703
Total epoch: 47. epoch loss: 8.461828231811523
Total epoch: 48. epoch loss: 8.423151969909668
Total epoch: 49. epoch loss: 8.38556957244873
Total epoch: 50. epoch loss: 8.349037170410156
Total epoch: 51. epoch loss: 8.31335735321045
Total epoch: 52. epoch loss: 8.278433799743652
Total epoch: 53. epoch loss: 8.244340896606445
Total epoch: 54. epoch loss: 8.211095809936523
Total epoch: 55. epoch loss: 8.178576469421387
Total epoch: 56. epoch loss: 8.146916389465332
Total epoch: 57. epoch loss: 8.116337776184082
Total epoch: 58. epoch loss: 8.086572647094727
Total epoch: 59. epoch loss: 8.057424545288086
Total epoch: 60. epoch loss: 8.029067039489746
Total epoch: 61. epoch loss: 8.001304626464844
Total epoch: 62. epoch loss: 7.974028587341309
Total epoch: 63. epoch loss: 7.947484970092773
Total epoch: 64. epoch loss: 7.921565532684326
Total epoch: 65. epoch loss: 7.896343231201172
Total epoch: 66. epoch loss: 7.871386528015137
Total epoch: 67. epoch loss: 7.846826553344727
Total epoch: 68. epoch loss: 7.823005199432373
Total epoch: 69. epoch loss: 7.800002098083496
Total epoch: 70. epoch loss: 7.777310848236084
Total epoch: 71. epoch loss: 7.755045413970947
Total epoch: 72. epoch loss: 7.733556747436523
Total epoch: 73. epoch loss: 7.7118988037109375
Total epoch: 74. epoch loss: 7.690727233886719
Total epoch: 75. epoch loss: 7.67031192779541
Total epoch: 76. epoch loss: 7.650073051452637
Total epoch: 77. epoch loss: 7.63006067276001
Total epoch: 78. epoch loss: 7.610553741455078
Total epoch: 79. epoch loss: 7.591746807098389
Total epoch: 80. epoch loss: 7.5732831954956055
Total epoch: 81. epoch loss: 7.555032253265381
Total epoch: 82. epoch loss: 7.536973476409912
Total epoch: 83. epoch loss: 7.519996643066406
Total epoch: 84. epoch loss: 7.503098487854004
Total epoch: 85. epoch loss: 7.486534595489502
Total epoch: 86. epoch loss: 7.470897674560547
Total epoch: 87. epoch loss: 7.454216003417969
Total epoch: 88. epoch loss: 7.438526630401611
Total epoch: 89. epoch loss: 7.423872947692871
Total epoch: 90. epoch loss: 7.408227920532227
Total epoch: 91. epoch loss: 7.395615100860596
Total epoch: 92. epoch loss: 7.37900447845459
Total epoch: 93. epoch loss: 7.368592262268066
Total epoch: 94. epoch loss: 7.351241588592529
Total epoch: 95. epoch loss: 7.3445658683776855
Total epoch: 96. epoch loss: 7.327559947967529
Total epoch: 97. epoch loss: 7.323193073272705
Total epoch: 98. epoch loss: 7.313248634338379
Total epoch: 99. epoch loss: 7.2935309410095215
Total epoch: 99. DecT loss: 7.2935309410095215
Training time: 0.5331864356994629
APL_precision: 0.3, APL_recall: 0.5117647058823529, APL_f1: 0.3782608695652173, APL_number: 170
CMT_precision: 0.41924398625429554, CMT_recall: 0.6256410256410256, CMT_f1: 0.5020576131687243, CMT_number: 195
DSC_precision: 0.46579804560260585, DSC_recall: 0.6544622425629291, DSC_f1: 0.544243577545195, DSC_number: 437
MAT_precision: 0.6316666666666667, MAT_recall: 0.5557184750733137, MAT_f1: 0.5912636505460218, MAT_number: 682
PRO_precision: 0.4695121951219512, PRO_recall: 0.4993514915693904, PRO_f1: 0.48397234443746073, PRO_number: 771
SMT_precision: 0.38860103626943004, SMT_recall: 0.43859649122807015, SMT_f1: 0.41208791208791207, SMT_number: 171
SPL_precision: 0.4044943820224719, SPL_recall: 0.48, SPL_f1: 0.43902439024390244, SPL_number: 75
overall_precision: 0.47290300310666206, overall_recall: 0.547780887644942, overall_f1: 0.5075954057058171, overall_accuracy: 0.8316775069687656
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:03<?, ?it/s]