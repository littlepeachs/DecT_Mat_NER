/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:44 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:45 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-21be78dd80d291c5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 50
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1179.50it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:47 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-21be78dd80d291c5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9711e90455b24fbf.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:51 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:51 - INFO - __main__ -   Num examples = 66
06/01/2023 14:48:51 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:51 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:51 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:51 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:51 - INFO - __main__ -   Total optimization steps = 300
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.4688720703125
Total epoch: 1. epoch loss: 15.7005033493042
Total epoch: 2. epoch loss: 15.003697395324707
Total epoch: 3. epoch loss: 14.395231246948242
Total epoch: 4. epoch loss: 13.871675491333008
Total epoch: 5. epoch loss: 13.416937828063965
Total epoch: 6. epoch loss: 13.011980056762695
Total epoch: 7. epoch loss: 12.64106559753418
Total epoch: 8. epoch loss: 12.293859481811523
Total epoch: 9. epoch loss: 11.96481990814209
Total epoch: 10. epoch loss: 11.651829719543457
Total epoch: 11. epoch loss: 11.355155944824219
Total epoch: 12. epoch loss: 11.076349258422852
Total epoch: 13. epoch loss: 10.817282676696777
Total epoch: 14. epoch loss: 10.579275131225586
Total epoch: 15. epoch loss: 10.362597465515137
Total epoch: 16. epoch loss: 10.166600227355957
Total epoch: 17. epoch loss: 9.989789962768555
Total epoch: 18. epoch loss: 9.82983684539795
Total epoch: 19. epoch loss: 9.68355655670166
Total epoch: 20. epoch loss: 9.547439575195312
Total epoch: 21. epoch loss: 9.418580055236816
Total epoch: 22. epoch loss: 9.295341491699219
Total epoch: 23. epoch loss: 9.177305221557617
Total epoch: 24. epoch loss: 9.064640998840332
Total epoch: 25. epoch loss: 8.957465171813965
Total epoch: 26. epoch loss: 8.857155799865723
Total epoch: 27. epoch loss: 8.76352596282959
Total epoch: 28. epoch loss: 8.675379753112793
Total epoch: 29. epoch loss: 8.592164993286133
Total epoch: 30. epoch loss: 8.514949798583984
Total epoch: 31. epoch loss: 8.44450855255127
Total epoch: 32. epoch loss: 8.376615524291992
Total epoch: 33. epoch loss: 8.307330131530762
Total epoch: 34. epoch loss: 8.240127563476562
Total epoch: 35. epoch loss: 8.179248809814453
Total epoch: 36. epoch loss: 8.124406814575195
Total epoch: 37. epoch loss: 8.07315731048584
Total epoch: 38. epoch loss: 8.023810386657715
Total epoch: 39. epoch loss: 7.975720405578613
Total epoch: 40. epoch loss: 7.9287261962890625
Total epoch: 41. epoch loss: 7.882983207702637
Total epoch: 42. epoch loss: 7.838887691497803
Total epoch: 43. epoch loss: 7.796861171722412
Total epoch: 44. epoch loss: 7.7570295333862305
Total epoch: 45. epoch loss: 7.71890926361084
Total epoch: 46. epoch loss: 7.681704521179199
Total epoch: 47. epoch loss: 7.645005702972412
Total epoch: 48. epoch loss: 7.60891580581665
Total epoch: 49. epoch loss: 7.573862075805664
Total epoch: 50. epoch loss: 7.54018497467041
Total epoch: 51. epoch loss: 7.5077643394470215
Total epoch: 52. epoch loss: 7.476200580596924
Total epoch: 53. epoch loss: 7.445184230804443
Total epoch: 54. epoch loss: 7.414702415466309
Total epoch: 55. epoch loss: 7.384978294372559
Total epoch: 56. epoch loss: 7.356186389923096
Total epoch: 57. epoch loss: 7.328349590301514
Total epoch: 58. epoch loss: 7.30133581161499
Total epoch: 59. epoch loss: 7.274824619293213
Total epoch: 60. epoch loss: 7.2487969398498535
Total epoch: 61. epoch loss: 7.223506927490234
Total epoch: 62. epoch loss: 7.199065208435059
Total epoch: 63. epoch loss: 7.175220489501953
Total epoch: 64. epoch loss: 7.151834011077881
Total epoch: 65. epoch loss: 7.1289381980896
Total epoch: 66. epoch loss: 7.106808185577393
Total epoch: 67. epoch loss: 7.085226058959961
Total epoch: 68. epoch loss: 7.064057350158691
Total epoch: 69. epoch loss: 7.043926239013672
Total epoch: 70. epoch loss: 7.024424076080322
Total epoch: 71. epoch loss: 7.004728317260742
Total epoch: 72. epoch loss: 6.98563814163208
Total epoch: 73. epoch loss: 6.967568397521973
Total epoch: 74. epoch loss: 6.949052333831787
Total epoch: 75. epoch loss: 6.931157112121582
Total epoch: 76. epoch loss: 6.913752555847168
Total epoch: 77. epoch loss: 6.896495819091797
Total epoch: 78. epoch loss: 6.879644393920898
Total epoch: 79. epoch loss: 6.863079071044922
Total epoch: 80. epoch loss: 6.847696781158447
Total epoch: 81. epoch loss: 6.832842826843262
Total epoch: 82. epoch loss: 6.817160129547119
Total epoch: 83. epoch loss: 6.802014350891113
Total epoch: 84. epoch loss: 6.787530422210693
Total epoch: 85. epoch loss: 6.772669315338135
Total epoch: 86. epoch loss: 6.759584426879883
Total epoch: 87. epoch loss: 6.744248867034912
Total epoch: 88. epoch loss: 6.732623100280762
Total epoch: 89. epoch loss: 6.717463493347168
Total epoch: 90. epoch loss: 6.708616256713867
Total epoch: 91. epoch loss: 6.692120552062988
Total epoch: 92. epoch loss: 6.688296794891357
Total epoch: 93. epoch loss: 6.673769950866699
Total epoch: 94. epoch loss: 6.663679599761963
Total epoch: 95. epoch loss: 6.653099060058594
Total epoch: 96. epoch loss: 6.639793872833252
Total epoch: 97. epoch loss: 6.630861759185791
Total epoch: 98. epoch loss: 6.619204044342041
Total epoch: 99. epoch loss: 6.610579490661621
Total epoch: 99. DecT loss: 6.610579490661621
Training time: 0.5332326889038086
APL_precision: 0.28391167192429023, APL_recall: 0.5294117647058824, APL_f1: 0.36960985626283366, APL_number: 170
CMT_precision: 0.3203342618384401, CMT_recall: 0.5897435897435898, CMT_f1: 0.4151624548736462, CMT_number: 195
DSC_precision: 0.43174603174603177, DSC_recall: 0.6224256292906178, DSC_f1: 0.5098406747891284, DSC_number: 437
MAT_precision: 0.626984126984127, MAT_recall: 0.4633431085043988, MAT_f1: 0.5328836424957841, MAT_number: 682
PRO_precision: 0.5033112582781457, PRO_recall: 0.3942931258106355, PRO_f1: 0.4421818181818182, PRO_number: 771
SMT_precision: 0.3791208791208791, SMT_recall: 0.40350877192982454, SMT_f1: 0.3909348441926345, SMT_number: 171
SPL_precision: 0.6086956521739131, SPL_recall: 0.37333333333333335, SPL_f1: 0.46280991735537197, SPL_number: 75
overall_precision: 0.4519303557910674, overall_recall: 0.4774090363854458, overall_f1: 0.46432043554345714, overall_accuracy: 0.8187406189693375
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:02<?, ?it/s]