/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:09 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:11 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1139.60it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0da191125b5a9cf3.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4724.13 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:16 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:16 - INFO - __main__ -   Num examples = 21
06/01/2023 14:48:16 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:16 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:16 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:16 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:16 - INFO - __main__ -   Total optimization steps = 100
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.66894817352295
Total epoch: 1. epoch loss: 14.796545028686523
Total epoch: 2. epoch loss: 13.92495346069336
Total epoch: 3. epoch loss: 13.097169876098633
Total epoch: 4. epoch loss: 12.350247383117676
Total epoch: 5. epoch loss: 11.690853118896484
Total epoch: 6. epoch loss: 11.112652778625488
Total epoch: 7. epoch loss: 10.606247901916504
Total epoch: 8. epoch loss: 10.158653259277344
Total epoch: 9. epoch loss: 9.757577896118164
Total epoch: 10. epoch loss: 9.393787384033203
Total epoch: 11. epoch loss: 9.062451362609863
Total epoch: 12. epoch loss: 8.761173248291016
Total epoch: 13. epoch loss: 8.488344192504883
Total epoch: 14. epoch loss: 8.242777824401855
Total epoch: 15. epoch loss: 8.023904800415039
Total epoch: 16. epoch loss: 7.830671310424805
Total epoch: 17. epoch loss: 7.660190582275391
Total epoch: 18. epoch loss: 7.508035659790039
Total epoch: 19. epoch loss: 7.370185375213623
Total epoch: 20. epoch loss: 7.244166374206543
Total epoch: 21. epoch loss: 7.128463268280029
Total epoch: 22. epoch loss: 7.022475242614746
Total epoch: 23. epoch loss: 6.925589084625244
Total epoch: 24. epoch loss: 6.83695650100708
Total epoch: 25. epoch loss: 6.756661891937256
Total epoch: 26. epoch loss: 6.685630798339844
Total epoch: 27. epoch loss: 6.624171257019043
Total epoch: 28. epoch loss: 6.570023536682129
Total epoch: 29. epoch loss: 6.517945766448975
Total epoch: 30. epoch loss: 6.467156887054443
Total epoch: 31. epoch loss: 6.421908855438232
Total epoch: 32. epoch loss: 6.3824782371521
Total epoch: 33. epoch loss: 6.346411228179932
Total epoch: 34. epoch loss: 6.310984134674072
Total epoch: 35. epoch loss: 6.276843070983887
Total epoch: 36. epoch loss: 6.244216442108154
Total epoch: 37. epoch loss: 6.211526393890381
Total epoch: 38. epoch loss: 6.178661823272705
Total epoch: 39. epoch loss: 6.147861957550049
Total epoch: 40. epoch loss: 6.118937015533447
Total epoch: 41. epoch loss: 6.091281414031982
Total epoch: 42. epoch loss: 6.063930034637451
Total epoch: 43. epoch loss: 6.038306713104248
Total epoch: 44. epoch loss: 6.014095783233643
Total epoch: 45. epoch loss: 5.9901862144470215
Total epoch: 46. epoch loss: 5.967830181121826
Total epoch: 47. epoch loss: 5.94543981552124
Total epoch: 48. epoch loss: 5.9237847328186035
Total epoch: 49. epoch loss: 5.904287338256836
Total epoch: 50. epoch loss: 5.886481285095215
Total epoch: 51. epoch loss: 5.868812561035156
Total epoch: 52. epoch loss: 5.851703643798828
Total epoch: 53. epoch loss: 5.836445331573486
Total epoch: 54. epoch loss: 5.82127571105957
Total epoch: 55. epoch loss: 5.807322978973389
Total epoch: 56. epoch loss: 5.792532444000244
Total epoch: 57. epoch loss: 5.7789411544799805
Total epoch: 58. epoch loss: 5.7648701667785645
Total epoch: 59. epoch loss: 5.753602504730225
Total epoch: 60. epoch loss: 5.740499496459961
Total epoch: 61. epoch loss: 5.729870796203613
Total epoch: 62. epoch loss: 5.71705961227417
Total epoch: 63. epoch loss: 5.70790958404541
Total epoch: 64. epoch loss: 5.695929527282715
Total epoch: 65. epoch loss: 5.689031600952148
Total epoch: 66. epoch loss: 5.677425384521484
Total epoch: 67. epoch loss: 5.670299053192139
Total epoch: 68. epoch loss: 5.662158012390137
Total epoch: 69. epoch loss: 5.651242733001709
Total epoch: 70. epoch loss: 5.645607948303223
Total epoch: 71. epoch loss: 5.635401248931885
Total epoch: 72. epoch loss: 5.627964496612549
Total epoch: 73. epoch loss: 5.6209516525268555
Total epoch: 74. epoch loss: 5.613063335418701
Total epoch: 75. epoch loss: 5.607250690460205
Total epoch: 76. epoch loss: 5.600584030151367
Total epoch: 77. epoch loss: 5.594519138336182
Total epoch: 78. epoch loss: 5.58927583694458
Total epoch: 79. epoch loss: 5.582959175109863
Total epoch: 80. epoch loss: 5.577633857727051
Total epoch: 81. epoch loss: 5.572207927703857
Total epoch: 82. epoch loss: 5.567349910736084
Total epoch: 83. epoch loss: 5.561676502227783
Total epoch: 84. epoch loss: 5.560836315155029
Total epoch: 85. epoch loss: 5.5516743659973145
Total epoch: 86. epoch loss: 5.5586256980896
Total epoch: 87. epoch loss: 5.54904842376709
Total epoch: 88. epoch loss: 5.558085918426514
Total epoch: 89. epoch loss: 5.5549774169921875
Total epoch: 90. epoch loss: 5.5472540855407715
Total epoch: 91. epoch loss: 5.540981292724609
Total epoch: 92. epoch loss: 5.540004253387451
Total epoch: 93. epoch loss: 5.533578395843506
Total epoch: 94. epoch loss: 5.532180309295654
Total epoch: 95. epoch loss: 5.527613639831543
Total epoch: 96. epoch loss: 5.525203227996826
Total epoch: 97. epoch loss: 5.5212578773498535
Total epoch: 98. epoch loss: 5.518179416656494
Total epoch: 99. epoch loss: 5.514624118804932
Total epoch: 99. DecT loss: 5.514624118804932
Training time: 0.4327850341796875
APL_precision: 0.2471264367816092, APL_recall: 0.2529411764705882, APL_f1: 0.25, APL_number: 170
CMT_precision: 0.07398568019093078, CMT_recall: 0.31794871794871793, CMT_f1: 0.12003872216844143, CMT_number: 195
DSC_precision: 0.4152823920265781, DSC_recall: 0.28604118993135014, DSC_f1: 0.33875338753387535, DSC_number: 437
MAT_precision: 0.5444444444444444, MAT_recall: 0.2873900293255132, MAT_f1: 0.37619961612284064, MAT_number: 682
PRO_precision: 0.325, PRO_recall: 0.13488975356679636, PRO_f1: 0.1906507791017415, PRO_number: 771
SMT_precision: 0.19318181818181818, SMT_recall: 0.2982456140350877, SMT_f1: 0.23448275862068962, SMT_number: 171
SPL_precision: 0.6382978723404256, SPL_recall: 0.4, SPL_f1: 0.4918032786885246, SPL_number: 75
overall_precision: 0.2651909722222222, overall_recall: 0.24430227908836466, overall_f1: 0.254318418314256, overall_accuracy: 0.741190765492102
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:01<?, ?it/s]