/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:10 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:11 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-18e93fae9dc4599b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1181.49it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-18e93fae9dc4599b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-97557a84e28fac49.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:17 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:17 - INFO - __main__ -   Num examples = 21
06/01/2023 14:48:17 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:17 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:17 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:17 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:17 - INFO - __main__ -   Total optimization steps = 100
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.21102523803711
Total epoch: 1. epoch loss: 15.318197250366211
Total epoch: 2. epoch loss: 14.438740730285645
Total epoch: 3. epoch loss: 13.601417541503906
Total epoch: 4. epoch loss: 12.828664779663086
Total epoch: 5. epoch loss: 12.136031150817871
Total epoch: 6. epoch loss: 11.534451484680176
Total epoch: 7. epoch loss: 11.024243354797363
Total epoch: 8. epoch loss: 10.591832160949707
Total epoch: 9. epoch loss: 10.22105598449707
Total epoch: 10. epoch loss: 9.896490097045898
Total epoch: 11. epoch loss: 9.60629940032959
Total epoch: 12. epoch loss: 9.343347549438477
Total epoch: 13. epoch loss: 9.103713035583496
Total epoch: 14. epoch loss: 8.885251998901367
Total epoch: 15. epoch loss: 8.686567306518555
Total epoch: 16. epoch loss: 8.505875587463379
Total epoch: 17. epoch loss: 8.34040355682373
Total epoch: 18. epoch loss: 8.1870698928833
Total epoch: 19. epoch loss: 8.043293952941895
Total epoch: 20. epoch loss: 7.907505035400391
Total epoch: 21. epoch loss: 7.7790303230285645
Total epoch: 22. epoch loss: 7.65898323059082
Total epoch: 23. epoch loss: 7.5477423667907715
Total epoch: 24. epoch loss: 7.4448676109313965
Total epoch: 25. epoch loss: 7.3491530418396
Total epoch: 26. epoch loss: 7.259070873260498
Total epoch: 27. epoch loss: 7.174144268035889
Total epoch: 28. epoch loss: 7.094858169555664
Total epoch: 29. epoch loss: 7.021514415740967
Total epoch: 30. epoch loss: 6.953981399536133
Total epoch: 31. epoch loss: 6.891388893127441
Total epoch: 32. epoch loss: 6.833341121673584
Total epoch: 33. epoch loss: 6.779841423034668
Total epoch: 34. epoch loss: 6.729925155639648
Total epoch: 35. epoch loss: 6.682897567749023
Total epoch: 36. epoch loss: 6.638792991638184
Total epoch: 37. epoch loss: 6.597200870513916
Total epoch: 38. epoch loss: 6.558142185211182
Total epoch: 39. epoch loss: 6.520593166351318
Total epoch: 40. epoch loss: 6.485254764556885
Total epoch: 41. epoch loss: 6.451907634735107
Total epoch: 42. epoch loss: 6.420167922973633
Total epoch: 43. epoch loss: 6.389528751373291
Total epoch: 44. epoch loss: 6.359923362731934
Total epoch: 45. epoch loss: 6.331446647644043
Total epoch: 46. epoch loss: 6.304325580596924
Total epoch: 47. epoch loss: 6.278120517730713
Total epoch: 48. epoch loss: 6.2533698081970215
Total epoch: 49. epoch loss: 6.2293596267700195
Total epoch: 50. epoch loss: 6.207157135009766
Total epoch: 51. epoch loss: 6.186853408813477
Total epoch: 52. epoch loss: 6.167133331298828
Total epoch: 53. epoch loss: 6.148073196411133
Total epoch: 54. epoch loss: 6.129207611083984
Total epoch: 55. epoch loss: 6.11161994934082
Total epoch: 56. epoch loss: 6.095974922180176
Total epoch: 57. epoch loss: 6.0817155838012695
Total epoch: 58. epoch loss: 6.069003582000732
Total epoch: 59. epoch loss: 6.054948806762695
Total epoch: 60. epoch loss: 6.042065620422363
Total epoch: 61. epoch loss: 6.028940200805664
Total epoch: 62. epoch loss: 6.017719745635986
Total epoch: 63. epoch loss: 6.006863117218018
Total epoch: 64. epoch loss: 5.994791507720947
Total epoch: 65. epoch loss: 5.986301422119141
Total epoch: 66. epoch loss: 5.973422527313232
Total epoch: 67. epoch loss: 5.965303421020508
Total epoch: 68. epoch loss: 5.954071044921875
Total epoch: 69. epoch loss: 5.945804595947266
Total epoch: 70. epoch loss: 5.9374918937683105
Total epoch: 71. epoch loss: 5.928234577178955
Total epoch: 72. epoch loss: 5.9212446212768555
Total epoch: 73. epoch loss: 5.912189960479736
Total epoch: 74. epoch loss: 5.9069600105285645
Total epoch: 75. epoch loss: 5.898693084716797
Total epoch: 76. epoch loss: 5.894275665283203
Total epoch: 77. epoch loss: 5.887239456176758
Total epoch: 78. epoch loss: 5.882389068603516
Total epoch: 79. epoch loss: 5.876742839813232
Total epoch: 80. epoch loss: 5.870556354522705
Total epoch: 81. epoch loss: 5.866217613220215
Total epoch: 82. epoch loss: 5.8607048988342285
Total epoch: 83. epoch loss: 5.855044841766357
Total epoch: 84. epoch loss: 5.850824356079102
Total epoch: 85. epoch loss: 5.845914840698242
Total epoch: 86. epoch loss: 5.841691493988037
Total epoch: 87. epoch loss: 5.836904525756836
Total epoch: 88. epoch loss: 5.833428382873535
Total epoch: 89. epoch loss: 5.82802152633667
Total epoch: 90. epoch loss: 5.826206684112549
Total epoch: 91. epoch loss: 5.820272922515869
Total epoch: 92. epoch loss: 5.819433689117432
Total epoch: 93. epoch loss: 5.81466817855835
Total epoch: 94. epoch loss: 5.811737060546875
Total epoch: 95. epoch loss: 5.809364318847656
Total epoch: 96. epoch loss: 5.804064750671387
Total epoch: 97. epoch loss: 5.805784225463867
Total epoch: 98. epoch loss: 5.797393798828125
Total epoch: 99. epoch loss: 5.804819583892822
Total epoch: 99. DecT loss: 5.804819583892822
Training time: 0.446897029876709
APL_precision: 0.23655913978494625, APL_recall: 0.38823529411764707, APL_f1: 0.29398663697104677, APL_number: 170
CMT_precision: 0.13450292397660818, CMT_recall: 0.2358974358974359, CMT_f1: 0.17132216014897578, CMT_number: 195
DSC_precision: 0.36042402826855124, DSC_recall: 0.2334096109839817, DSC_f1: 0.2833333333333333, DSC_number: 437
MAT_precision: 0.5187861271676301, MAT_recall: 0.5263929618768328, MAT_f1: 0.5225618631732168, MAT_number: 682
PRO_precision: 0.2957446808510638, PRO_recall: 0.18028534370946822, PRO_f1: 0.22401289282836423, PRO_number: 771
SMT_precision: 0.31645569620253167, SMT_recall: 0.14619883040935672, SMT_f1: 0.2, SMT_number: 171
SPL_precision: 0.5, SPL_recall: 0.29333333333333333, SPL_f1: 0.3697478991596639, SPL_number: 75
overall_precision: 0.34673366834170855, overall_recall: 0.3034786085565774, overall_f1: 0.3236673773987207, overall_accuracy: 0.7697090987063112
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:02<?, ?it/s]