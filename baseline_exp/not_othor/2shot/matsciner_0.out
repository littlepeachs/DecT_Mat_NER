/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:47:34 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:47:35 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3df0612bcc87014e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:2, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 32
model_logits_weight: 20
num_train_epochs: 35
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1197.35it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:47:37 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-3df0612bcc87014e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90e7c5352129796f.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5395.14 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:47:41 - INFO - __main__ - ***** Running training *****
06/01/2023 14:47:41 - INFO - __main__ -   Num examples = 8
06/01/2023 14:47:41 - INFO - __main__ -   Num Epochs = 35
06/01/2023 14:47:41 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:47:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:47:41 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:47:41 - INFO - __main__ -   Total optimization steps = 35
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 14.728363990783691
Total epoch: 1. epoch loss: 14.092371940612793
Total epoch: 2. epoch loss: 13.447834014892578
Total epoch: 3. epoch loss: 12.802257537841797
Total epoch: 4. epoch loss: 12.16366958618164
Total epoch: 5. epoch loss: 11.540133476257324
Total epoch: 6. epoch loss: 10.940549850463867
Total epoch: 7. epoch loss: 10.374811172485352
Total epoch: 8. epoch loss: 9.853135108947754
Total epoch: 9. epoch loss: 9.384475708007812
Total epoch: 10. epoch loss: 8.974699974060059
Total epoch: 11. epoch loss: 8.624255180358887
Total epoch: 12. epoch loss: 8.32675552368164
Total epoch: 13. epoch loss: 8.073675155639648
Total epoch: 14. epoch loss: 7.8587212562561035
Total epoch: 15. epoch loss: 7.676324367523193
Total epoch: 16. epoch loss: 7.518640041351318
Total epoch: 17. epoch loss: 7.377375602722168
Total epoch: 18. epoch loss: 7.246767997741699
Total epoch: 19. epoch loss: 7.124183177947998
Total epoch: 20. epoch loss: 7.009037017822266
Total epoch: 21. epoch loss: 6.901545524597168
Total epoch: 22. epoch loss: 6.8015618324279785
Total epoch: 23. epoch loss: 6.708330154418945
Total epoch: 24. epoch loss: 6.621010780334473
Total epoch: 25. epoch loss: 6.539096832275391
Total epoch: 26. epoch loss: 6.462461948394775
Total epoch: 27. epoch loss: 6.391168594360352
Total epoch: 28. epoch loss: 6.325316429138184
Total epoch: 29. epoch loss: 6.264716625213623
Total epoch: 30. epoch loss: 6.208824634552002
Total epoch: 31. epoch loss: 6.156853199005127
Total epoch: 32. epoch loss: 6.108216285705566
Total epoch: 33. epoch loss: 6.062687873840332
Total epoch: 34. epoch loss: 6.019878387451172
Total epoch: 34. DecT loss: 6.019878387451172
Training time: 0.16628193855285645
APL_precision: 0.10909090909090909, APL_recall: 0.17647058823529413, APL_f1: 0.1348314606741573, APL_number: 170
CMT_precision: 0.11467889908256881, CMT_recall: 0.1282051282051282, CMT_f1: 0.12106537530266344, CMT_number: 195
DSC_precision: 0.28888888888888886, DSC_recall: 0.20823798627002288, DSC_f1: 0.24202127659574468, DSC_number: 437
MAT_precision: 0.7886178861788617, MAT_recall: 0.14222873900293256, MAT_f1: 0.2409937888198758, MAT_number: 682
PRO_precision: 0.0625, PRO_recall: 0.005188067444876783, PRO_f1: 0.009580838323353293, PRO_number: 771
SMT_precision: 0.25, SMT_recall: 0.08187134502923976, SMT_f1: 0.12334801762114538, SMT_number: 171
SPL_precision: 0.48717948717948717, SPL_recall: 0.25333333333333335, SPL_f1: 0.33333333333333337, SPL_number: 75
overall_precision: 0.25688073394495414, overall_recall: 0.11195521791283487, overall_f1: 0.15594541910331386, overall_accuracy: 0.7053105567865056
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:02<?, ?it/s]