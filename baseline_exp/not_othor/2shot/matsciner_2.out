/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:47:34 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:47:35 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-c294cc2a6522e19d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:2, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 32
model_logits_weight: 20
num_train_epochs: 35
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1239.45it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:47:37 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-c294cc2a6522e19d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-05e767b760d711de.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:47:41 - INFO - __main__ - ***** Running training *****
06/01/2023 14:47:41 - INFO - __main__ -   Num examples = 7
06/01/2023 14:47:41 - INFO - __main__ -   Num Epochs = 35
06/01/2023 14:47:41 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:47:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:47:41 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:47:41 - INFO - __main__ -   Total optimization steps = 35
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 14.584261894226074
Total epoch: 1. epoch loss: 13.914843559265137
Total epoch: 2. epoch loss: 13.23195743560791
Total epoch: 3. epoch loss: 12.544469833374023
Total epoch: 4. epoch loss: 11.86166000366211
Total epoch: 5. epoch loss: 11.19327449798584
Total epoch: 6. epoch loss: 10.549604415893555
Total epoch: 7. epoch loss: 9.941862106323242
Total epoch: 8. epoch loss: 9.381299018859863
Total epoch: 9. epoch loss: 8.877503395080566
Total epoch: 10. epoch loss: 8.435347557067871
Total epoch: 11. epoch loss: 8.053552627563477
Total epoch: 12. epoch loss: 7.72704553604126
Total epoch: 13. epoch loss: 7.447727203369141
Total epoch: 14. epoch loss: 7.206487655639648
Total epoch: 15. epoch loss: 6.996555328369141
Total epoch: 16. epoch loss: 6.814267635345459
Total epoch: 17. epoch loss: 6.657474517822266
Total epoch: 18. epoch loss: 6.523755073547363
Total epoch: 19. epoch loss: 6.409844875335693
Total epoch: 20. epoch loss: 6.311979293823242
Total epoch: 21. epoch loss: 6.226872444152832
Total epoch: 22. epoch loss: 6.151998043060303
Total epoch: 23. epoch loss: 6.085296630859375
Total epoch: 24. epoch loss: 6.024913787841797
Total epoch: 25. epoch loss: 5.969448089599609
Total epoch: 26. epoch loss: 5.918158054351807
Total epoch: 27. epoch loss: 5.870871543884277
Total epoch: 28. epoch loss: 5.8277459144592285
Total epoch: 29. epoch loss: 5.78948974609375
Total epoch: 30. epoch loss: 5.754622936248779
Total epoch: 31. epoch loss: 5.722728729248047
Total epoch: 32. epoch loss: 5.69603157043457
Total epoch: 33. epoch loss: 5.673223972320557
Total epoch: 34. epoch loss: 5.646678447723389
Total epoch: 34. DecT loss: 5.646678447723389
Training time: 0.16567063331604004
APL_precision: 0.43209876543209874, APL_recall: 0.20588235294117646, APL_f1: 0.2788844621513944, APL_number: 170
CMT_precision: 0.0898876404494382, CMT_recall: 0.041025641025641026, CMT_f1: 0.056338028169014086, CMT_number: 195
DSC_precision: 0.4568345323741007, DSC_recall: 0.2906178489702517, DSC_f1: 0.35524475524475524, DSC_number: 437
MAT_precision: 0.4018324607329843, MAT_recall: 0.4501466275659824, MAT_f1: 0.4246196403872753, MAT_number: 682
PRO_precision: 0.11061946902654868, PRO_recall: 0.0324254215304799, PRO_f1: 0.050150451354062195, PRO_number: 771
SMT_precision: 0.18823529411764706, SMT_recall: 0.0935672514619883, SMT_f1: 0.125, SMT_number: 171
SPL_precision: 0.09375, SPL_recall: 0.04, SPL_f1: 0.05607476635514018, SPL_number: 75
overall_precision: 0.335048231511254, overall_recall: 0.20831667333066772, overall_f1: 0.2569033530571992, overall_accuracy: 0.7233221356586377
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:01<?, ?it/s]