/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:47:34 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:47:35 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-d4ae5fe9fd32b8ca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:2, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 32
model_logits_weight: 20
num_train_epochs: 35
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1175.37it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:47:37 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-d4ae5fe9fd32b8ca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b1c304d575c3e972.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5057.15 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:47:41 - INFO - __main__ - ***** Running training *****
06/01/2023 14:47:41 - INFO - __main__ -   Num examples = 6
06/01/2023 14:47:41 - INFO - __main__ -   Num Epochs = 35
06/01/2023 14:47:41 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:47:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:47:41 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:47:41 - INFO - __main__ -   Total optimization steps = 35
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.681188583374023
Total epoch: 1. epoch loss: 15.043760299682617
Total epoch: 2. epoch loss: 14.396007537841797
Total epoch: 3. epoch loss: 13.743027687072754
Total epoch: 4. epoch loss: 13.08999252319336
Total epoch: 5. epoch loss: 12.442702293395996
Total epoch: 6. epoch loss: 11.807929039001465
Total epoch: 7. epoch loss: 11.194040298461914
Total epoch: 8. epoch loss: 10.611382484436035
Total epoch: 9. epoch loss: 10.071977615356445
Total epoch: 10. epoch loss: 9.588103294372559
Total epoch: 11. epoch loss: 9.169428825378418
Total epoch: 12. epoch loss: 8.818746566772461
Total epoch: 13. epoch loss: 8.53142261505127
Total epoch: 14. epoch loss: 8.300177574157715
Total epoch: 15. epoch loss: 8.116942405700684
Total epoch: 16. epoch loss: 7.970292568206787
Total epoch: 17. epoch loss: 7.848247051239014
Total epoch: 18. epoch loss: 7.741916179656982
Total epoch: 19. epoch loss: 7.645684719085693
Total epoch: 20. epoch loss: 7.556146621704102
Total epoch: 21. epoch loss: 7.471320152282715
Total epoch: 22. epoch loss: 7.389958381652832
Total epoch: 23. epoch loss: 7.311189651489258
Total epoch: 24. epoch loss: 7.234567642211914
Total epoch: 25. epoch loss: 7.159910678863525
Total epoch: 26. epoch loss: 7.087228775024414
Total epoch: 27. epoch loss: 7.0165910720825195
Total epoch: 28. epoch loss: 6.948005676269531
Total epoch: 29. epoch loss: 6.881624698638916
Total epoch: 30. epoch loss: 6.817695140838623
Total epoch: 31. epoch loss: 6.756565570831299
Total epoch: 32. epoch loss: 6.698713779449463
Total epoch: 33. epoch loss: 6.643382549285889
Total epoch: 34. epoch loss: 6.589425086975098
Total epoch: 34. DecT loss: 6.589425086975098
Training time: 0.17205309867858887
APL_precision: 0.20408163265306123, APL_recall: 0.17647058823529413, APL_f1: 0.18927444794952683, APL_number: 170
CMT_precision: 0.016556291390728478, CMT_recall: 0.02564102564102564, CMT_f1: 0.02012072434607646, CMT_number: 195
DSC_precision: 0.40816326530612246, DSC_recall: 0.13729977116704806, DSC_f1: 0.20547945205479454, DSC_number: 437
MAT_precision: 0.6962962962962963, MAT_recall: 0.1378299120234604, MAT_f1: 0.23011015911872704, MAT_number: 682
PRO_precision: 0.2597402597402597, PRO_recall: 0.02594033722438392, PRO_f1: 0.04716981132075472, PRO_number: 771
SMT_precision: 0.055900621118012424, SMT_recall: 0.10526315789473684, SMT_f1: 0.07302231237322515, SMT_number: 171
SPL_precision: 0.20422535211267606, SPL_recall: 0.38666666666666666, SPL_f1: 0.2672811059907834, SPL_number: 75
overall_precision: 0.20125786163522014, overall_recall: 0.10235905637744902, overall_f1: 0.13570103366021735, overall_accuracy: 0.6921592452290758
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:02<?, ?it/s]