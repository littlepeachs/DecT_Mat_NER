/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:26 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:27 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-61df0b13c7c2e0bc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1096.26it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:28 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-61df0b13c7c2e0bc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-012278bb850383ba.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4401.40 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:32 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:32 - INFO - __main__ -   Num examples = 45
06/01/2023 14:48:32 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:32 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:32 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:32 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:32 - INFO - __main__ -   Total optimization steps = 200
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.944846153259277
Total epoch: 1. epoch loss: 15.152094841003418
Total epoch: 2. epoch loss: 14.375646591186523
Total epoch: 3. epoch loss: 13.656210899353027
Total epoch: 4. epoch loss: 13.014723777770996
Total epoch: 5. epoch loss: 12.442597389221191
Total epoch: 6. epoch loss: 11.930255889892578
Total epoch: 7. epoch loss: 11.47010612487793
Total epoch: 8. epoch loss: 11.055988311767578
Total epoch: 9. epoch loss: 10.681944847106934
Total epoch: 10. epoch loss: 10.34130859375
Total epoch: 11. epoch loss: 10.028833389282227
Total epoch: 12. epoch loss: 9.741485595703125
Total epoch: 13. epoch loss: 9.477752685546875
Total epoch: 14. epoch loss: 9.236380577087402
Total epoch: 15. epoch loss: 9.015758514404297
Total epoch: 16. epoch loss: 8.813944816589355
Total epoch: 17. epoch loss: 8.628883361816406
Total epoch: 18. epoch loss: 8.458789825439453
Total epoch: 19. epoch loss: 8.301935195922852
Total epoch: 20. epoch loss: 8.156479835510254
Total epoch: 21. epoch loss: 8.020696640014648
Total epoch: 22. epoch loss: 7.8933844566345215
Total epoch: 23. epoch loss: 7.773946762084961
Total epoch: 24. epoch loss: 7.663407802581787
Total epoch: 25. epoch loss: 7.5622782707214355
Total epoch: 26. epoch loss: 7.470230579376221
Total epoch: 27. epoch loss: 7.386229515075684
Total epoch: 28. epoch loss: 7.308876991271973
Total epoch: 29. epoch loss: 7.23634672164917
Total epoch: 30. epoch loss: 7.16709041595459
Total epoch: 31. epoch loss: 7.10115909576416
Total epoch: 32. epoch loss: 7.039712905883789
Total epoch: 33. epoch loss: 6.983407974243164
Total epoch: 34. epoch loss: 6.931990623474121
Total epoch: 35. epoch loss: 6.884790897369385
Total epoch: 36. epoch loss: 6.840986728668213
Total epoch: 37. epoch loss: 6.799629211425781
Total epoch: 38. epoch loss: 6.760087966918945
Total epoch: 39. epoch loss: 6.722681045532227
Total epoch: 40. epoch loss: 6.688132286071777
Total epoch: 41. epoch loss: 6.656411170959473
Total epoch: 42. epoch loss: 6.626103401184082
Total epoch: 43. epoch loss: 6.596354961395264
Total epoch: 44. epoch loss: 6.567677974700928
Total epoch: 45. epoch loss: 6.540151596069336
Total epoch: 46. epoch loss: 6.513777732849121
Total epoch: 47. epoch loss: 6.488668918609619
Total epoch: 48. epoch loss: 6.46439790725708
Total epoch: 49. epoch loss: 6.4405107498168945
Total epoch: 50. epoch loss: 6.41732120513916
Total epoch: 51. epoch loss: 6.394966125488281
Total epoch: 52. epoch loss: 6.373461723327637
Total epoch: 53. epoch loss: 6.352481842041016
Total epoch: 54. epoch loss: 6.3318562507629395
Total epoch: 55. epoch loss: 6.312035083770752
Total epoch: 56. epoch loss: 6.292993545532227
Total epoch: 57. epoch loss: 6.274486064910889
Total epoch: 58. epoch loss: 6.255889892578125
Total epoch: 59. epoch loss: 6.2383294105529785
Total epoch: 60. epoch loss: 6.221914768218994
Total epoch: 61. epoch loss: 6.206542491912842
Total epoch: 62. epoch loss: 6.191624164581299
Total epoch: 63. epoch loss: 6.1765923500061035
Total epoch: 64. epoch loss: 6.163877964019775
Total epoch: 65. epoch loss: 6.148176193237305
Total epoch: 66. epoch loss: 6.138057708740234
Total epoch: 67. epoch loss: 6.123039722442627
Total epoch: 68. epoch loss: 6.115911483764648
Total epoch: 69. epoch loss: 6.103750228881836
Total epoch: 70. epoch loss: 6.091484069824219
Total epoch: 71. epoch loss: 6.08297061920166
Total epoch: 72. epoch loss: 6.070682525634766
Total epoch: 73. epoch loss: 6.062309741973877
Total epoch: 74. epoch loss: 6.053199768066406
Total epoch: 75. epoch loss: 6.041496753692627
Total epoch: 76. epoch loss: 6.038687705993652
Total epoch: 77. epoch loss: 6.029707908630371
Total epoch: 78. epoch loss: 6.020400524139404
Total epoch: 79. epoch loss: 6.013270378112793
Total epoch: 80. epoch loss: 6.005902290344238
Total epoch: 81. epoch loss: 5.997520923614502
Total epoch: 82. epoch loss: 5.990764141082764
Total epoch: 83. epoch loss: 5.984114646911621
Total epoch: 84. epoch loss: 5.976651668548584
Total epoch: 85. epoch loss: 5.972643852233887
Total epoch: 86. epoch loss: 5.963990688323975
Total epoch: 87. epoch loss: 5.963566780090332
Total epoch: 88. epoch loss: 5.953332424163818
Total epoch: 89. epoch loss: 5.954199314117432
Total epoch: 90. epoch loss: 5.946138858795166
Total epoch: 91. epoch loss: 5.9423370361328125
Total epoch: 92. epoch loss: 5.936480522155762
Total epoch: 93. epoch loss: 5.9319748878479
Total epoch: 94. epoch loss: 5.92588996887207
Total epoch: 95. epoch loss: 5.921825408935547
Total epoch: 96. epoch loss: 5.9166741371154785
Total epoch: 97. epoch loss: 5.912532806396484
Total epoch: 98. epoch loss: 5.908215045928955
Total epoch: 99. epoch loss: 5.9056830406188965
Total epoch: 99. DecT loss: 5.9056830406188965
Training time: 0.479036808013916
APL_precision: 0.297029702970297, APL_recall: 0.5294117647058824, APL_f1: 0.3805496828752643, APL_number: 170
CMT_precision: 0.21372549019607842, CMT_recall: 0.558974358974359, CMT_f1: 0.30921985815602837, CMT_number: 195
DSC_precision: 0.5119825708061002, DSC_recall: 0.5377574370709383, DSC_f1: 0.5245535714285715, DSC_number: 437
MAT_precision: 0.5375816993464052, MAT_recall: 0.48240469208211145, MAT_f1: 0.508500772797527, MAT_number: 682
PRO_precision: 0.33035714285714285, PRO_recall: 0.28793774319066145, PRO_f1: 0.30769230769230765, PRO_number: 771
SMT_precision: 0.29927007299270075, SMT_recall: 0.47953216374269003, SMT_f1: 0.36853932584269666, SMT_number: 171
SPL_precision: 0.7096774193548387, SPL_recall: 0.29333333333333333, SPL_f1: 0.4150943396226416, SPL_number: 75
overall_precision: 0.38063614120936734, overall_recall: 0.43542582966813276, overall_f1: 0.40619171950764643, overall_accuracy: 0.7982989064398542
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:01<?, ?it/s]