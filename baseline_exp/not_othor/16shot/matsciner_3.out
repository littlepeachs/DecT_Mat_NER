/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:26 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:27 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-2a29c2bd2933348e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1088.86it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:28 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-2a29c2bd2933348e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0e3288f9f058bb0b.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4820.43 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:33 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:33 - INFO - __main__ -   Num examples = 36
06/01/2023 14:48:33 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:33 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:33 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:33 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:33 - INFO - __main__ -   Total optimization steps = 200
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.73489761352539
Total epoch: 1. epoch loss: 14.950429916381836
Total epoch: 2. epoch loss: 14.179435729980469
Total epoch: 3. epoch loss: 13.448554992675781
Total epoch: 4. epoch loss: 12.773513793945312
Total epoch: 5. epoch loss: 12.156448364257812
Total epoch: 6. epoch loss: 11.597965240478516
Total epoch: 7. epoch loss: 11.098799705505371
Total epoch: 8. epoch loss: 10.65550422668457
Total epoch: 9. epoch loss: 10.262190818786621
Total epoch: 10. epoch loss: 9.910799980163574
Total epoch: 11. epoch loss: 9.593127250671387
Total epoch: 12. epoch loss: 9.303174018859863
Total epoch: 13. epoch loss: 9.037991523742676
Total epoch: 14. epoch loss: 8.796762466430664
Total epoch: 15. epoch loss: 8.578957557678223
Total epoch: 16. epoch loss: 8.382837295532227
Total epoch: 17. epoch loss: 8.205233573913574
Total epoch: 18. epoch loss: 8.04262638092041
Total epoch: 19. epoch loss: 7.892438888549805
Total epoch: 20. epoch loss: 7.753598213195801
Total epoch: 21. epoch loss: 7.626200199127197
Total epoch: 22. epoch loss: 7.511180400848389
Total epoch: 23. epoch loss: 7.4074554443359375
Total epoch: 24. epoch loss: 7.312267780303955
Total epoch: 25. epoch loss: 7.223249435424805
Total epoch: 26. epoch loss: 7.139793872833252
Total epoch: 27. epoch loss: 7.061921119689941
Total epoch: 28. epoch loss: 6.989537239074707
Total epoch: 29. epoch loss: 6.922800064086914
Total epoch: 30. epoch loss: 6.8616461753845215
Total epoch: 31. epoch loss: 6.805232048034668
Total epoch: 32. epoch loss: 6.752587795257568
Total epoch: 33. epoch loss: 6.703392028808594
Total epoch: 34. epoch loss: 6.657476425170898
Total epoch: 35. epoch loss: 6.61445426940918
Total epoch: 36. epoch loss: 6.5740275382995605
Total epoch: 37. epoch loss: 6.536021709442139
Total epoch: 38. epoch loss: 6.50025749206543
Total epoch: 39. epoch loss: 6.466344833374023
Total epoch: 40. epoch loss: 6.433964729309082
Total epoch: 41. epoch loss: 6.4028825759887695
Total epoch: 42. epoch loss: 6.372785568237305
Total epoch: 43. epoch loss: 6.3439106941223145
Total epoch: 44. epoch loss: 6.316403865814209
Total epoch: 45. epoch loss: 6.289880752563477
Total epoch: 46. epoch loss: 6.264269828796387
Total epoch: 47. epoch loss: 6.2398786544799805
Total epoch: 48. epoch loss: 6.216463088989258
Total epoch: 49. epoch loss: 6.193317890167236
Total epoch: 50. epoch loss: 6.171031951904297
Total epoch: 51. epoch loss: 6.1498613357543945
Total epoch: 52. epoch loss: 6.129347801208496
Total epoch: 53. epoch loss: 6.108983039855957
Total epoch: 54. epoch loss: 6.089393615722656
Total epoch: 55. epoch loss: 6.070243835449219
Total epoch: 56. epoch loss: 6.051600933074951
Total epoch: 57. epoch loss: 6.0343098640441895
Total epoch: 58. epoch loss: 6.017828941345215
Total epoch: 59. epoch loss: 6.001439094543457
Total epoch: 60. epoch loss: 5.984785556793213
Total epoch: 61. epoch loss: 5.9687371253967285
Total epoch: 62. epoch loss: 5.9545769691467285
Total epoch: 63. epoch loss: 5.939228057861328
Total epoch: 64. epoch loss: 5.925249099731445
Total epoch: 65. epoch loss: 5.912866592407227
Total epoch: 66. epoch loss: 5.898497581481934
Total epoch: 67. epoch loss: 5.889382839202881
Total epoch: 68. epoch loss: 5.874766826629639
Total epoch: 69. epoch loss: 5.870148181915283
Total epoch: 70. epoch loss: 5.85910701751709
Total epoch: 71. epoch loss: 5.847599029541016
Total epoch: 72. epoch loss: 5.840226173400879
Total epoch: 73. epoch loss: 5.829680919647217
Total epoch: 74. epoch loss: 5.822420597076416
Total epoch: 75. epoch loss: 5.814542770385742
Total epoch: 76. epoch loss: 5.804001808166504
Total epoch: 77. epoch loss: 5.801362037658691
Total epoch: 78. epoch loss: 5.791875839233398
Total epoch: 79. epoch loss: 5.785895347595215
Total epoch: 80. epoch loss: 5.779148578643799
Total epoch: 81. epoch loss: 5.772261619567871
Total epoch: 82. epoch loss: 5.764632701873779
Total epoch: 83. epoch loss: 5.760394096374512
Total epoch: 84. epoch loss: 5.75127649307251
Total epoch: 85. epoch loss: 5.751420974731445
Total epoch: 86. epoch loss: 5.742716312408447
Total epoch: 87. epoch loss: 5.740540027618408
Total epoch: 88. epoch loss: 5.734703063964844
Total epoch: 89. epoch loss: 5.729696750640869
Total epoch: 90. epoch loss: 5.724221229553223
Total epoch: 91. epoch loss: 5.72041130065918
Total epoch: 92. epoch loss: 5.715099334716797
Total epoch: 93. epoch loss: 5.711010456085205
Total epoch: 94. epoch loss: 5.7082037925720215
Total epoch: 95. epoch loss: 5.702756404876709
Total epoch: 96. epoch loss: 5.702247619628906
Total epoch: 97. epoch loss: 5.6943464279174805
Total epoch: 98. epoch loss: 5.6978654861450195
Total epoch: 99. epoch loss: 5.690971374511719
Total epoch: 99. DecT loss: 5.690971374511719
Training time: 0.47138404846191406
APL_precision: 0.19325153374233128, APL_recall: 0.37058823529411766, APL_f1: 0.2540322580645161, APL_number: 170
CMT_precision: 0.40869565217391307, CMT_recall: 0.48205128205128206, CMT_f1: 0.44235294117647056, CMT_number: 195
DSC_precision: 0.459915611814346, DSC_recall: 0.4988558352402746, DSC_f1: 0.4785949506037322, DSC_number: 437
MAT_precision: 0.5886178861788618, MAT_recall: 0.530791788856305, MAT_f1: 0.5582112567463378, MAT_number: 682
PRO_precision: 0.40634441087613293, PRO_recall: 0.34889753566796367, PRO_f1: 0.3754361479413817, PRO_number: 771
SMT_precision: 0.3147410358565737, SMT_recall: 0.4619883040935672, SMT_f1: 0.3744075829383886, SMT_number: 171
SPL_precision: 0.6206896551724138, SPL_recall: 0.48, SPL_f1: 0.5413533834586466, SPL_number: 75
overall_precision: 0.42851681957186544, overall_recall: 0.44822071171531386, overall_f1: 0.4381473519640414, overall_accuracy: 0.8130226574226288
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:02<?, ?it/s]