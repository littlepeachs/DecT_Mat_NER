/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:48:26 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:48:27 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-f187e4508a2c5921/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1040.90it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:48:28 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-f187e4508a2c5921/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-70d1538777699481.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5331.38 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:48:33 - INFO - __main__ - ***** Running training *****
06/01/2023 14:48:33 - INFO - __main__ -   Num examples = 45
06/01/2023 14:48:33 - INFO - __main__ -   Num Epochs = 100
06/01/2023 14:48:33 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:48:33 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:48:33 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:48:33 - INFO - __main__ -   Total optimization steps = 200
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.865702629089355
Total epoch: 1. epoch loss: 15.11855697631836
Total epoch: 2. epoch loss: 14.37221622467041
Total epoch: 3. epoch loss: 13.647976875305176
Total epoch: 4. epoch loss: 12.967998504638672
Total epoch: 5. epoch loss: 12.343822479248047
Total epoch: 6. epoch loss: 11.778571128845215
Total epoch: 7. epoch loss: 11.275355339050293
Total epoch: 8. epoch loss: 10.831873893737793
Total epoch: 9. epoch loss: 10.440187454223633
Total epoch: 10. epoch loss: 10.091299057006836
Total epoch: 11. epoch loss: 9.777618408203125
Total epoch: 12. epoch loss: 9.49360466003418
Total epoch: 13. epoch loss: 9.235319137573242
Total epoch: 14. epoch loss: 8.999701499938965
Total epoch: 15. epoch loss: 8.784231185913086
Total epoch: 16. epoch loss: 8.586607933044434
Total epoch: 17. epoch loss: 8.404705047607422
Total epoch: 18. epoch loss: 8.236663818359375
Total epoch: 19. epoch loss: 8.081229209899902
Total epoch: 20. epoch loss: 7.937753200531006
Total epoch: 21. epoch loss: 7.806051731109619
Total epoch: 22. epoch loss: 7.685508728027344
Total epoch: 23. epoch loss: 7.575504302978516
Total epoch: 24. epoch loss: 7.475687503814697
Total epoch: 25. epoch loss: 7.385624408721924
Total epoch: 26. epoch loss: 7.304131984710693
Total epoch: 27. epoch loss: 7.2294111251831055
Total epoch: 28. epoch loss: 7.160058975219727
Total epoch: 29. epoch loss: 7.095332145690918
Total epoch: 30. epoch loss: 7.03472375869751
Total epoch: 31. epoch loss: 6.977803707122803
Total epoch: 32. epoch loss: 6.924479007720947
Total epoch: 33. epoch loss: 6.874788284301758
Total epoch: 34. epoch loss: 6.82859468460083
Total epoch: 35. epoch loss: 6.785431861877441
Total epoch: 36. epoch loss: 6.744601249694824
Total epoch: 37. epoch loss: 6.705474853515625
Total epoch: 38. epoch loss: 6.667802333831787
Total epoch: 39. epoch loss: 6.631662368774414
Total epoch: 40. epoch loss: 6.597118854522705
Total epoch: 41. epoch loss: 6.564135551452637
Total epoch: 42. epoch loss: 6.532760143280029
Total epoch: 43. epoch loss: 6.503048896789551
Total epoch: 44. epoch loss: 6.474808216094971
Total epoch: 45. epoch loss: 6.447702884674072
Total epoch: 46. epoch loss: 6.421520233154297
Total epoch: 47. epoch loss: 6.396239757537842
Total epoch: 48. epoch loss: 6.372044086456299
Total epoch: 49. epoch loss: 6.3487138748168945
Total epoch: 50. epoch loss: 6.326011657714844
Total epoch: 51. epoch loss: 6.304149150848389
Total epoch: 52. epoch loss: 6.2828521728515625
Total epoch: 53. epoch loss: 6.26242733001709
Total epoch: 54. epoch loss: 6.242429256439209
Total epoch: 55. epoch loss: 6.222806453704834
Total epoch: 56. epoch loss: 6.203465461730957
Total epoch: 57. epoch loss: 6.184419631958008
Total epoch: 58. epoch loss: 6.165985107421875
Total epoch: 59. epoch loss: 6.148081302642822
Total epoch: 60. epoch loss: 6.130529403686523
Total epoch: 61. epoch loss: 6.113488674163818
Total epoch: 62. epoch loss: 6.0976362228393555
Total epoch: 63. epoch loss: 6.082419395446777
Total epoch: 64. epoch loss: 6.0659050941467285
Total epoch: 65. epoch loss: 6.052788257598877
Total epoch: 66. epoch loss: 6.038197040557861
Total epoch: 67. epoch loss: 6.0268096923828125
Total epoch: 68. epoch loss: 6.011466979980469
Total epoch: 69. epoch loss: 6.003509521484375
Total epoch: 70. epoch loss: 5.988460063934326
Total epoch: 71. epoch loss: 5.983208179473877
Total epoch: 72. epoch loss: 5.97239351272583
Total epoch: 73. epoch loss: 5.959859848022461
Total epoch: 74. epoch loss: 5.951166152954102
Total epoch: 75. epoch loss: 5.941320419311523
Total epoch: 76. epoch loss: 5.932056427001953
Total epoch: 77. epoch loss: 5.922680854797363
Total epoch: 78. epoch loss: 5.915224552154541
Total epoch: 79. epoch loss: 5.905832290649414
Total epoch: 80. epoch loss: 5.899328231811523
Total epoch: 81. epoch loss: 5.892961025238037
Total epoch: 82. epoch loss: 5.883819103240967
Total epoch: 83. epoch loss: 5.881643295288086
Total epoch: 84. epoch loss: 5.87338924407959
Total epoch: 85. epoch loss: 5.866852760314941
Total epoch: 86. epoch loss: 5.86234188079834
Total epoch: 87. epoch loss: 5.855258464813232
Total epoch: 88. epoch loss: 5.849274635314941
Total epoch: 89. epoch loss: 5.84469747543335
Total epoch: 90. epoch loss: 5.838506698608398
Total epoch: 91. epoch loss: 5.833374500274658
Total epoch: 92. epoch loss: 5.829562187194824
Total epoch: 93. epoch loss: 5.824159145355225
Total epoch: 94. epoch loss: 5.8186259269714355
Total epoch: 95. epoch loss: 5.816267490386963
Total epoch: 96. epoch loss: 5.81015682220459
Total epoch: 97. epoch loss: 5.807896137237549
Total epoch: 98. epoch loss: 5.801920413970947
Total epoch: 99. epoch loss: 5.799959182739258
Total epoch: 99. DecT loss: 5.799959182739258
Training time: 0.49039244651794434
APL_precision: 0.27124183006535946, APL_recall: 0.48823529411764705, APL_f1: 0.34873949579831937, APL_number: 170
CMT_precision: 0.3277027027027027, CMT_recall: 0.49743589743589745, CMT_f1: 0.3951120162932791, CMT_number: 195
DSC_precision: 0.4682713347921225, DSC_recall: 0.4897025171624714, DSC_f1: 0.4787472035794183, DSC_number: 437
MAT_precision: 0.4033850493653032, MAT_recall: 0.41935483870967744, MAT_f1: 0.411214953271028, MAT_number: 682
PRO_precision: 0.4656188605108055, PRO_recall: 0.30739299610894943, PRO_f1: 0.3703125, PRO_number: 771
SMT_precision: 0.30578512396694213, SMT_recall: 0.4327485380116959, SMT_f1: 0.35835351089588374, SMT_number: 171
SPL_precision: 0.48333333333333334, SPL_recall: 0.38666666666666666, SPL_f1: 0.42962962962962964, SPL_number: 75
overall_precision: 0.3955021326095386, overall_recall: 0.40783686525389845, overall_f1: 0.4015748031496063, overall_accuracy: 0.8071617468372525
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:02<?, ?it/s]