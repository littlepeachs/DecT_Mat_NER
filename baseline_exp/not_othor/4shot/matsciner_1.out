/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:47:52 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:47:53 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-9dfab3336ef43c32/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:4, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 32
model_logits_weight: 20
num_train_epochs: 35
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 973.16it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:47:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-9dfab3336ef43c32/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0dcc4d0477e7e251.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4735.54 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:47:58 - INFO - __main__ - ***** Running training *****
06/01/2023 14:47:58 - INFO - __main__ -   Num examples = 13
06/01/2023 14:47:58 - INFO - __main__ -   Num Epochs = 35
06/01/2023 14:47:58 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:47:58 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:47:58 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:47:58 - INFO - __main__ -   Total optimization steps = 35
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 14.619308471679688
Total epoch: 1. epoch loss: 14.115133285522461
Total epoch: 2. epoch loss: 13.61053466796875
Total epoch: 3. epoch loss: 13.109525680541992
Total epoch: 4. epoch loss: 12.615739822387695
Total epoch: 5. epoch loss: 12.13227653503418
Total epoch: 6. epoch loss: 11.662193298339844
Total epoch: 7. epoch loss: 11.209155082702637
Total epoch: 8. epoch loss: 10.777168273925781
Total epoch: 9. epoch loss: 10.37026596069336
Total epoch: 10. epoch loss: 9.992053031921387
Total epoch: 11. epoch loss: 9.645834922790527
Total epoch: 12. epoch loss: 9.333891868591309
Total epoch: 13. epoch loss: 9.055024147033691
Total epoch: 14. epoch loss: 8.804038047790527
Total epoch: 15. epoch loss: 8.575831413269043
Total epoch: 16. epoch loss: 8.367079734802246
Total epoch: 17. epoch loss: 8.175108909606934
Total epoch: 18. epoch loss: 7.997017860412598
Total epoch: 19. epoch loss: 7.830287456512451
Total epoch: 20. epoch loss: 7.673235893249512
Total epoch: 21. epoch loss: 7.524994850158691
Total epoch: 22. epoch loss: 7.385082244873047
Total epoch: 23. epoch loss: 7.252993583679199
Total epoch: 24. epoch loss: 7.128109931945801
Total epoch: 25. epoch loss: 7.009879112243652
Total epoch: 26. epoch loss: 6.898073196411133
Total epoch: 27. epoch loss: 6.7927703857421875
Total epoch: 28. epoch loss: 6.694502353668213
Total epoch: 29. epoch loss: 6.604097366333008
Total epoch: 30. epoch loss: 6.521787166595459
Total epoch: 31. epoch loss: 6.446611404418945
Total epoch: 32. epoch loss: 6.37773323059082
Total epoch: 33. epoch loss: 6.315467834472656
Total epoch: 34. epoch loss: 6.260651588439941
Total epoch: 34. DecT loss: 6.260651588439941
Training time: 0.17316055297851562
APL_precision: 0.1806615776081425, APL_recall: 0.4176470588235294, APL_f1: 0.25222024866785087, APL_number: 170
CMT_precision: 0.13385826771653545, CMT_recall: 0.3487179487179487, CMT_f1: 0.19345661450924612, CMT_number: 195
DSC_precision: 0.33125, DSC_recall: 0.2425629290617849, DSC_f1: 0.2800528401585205, DSC_number: 437
MAT_precision: 0.43438453713123093, MAT_recall: 0.626099706744868, MAT_f1: 0.5129129129129129, MAT_number: 682
PRO_precision: 0.2923387096774194, PRO_recall: 0.1880674448767834, PRO_f1: 0.22888713496448304, PRO_number: 771
SMT_precision: 0.16791044776119404, SMT_recall: 0.2631578947368421, SMT_f1: 0.20501138952164008, SMT_number: 171
SPL_precision: 0.4782608695652174, SPL_recall: 0.44, SPL_f1: 0.4583333333333333, SPL_number: 75
overall_precision: 0.294698715837998, overall_recall: 0.35785685725709715, overall_f1: 0.3232213795594077, overall_accuracy: 0.7599885640769066
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:01<?, ?it/s]