/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
06/01/2023 14:47:15 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/01/2023 14:47:17 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-11bd3552460ff762/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:1, proto_dim:160, logits_weight:10.0, weight_decay:1e-07 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 32
model_logits_weight: 20
num_train_epochs: 35
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1013.36it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json
Model config BertConfig {
  "_name_or_path": "pranav-s/MaterialsBERT",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at pranav-s/MaterialsBERT.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
06/01/2023 14:47:19 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/liwentao/.cache/huggingface/datasets/json/default-11bd3552460ff762/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b96c3783350e2d34.arrow
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]                                                                            /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
06/01/2023 14:47:24 - INFO - __main__ - ***** Running training *****
06/01/2023 14:47:24 - INFO - __main__ -   Num examples = 4
06/01/2023 14:47:24 - INFO - __main__ -   Num Epochs = 35
06/01/2023 14:47:24 - INFO - __main__ -   Instantaneous batch size per device = 32
06/01/2023 14:47:24 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/01/2023 14:47:24 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2023 14:47:24 - INFO - __main__ -   Total optimization steps = 35
tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  0%|          | 0/35 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:405: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.969138145446777
Total epoch: 1. epoch loss: 15.177751541137695
Total epoch: 2. epoch loss: 14.371709823608398
Total epoch: 3. epoch loss: 13.562553405761719
Total epoch: 4. epoch loss: 12.76152229309082
Total epoch: 5. epoch loss: 11.981005668640137
Total epoch: 6. epoch loss: 11.23585319519043
Total epoch: 7. epoch loss: 10.544254302978516
Total epoch: 8. epoch loss: 9.926497459411621
Total epoch: 9. epoch loss: 9.395622253417969
Total epoch: 10. epoch loss: 8.944904327392578
Total epoch: 11. epoch loss: 8.55897045135498
Total epoch: 12. epoch loss: 8.230401039123535
Total epoch: 13. epoch loss: 7.960045337677002
Total epoch: 14. epoch loss: 7.749668121337891
Total epoch: 15. epoch loss: 7.589312553405762
Total epoch: 16. epoch loss: 7.459310054779053
Total epoch: 17. epoch loss: 7.345301628112793
Total epoch: 18. epoch loss: 7.243059158325195
Total epoch: 19. epoch loss: 7.1529998779296875
Total epoch: 20. epoch loss: 7.074488639831543
Total epoch: 21. epoch loss: 7.0045952796936035
Total epoch: 22. epoch loss: 6.940420150756836
Total epoch: 23. epoch loss: 6.88006591796875
Total epoch: 24. epoch loss: 6.822887897491455
Total epoch: 25. epoch loss: 6.769165515899658
Total epoch: 26. epoch loss: 6.719149589538574
Total epoch: 27. epoch loss: 6.672365188598633
Total epoch: 28. epoch loss: 6.627485752105713
Total epoch: 29. epoch loss: 6.583106994628906
Total epoch: 30. epoch loss: 6.5386433601379395
Total epoch: 31. epoch loss: 6.494475841522217
Total epoch: 32. epoch loss: 6.451131820678711
Total epoch: 33. epoch loss: 6.409527778625488
Total epoch: 34. epoch loss: 6.3820390701293945
Total epoch: 34. DecT loss: 6.3820390701293945
Training time: 0.16254878044128418
APL_precision: 0.2777777777777778, APL_recall: 0.029411764705882353, APL_f1: 0.05319148936170213, APL_number: 170
CMT_precision: 0.058823529411764705, CMT_recall: 0.010256410256410256, CMT_f1: 0.017467248908296942, CMT_number: 195
DSC_precision: 0.7906976744186046, DSC_recall: 0.07780320366132723, DSC_f1: 0.14166666666666666, DSC_number: 437
MAT_precision: 0.6923076923076923, MAT_recall: 0.013196480938416423, MAT_f1: 0.02589928057553957, MAT_number: 682
PRO_precision: 0.26506024096385544, PRO_recall: 0.028534370946822308, PRO_f1: 0.051522248243559714, PRO_number: 771
SMT_precision: 0.23076923076923078, SMT_recall: 0.017543859649122806, SMT_f1: 0.03260869565217391, SMT_number: 171
SPL_precision: 0.23076923076923078, SPL_recall: 0.04, SPL_f1: 0.06818181818181819, SPL_number: 75
overall_precision: 0.35944700460829493, overall_recall: 0.031187524990004, overall_f1: 0.05739514348785872, overall_accuracy: 0.6940175827317562
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 992, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 815, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 513, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/35 [00:02<?, ?it/s]