05/11/2023 00:32:59 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/11/2023 00:33:00 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-e2e87af22853dbef/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 349.21it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/final_verbalizer.json...
{'I-CMT': ['electron', 'diffraction', 'microscopy', 'spectroscopy', 'scanning', 'raman', 'transmission', 'test', 'neutron'], 'I-MAT': ['oxide', 'silicon', 'carbon', 'graphene', 'aluminum', 'titanium', 'steel', 'oxides'], 'I-DSC': ['films', 'doped', 'thin', 'film', 'alloy', 'substrate', 'layer', 'single', 'alloys'], 'I-PRO': ['properties', 'structure', 'magnetic', 'band', 'conductivity', 'resistance', 'density', 'behavior', 'electrical'], 'I-SMT': ['annealing', 'gel', 'plasma', 'hydrothermal', 'annealed', 'sol', 'treatment', 'irradiation', 'vapor'], 'I-APL': ['coatings', 'coating', 'solar', 'cells', 'electrode', 'applications', 'electrodes', 'catalysts', 'cathode'], 'I-SPL': ['cubic', 'hexagonal', 'rock', 'ch']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081, 10314, 5820, 7779, 7070, 12009, 2856, 856, 14049]
I-MAT
[6678, 8605, 3473, 11106, 15028, 19090, 10735, 20464]
I-DSC
[7423, 21155, 5197, 5796, 15937, 4061, 2474, 1232, 20683]
I-PRO
[1784, 1187, 3510, 2102, 9370, 2661, 2027, 1689, 4874]
I-SMT
[12040, 4051, 2780, 29973, 27202, 638, 922, 8896, 12766]
I-APL
[20189, 9754, 6911, 576, 6665, 2040, 8438, 15834, 19112]
I-SPL
[13879, 22235, 8863, 249]
{'I-CMT': ['electron', 'diffraction', 'microscopy', 'spectroscopy', 'scanning', 'raman', 'transmission', 'test', 'neutron'], 'I-MAT': ['oxide', 'silicon', 'carbon', 'graphene', 'aluminum', 'titanium', 'steel', 'oxides'], 'I-DSC': ['films', 'doped', 'thin', 'film', 'alloy', 'substrate', 'layer', 'single', 'alloys'], 'I-PRO': ['properties', 'structure', 'magnetic', 'band', 'conductivity', 'resistance', 'density', 'behavior', 'electrical'], 'I-SMT': ['annealing', 'gel', 'plasma', 'hydrothermal', 'annealed', 'sol', 'treatment', 'irradiation', 'vapor'], 'I-APL': ['coatings', 'coating', 'solar', 'cells', 'electrode', 'applications', 'electrodes', 'catalysts', 'cathode'], 'I-SPL': ['cubic', 'hexagonal', 'rock', 'ch']}
Running tokenizer on dataset:   0%|          | 0/22 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3915.05 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/train_transformer.py:548: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/11/2023 00:33:35 - INFO - __main__ - ***** Running training *****
05/11/2023 00:33:35 - INFO - __main__ -   Num examples = 22
05/11/2023 00:33:35 - INFO - __main__ -   Num Epochs = 60
05/11/2023 00:33:35 - INFO - __main__ -   Instantaneous batch size per device = 4
05/11/2023 00:33:35 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/11/2023 00:33:35 - INFO - __main__ -   Gradient Accumulation steps = 1
05/11/2023 00:33:35 - INFO - __main__ -   Total optimization steps = 360
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/360 [00:00<?, ?it/s]  0%|          | 1/360 [00:00<00:47,  7.62it/s]  1%|          | 3/360 [00:00<00:29, 11.96it/s]  1%|â–         | 5/360 [00:00<00:26, 13.63it/s]  2%|â–         | 7/360 [00:00<00:27, 12.65it/s]  2%|â–Ž         | 9/360 [00:00<00:27, 12.84it/s]  3%|â–Ž         | 11/360 [00:00<00:26, 13.31it/s]  4%|â–Ž         | 13/360 [00:00<00:25, 13.69it/s]  4%|â–         | 15/360 [00:01<00:24, 14.09it/s]  5%|â–         | 17/360 [00:01<00:23, 14.41it/s]  5%|â–Œ         | 19/360 [00:01<00:23, 14.72it/s]  6%|â–Œ         | 21/360 [00:01<00:22, 15.38it/s]  6%|â–‹         | 23/360 [00:01<00:21, 15.86it/s]  7%|â–‹         | 25/360 [00:01<00:19, 16.81it/s]  8%|â–Š         | 28/360 [00:01<00:18, 17.61it/s]  8%|â–Š         | 30/360 [00:02<00:19, 17.14it/s]  9%|â–‰         | 32/360 [00:02<00:19, 16.42it/s]  9%|â–‰         | 34/360 [00:02<00:20, 16.23it/s] 10%|â–ˆ         | 36/360 [00:02<00:19, 16.75it/s] 11%|â–ˆ         | 38/360 [00:02<00:19, 16.56it/s] 11%|â–ˆ         | 40/360 [00:02<00:18, 17.17it/s] 12%|â–ˆâ–        | 42/360 [00:02<00:18, 16.77it/s] 12%|â–ˆâ–        | 44/360 [00:02<00:19, 16.08it/s] 13%|â–ˆâ–Ž        | 46/360 [00:02<00:19, 16.24it/s] 13%|â–ˆâ–Ž        | 48/360 [00:03<00:19, 16.40it/s] 14%|â–ˆâ–        | 50/360 [00:03<00:18, 16.91it/s] 14%|â–ˆâ–        | 52/360 [00:03<00:17, 17.44it/s] 15%|â–ˆâ–Œ        | 54/360 [00:03<00:17, 17.37it/s] 16%|â–ˆâ–Œ        | 56/360 [00:03<00:18, 16.63it/s] 16%|â–ˆâ–Œ        | 58/360 [00:03<00:18, 16.60it/s] 17%|â–ˆâ–‹        | 60/360 [00:03<00:17, 16.69it/s] 17%|â–ˆâ–‹        | 62/360 [00:03<00:18, 16.53it/s] 18%|â–ˆâ–Š        | 64/360 [00:04<00:17, 16.98it/s] 18%|â–ˆâ–Š        | 66/360 [00:04<00:16, 17.47it/s] 19%|â–ˆâ–‰        | 68/360 [00:04<00:16, 17.64it/s] 19%|â–ˆâ–‰        | 70/360 [00:04<00:16, 18.01it/s] 20%|â–ˆâ–ˆ        | 72/360 [00:04<00:16, 18.00it/s] 21%|â–ˆâ–ˆ        | 74/360 [00:04<00:16, 17.35it/s] 21%|â–ˆâ–ˆ        | 76/360 [00:04<00:16, 16.87it/s] 22%|â–ˆâ–ˆâ–       | 78/360 [00:04<00:16, 16.82it/s] 22%|â–ˆâ–ˆâ–       | 80/360 [00:04<00:16, 16.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 82/360 [00:05<00:16, 16.71it/s] 23%|â–ˆâ–ˆâ–Ž       | 84/360 [00:05<00:16, 16.87it/s] 24%|â–ˆâ–ˆâ–       | 86/360 [00:05<00:16, 16.80it/s] 24%|â–ˆâ–ˆâ–       | 88/360 [00:05<00:15, 17.06it/s] 25%|â–ˆâ–ˆâ–Œ       | 90/360 [00:05<00:15, 17.34it/s] 26%|â–ˆâ–ˆâ–Œ       | 92/360 [00:05<00:15, 17.37it/s] 26%|â–ˆâ–ˆâ–Œ       | 94/360 [00:05<00:15, 17.43it/s] 27%|â–ˆâ–ˆâ–‹       | 96/360 [00:05<00:15, 17.53it/s] 27%|â–ˆâ–ˆâ–‹       | 98/360 [00:06<00:15, 17.39it/s] 28%|â–ˆâ–ˆâ–Š       | 100/360 [00:06<00:14, 17.74it/s] 28%|â–ˆâ–ˆâ–Š       | 102/360 [00:06<00:14, 17.80it/s] 29%|â–ˆâ–ˆâ–‰       | 104/360 [00:06<00:14, 17.60it/s] 29%|â–ˆâ–ˆâ–‰       | 106/360 [00:06<00:14, 17.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 108/360 [00:06<00:14, 17.88it/s] 31%|â–ˆâ–ˆâ–ˆ       | 110/360 [00:06<00:14, 17.26it/s] 31%|â–ˆâ–ˆâ–ˆ       | 112/360 [00:06<00:14, 17.08it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 114/360 [00:06<00:13, 17.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 116/360 [00:07<00:13, 17.82it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 118/360 [00:07<00:13, 17.65it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 120/360 [00:07<00:13, 17.70it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 122/360 [00:07<00:13, 17.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 124/360 [00:07<00:13, 17.26it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 126/360 [00:07<00:13, 17.54it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 128/360 [00:07<00:13, 17.03it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/360 [00:07<00:13, 16.91it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 132/360 [00:07<00:13, 17.02it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/360 [00:08<00:13, 16.72it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 136/360 [00:08<00:13, 16.69it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/360 [00:08<00:13, 16.82it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 140/360 [00:08<00:13, 16.35it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 142/360 [00:08<00:13, 16.42it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144/360 [00:08<00:12, 16.62it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 146/360 [00:08<00:12, 16.48it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/360 [00:08<00:12, 16.45it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/360 [00:09<00:12, 16.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/360 [00:09<00:12, 16.43it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 154/360 [00:09<00:12, 16.20it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 156/360 [00:09<00:12, 16.88it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/360 [00:09<00:11, 17.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/360 [00:09<00:11, 16.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 162/360 [00:09<00:12, 16.07it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 164/360 [00:09<00:12, 15.28it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/360 [00:10<00:12, 15.39it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 168/360 [00:10<00:12, 15.94it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/360 [00:10<00:11, 16.34it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 172/360 [00:10<00:11, 16.77it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 174/360 [00:10<00:11, 16.71it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 176/360 [00:10<00:10, 17.15it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 178/360 [00:10<00:10, 17.50it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/360 [00:10<00:10, 16.85it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 182/360 [00:11<00:11, 15.96it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/360 [00:11<00:11, 15.73it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 186/360 [00:11<00:10, 15.95it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188/360 [00:11<00:10, 16.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 190/360 [00:11<00:10, 16.79it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 192/360 [00:11<00:09, 17.02it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/360 [00:11<00:09, 17.00it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/360 [00:11<00:09, 17.11it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 198/360 [00:11<00:09, 17.21it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 200/360 [00:12<00:09, 16.90it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/360 [00:12<00:09, 16.78it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 204/360 [00:12<00:09, 16.49it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 206/360 [00:12<00:09, 15.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 208/360 [00:12<00:09, 15.32it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 210/360 [00:12<00:09, 15.79it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 212/360 [00:12<00:09, 15.89it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 214/360 [00:12<00:09, 16.21it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 216/360 [00:13<00:08, 16.13it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 218/360 [00:13<00:08, 16.25it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 220/360 [00:13<00:08, 16.40it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 222/360 [00:13<00:08, 16.60it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224/360 [00:13<00:08, 15.88it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 226/360 [00:13<00:08, 16.15it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 228/360 [00:13<00:08, 16.42it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/360 [00:13<00:08, 16.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/360 [00:14<00:07, 16.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 234/360 [00:14<00:07, 16.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 236/360 [00:14<00:07, 16.31it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 238/360 [00:14<00:07, 16.30it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 240/360 [00:14<00:07, 16.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 242/360 [00:14<00:07, 16.10it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 244/360 [00:14<00:07, 16.21it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 246/360 [00:14<00:06, 16.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 248/360 [00:15<00:06, 16.20it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 250/360 [00:15<00:06, 16.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 252/360 [00:15<00:06, 16.52it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 254/360 [00:15<00:06, 16.39it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 256/360 [00:15<00:06, 16.33it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258/360 [00:15<00:06, 16.55it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260/360 [00:15<00:06, 16.40it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 262/360 [00:15<00:05, 16.40it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 264/360 [00:16<00:05, 16.51it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/360 [00:16<00:05, 16.32it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/360 [00:16<00:05, 16.41it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 270/360 [00:16<00:05, 16.62it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 272/360 [00:16<00:05, 16.16it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 274/360 [00:16<00:05, 16.47it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 276/360 [00:16<00:05, 16.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 278/360 [00:16<00:04, 16.42it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 280/360 [00:17<00:04, 16.45it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 282/360 [00:17<00:04, 16.40it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 284/360 [00:17<00:04, 16.51it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 286/360 [00:17<00:04, 16.80it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 288/360 [00:17<00:04, 16.55it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 290/360 [00:17<00:04, 16.04it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 292/360 [00:17<00:04, 16.19it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294/360 [00:17<00:04, 15.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296/360 [00:18<00:04, 15.03it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 298/360 [00:18<00:04, 14.88it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 300/360 [00:18<00:03, 15.57it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/360 [00:18<00:03, 15.88it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/360 [00:18<00:03, 16.78it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 307/360 [00:18<00:02, 18.59it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 309/360 [00:18<00:02, 18.77it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 312/360 [00:18<00:02, 20.05it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 315/360 [00:19<00:02, 20.76it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 318/360 [00:19<00:01, 21.49it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 321/360 [00:19<00:01, 21.55it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 324/360 [00:19<00:01, 22.10it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 327/360 [00:19<00:01, 21.68it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330/360 [00:19<00:01, 21.95it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 333/360 [00:19<00:01, 22.01it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 336/360 [00:19<00:01, 22.36it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/360 [00:20<00:00, 21.92it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 342/360 [00:20<00:00, 21.95it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 345/360 [00:20<00:00, 22.00it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 348/360 [00:20<00:00, 22.36it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 351/360 [00:20<00:00, 22.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 354/360 [00:20<00:00, 22.26it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 357/360 [00:20<00:00, 22.24it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [00:21<00:00, 21.47it/s]Configuration saved in models/matsciner-fewshot-test/config.json
Configuration saved in models/matsciner-fewshot-test/generation_config.json
Model weights saved in models/matsciner-fewshot-test/pytorch_model.bin
tokenizer config file saved in models/matsciner-fewshot-test/tokenizer_config.json
Special tokens file saved in models/matsciner-fewshot-test/special_tokens_map.json
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [00:24<00:00, 15.00it/s]
Decoding time: 1.280951976776123s
APL_precision: 0.3431372549019608, APL_recall: 0.4117647058823529, APL_f1: 0.374331550802139, APL_number: 170
CMT_precision: 0.5126582278481012, CMT_recall: 0.4153846153846154, CMT_f1: 0.45892351274787535, CMT_number: 195
DSC_precision: 0.6105263157894737, DSC_recall: 0.2654462242562929, DSC_f1: 0.3700159489633174, DSC_number: 437
MAT_precision: 0.6361904761904762, MAT_recall: 0.4897360703812317, MAT_f1: 0.5534382767191384, MAT_number: 682
PRO_precision: 0.4595744680851064, PRO_recall: 0.14007782101167315, PRO_f1: 0.21471172962226642, PRO_number: 771
SMT_precision: 0.5213675213675214, SMT_recall: 0.3567251461988304, SMT_f1: 0.4236111111111111, SMT_number: 171
SPL_precision: 0.5614035087719298, SPL_recall: 0.4266666666666667, SPL_f1: 0.48484848484848486, SPL_number: 75
overall_precision: 0.5397039030955586, overall_recall: 0.320671731307477, overall_f1: 0.4023074993729621, overall_accuracy: 0.7912229290258023
Finish training, best metric: 
{'APL_precision': 0.3431372549019608, 'APL_recall': 0.4117647058823529, 'APL_f1': 0.374331550802139, 'APL_number': 170, 'CMT_precision': 0.5126582278481012, 'CMT_recall': 0.4153846153846154, 'CMT_f1': 0.45892351274787535, 'CMT_number': 195, 'DSC_precision': 0.6105263157894737, 'DSC_recall': 0.2654462242562929, 'DSC_f1': 0.3700159489633174, 'DSC_number': 437, 'MAT_precision': 0.6361904761904762, 'MAT_recall': 0.4897360703812317, 'MAT_f1': 0.5534382767191384, 'MAT_number': 682, 'PRO_precision': 0.4595744680851064, 'PRO_recall': 0.14007782101167315, 'PRO_f1': 0.21471172962226642, 'PRO_number': 771, 'SMT_precision': 0.5213675213675214, 'SMT_recall': 0.3567251461988304, 'SMT_f1': 0.4236111111111111, 'SMT_number': 171, 'SPL_precision': 0.5614035087719298, 'SPL_recall': 0.4266666666666667, 'SPL_f1': 0.48484848484848486, 'SPL_number': 75, 'overall_precision': 0.5397039030955586, 'overall_recall': 0.320671731307477, 'overall_f1': 0.4023074993729621, 'overall_accuracy': 0.7912229290258023}
