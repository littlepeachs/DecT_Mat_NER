Some weights of the model checkpoint at /home/liwentao/learn/DecT_Mat_NER/model were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/liwentao/learn/DecT_Mat_NER/model and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device: cuda
9 511 546
['I-APL', 'I-CMT', 'I-DSC', 'I-MAT', 'I-PRO', 'I-SMT', 'I-SPL', 'O']
ModuleList(
  (0): BertLayer(
    (attention): BertAttention(
      (self): BertSelfAttention(
        (query): Linear(in_features=768, out_features=768, bias=True)
        (key): Linear(in_features=768, out_features=768, bias=True)
        (value): Linear(in_features=768, out_features=768, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (output): BertSelfOutput(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (intermediate): BertIntermediate(
      (dense): Linear(in_features=768, out_features=3072, bias=True)
      (intermediate_act_fn): GELUActivation()
    )
    (output): BertOutput(
      (dense): Linear(in_features=3072, out_features=768, bias=True)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (1): BertLayer(
    (attention): BertAttention(
      (self): BertSelfAttention(
        (query): Linear(in_features=768, out_features=768, bias=True)
        (key): Linear(in_features=768, out_features=768, bias=True)
        (value): Linear(in_features=768, out_features=768, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (output): BertSelfOutput(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (intermediate): BertIntermediate(
      (dense): Linear(in_features=768, out_features=3072, bias=True)
      (intermediate_act_fn): GELUActivation()
    )
    (output): BertOutput(
      (dense): Linear(in_features=3072, out_features=768, bias=True)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)
training_time:0.40103626251220703
loss:1.9263756275177002
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.0, 'micro_f1': 0}
training_time:0.058057308197021484
loss:0.5119693875312805
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.0, 'micro_f1': 0}
training_time:0.05844402313232422
loss:0.5087082982063293
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.0, 'micro_f1': 0}
training_time:0.05470466613769531
loss:0.4299652874469757
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.0, 'micro_f1': 0}
training_time:0.054308414459228516
loss:0.36887240409851074
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.0, 'micro_f1': 0}
training_time:0.05478930473327637
loss:0.2978084683418274
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.0, 'micro_f1': 0}
training_time:0.05469202995300293
loss:0.23702816665172577
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.0, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.0, 'micro_f1': 0}
training_time:0.05516409873962402
loss:0.18953996896743774
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.2936857562408223, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 0.0, 'macro_f1': 0.0004195510803440319, 'micro_f1': 0.0008009611533840609}
training_time:0.05481100082397461
loss:0.17252622544765472
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 0.5847953216374269, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 5.194805194805195, 'macro_f1': 0.008256572166346602, 'micro_f1': 0.0031974420463629096}
training_time:0.05388927459716797
loss:0.12963812053203583
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 2.873563218390805, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 5.194805194805195, 'macro_f1': 0.011526240590279999, 'micro_f1': 0.009535160905840287}
training_time:0.054134368896484375
loss:0.1141100823879242
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 5.915492957746479, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 5.194805194805195, 'macro_f1': 0.015871854503645248, 'micro_f1': 0.018174634531805612}
training_time:0.056238651275634766
loss:0.0874333381652832
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 7.777777777777778, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 5.194805194805195, 'macro_f1': 0.018532261389404246, 'micro_f1': 0.02362204724409449}
training_time:0.056817054748535156
loss:0.07252335548400879
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 8.275862068965518, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 5.194805194805195, 'macro_f1': 0.019243810376815305, 'micro_f1': 0.025147347740667975}
training_time:0.056191205978393555
loss:0.06371666491031647
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 8.275862068965518, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 5.194805194805195, 'macro_f1': 0.019243810376815305, 'micro_f1': 0.025147347740667975}
training_time:0.05456185340881348
loss:0.06185879930853844
{'APL': 0.0, 'CMT': 0.0, 'DSC': 0.0, 'MAT': 8.815426997245178, 'PRO': 0.0, 'SMT': 0.0, 'SPL': 5.194805194805195, 'macro_f1': 0.02001461741721482, 'micro_f1': 0.02670856245090338}
