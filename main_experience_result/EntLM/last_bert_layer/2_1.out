09/14/2023 08:51:07 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

loading configuration file /home/liwentao/learn/DecT_Mat_NER/model/config.json
Model config BertConfig {
  "_name_or_path": "/home/liwentao/learn/DecT_Mat_NER/model",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file /home/liwentao/learn/DecT_Mat_NER/model/config.json
Model config BertConfig {
  "_name_or_path": "/home/liwentao/learn/DecT_Mat_NER/model",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
loading configuration file /home/liwentao/learn/DecT_Mat_NER/model/config.json
Model config BertConfig {
  "_name_or_path": "/home/liwentao/learn/DecT_Mat_NER/model",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file /home/liwentao/learn/DecT_Mat_NER/model/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at /home/liwentao/learn/DecT_Mat_NER/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
/home/liwentao/learn/DecT_Mat_NER/baseline2_EntLM/train_transformer.py:569: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
09/14/2023 08:51:20 - INFO - __main__ - ***** Running training *****
09/14/2023 08:51:20 - INFO - __main__ -   Num examples = 9
09/14/2023 08:51:20 - INFO - __main__ -   Num Epochs = 60
09/14/2023 08:51:20 - INFO - __main__ -   Instantaneous batch size per device = 4
09/14/2023 08:51:20 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
09/14/2023 08:51:20 - INFO - __main__ -   Gradient Accumulation steps = 1
09/14/2023 08:51:20 - INFO - __main__ -   Total optimization steps = 180
Loading label map from scripts/matsciner/final_verbalizer.json...
{'I-CMT': ['electron', 'diffraction', 'microscopy', 'spectroscopy', 'transmission', 'test'], 'I-MAT': ['oxide', 'silicon', 'carbon', 'graphene', 'aluminum', 'oxides'], 'I-DSC': ['films', 'doped', 'thin', 'film', 'alloy'], 'I-PRO': ['properties', 'structure', 'magnetic', 'band', 'conductivity', 'electrical'], 'I-SMT': ['annealing', 'gel', 'plasma', 'hydrothermal', 'annealed', 'vapor'], 'I-APL': ['coatings', 'coating', 'solar', 'cells', 'electrode', 'applications', 'electrodes', 'catalysts', 'cathode'], 'I-SPL': ['cubic', 'hexagonal', 'rock', 'ch']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081, 10314, 5820, 7779, 2856, 856]
I-MAT
[6678, 8605, 3473, 11106, 15028, 20464]
I-DSC
[7423, 21155, 5197, 5796, 15937]
I-PRO
[1784, 1187, 3510, 2102, 9370, 4874]
I-SMT
[12040, 4051, 2780, 29973, 27202, 12766]
I-APL
[20189, 9754, 6911, 576, 6665, 2040, 8438, 15834, 19112]
I-SPL
[13879, 22235, 8863, 249]
{'I-CMT': ['electron', 'diffraction', 'microscopy', 'spectroscopy', 'transmission', 'test'], 'I-MAT': ['oxide', 'silicon', 'carbon', 'graphene', 'aluminum', 'oxides'], 'I-DSC': ['films', 'doped', 'thin', 'film', 'alloy'], 'I-PRO': ['properties', 'structure', 'magnetic', 'band', 'conductivity', 'electrical'], 'I-SMT': ['annealing', 'gel', 'plasma', 'hydrothermal', 'annealed', 'vapor'], 'I-APL': ['coatings', 'coating', 'solar', 'cells', 'electrode', 'applications', 'electrodes', 'catalysts', 'cathode'], 'I-SPL': ['cubic', 'hexagonal', 'rock', 'ch']}
BertForMaskedLM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(31097, 768)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (transform_act_fn): GELUActivation()
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=768, out_features=31097, bias=True)
    )
  )
)
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/180 [00:00<?, ?it/s]  1%|          | 2/180 [00:00<00:15, 11.79it/s]  2%|▏         | 4/180 [00:00<00:13, 13.53it/s]  3%|▎         | 6/180 [00:00<00:12, 13.69it/s]  4%|▍         | 8/180 [00:00<00:12, 13.65it/s]  6%|▌         | 10/180 [00:00<00:12, 13.51it/s]  7%|▋         | 12/180 [00:00<00:13, 12.27it/s]  8%|▊         | 14/180 [00:01<00:14, 11.68it/s]  9%|▉         | 16/180 [00:01<00:14, 11.18it/s] 10%|█         | 18/180 [00:01<00:14, 10.98it/s] 11%|█         | 20/180 [00:01<00:14, 10.72it/s] 12%|█▏        | 22/180 [00:01<00:14, 10.90it/s] 13%|█▎        | 24/180 [00:02<00:14, 10.82it/s] 14%|█▍        | 26/180 [00:02<00:14, 10.79it/s] 16%|█▌        | 28/180 [00:02<00:14, 10.68it/s] 17%|█▋        | 30/180 [00:02<00:14, 10.59it/s] 18%|█▊        | 32/180 [00:02<00:13, 10.65it/s] 19%|█▉        | 34/180 [00:03<00:13, 10.58it/s] 20%|██        | 36/180 [00:03<00:13, 10.48it/s] 21%|██        | 38/180 [00:03<00:13, 10.50it/s] 22%|██▏       | 40/180 [00:03<00:13, 10.47it/s] 23%|██▎       | 42/180 [00:03<00:13, 10.38it/s] 24%|██▍       | 44/180 [00:03<00:13, 10.45it/s] 26%|██▌       | 46/180 [00:04<00:12, 10.45it/s] 27%|██▋       | 48/180 [00:04<00:12, 10.26it/s] 28%|██▊       | 50/180 [00:04<00:12, 10.40it/s] 29%|██▉       | 52/180 [00:04<00:12, 10.54it/s] 30%|███       | 54/180 [00:04<00:12, 10.40it/s] 31%|███       | 56/180 [00:05<00:11, 10.57it/s] 32%|███▏      | 58/180 [00:05<00:11, 10.48it/s] 33%|███▎      | 60/180 [00:05<00:11, 10.63it/s] 34%|███▍      | 62/180 [00:05<00:11, 10.48it/s] 36%|███▌      | 64/180 [00:05<00:11, 10.47it/s] 37%|███▋      | 66/180 [00:06<00:10, 10.46it/s] 38%|███▊      | 68/180 [00:06<00:10, 10.50it/s] 39%|███▉      | 70/180 [00:06<00:10, 10.51it/s] 40%|████      | 72/180 [00:06<00:10, 10.48it/s] 41%|████      | 74/180 [00:06<00:10, 10.51it/s] 42%|████▏     | 76/180 [00:07<00:09, 10.54it/s] 43%|████▎     | 78/180 [00:07<00:09, 10.56it/s] 44%|████▍     | 80/180 [00:07<00:09, 10.54it/s] 46%|████▌     | 82/180 [00:07<00:09, 10.64it/s] 47%|████▋     | 84/180 [00:07<00:09, 10.46it/s] 48%|████▊     | 86/180 [00:07<00:09, 10.43it/s] 49%|████▉     | 88/180 [00:08<00:08, 10.50it/s] 50%|█████     | 90/180 [00:08<00:08, 10.48it/s] 51%|█████     | 92/180 [00:08<00:08, 10.49it/s] 52%|█████▏    | 94/180 [00:08<00:08, 10.59it/s] 53%|█████▎    | 96/180 [00:08<00:07, 10.59it/s] 54%|█████▍    | 98/180 [00:09<00:07, 10.66it/s] 56%|█████▌    | 100/180 [00:09<00:07, 10.52it/s] 57%|█████▋    | 102/180 [00:09<00:07, 10.41it/s] 58%|█████▊    | 104/180 [00:09<00:06, 11.53it/s] 59%|█████▉    | 106/180 [00:09<00:06, 12.26it/s] 60%|██████    | 108/180 [00:09<00:05, 12.81it/s] 61%|██████    | 110/180 [00:10<00:05, 12.89it/s] 62%|██████▏   | 112/180 [00:10<00:05, 13.24it/s] 63%|██████▎   | 114/180 [00:10<00:04, 13.64it/s] 64%|██████▍   | 116/180 [00:10<00:04, 13.90it/s] 66%|██████▌   | 118/180 [00:10<00:04, 13.47it/s] 67%|██████▋   | 120/180 [00:10<00:04, 12.58it/s] 68%|██████▊   | 122/180 [00:11<00:04, 12.16it/s] 69%|██████▉   | 124/180 [00:11<00:04, 11.57it/s] 70%|███████   | 126/180 [00:11<00:04, 11.76it/s] 71%|███████   | 128/180 [00:11<00:04, 12.73it/s] 72%|███████▏  | 130/180 [00:11<00:03, 13.84it/s] 73%|███████▎  | 132/180 [00:11<00:03, 15.06it/s] 75%|███████▌  | 135/180 [00:11<00:02, 16.68it/s] 76%|███████▌  | 137/180 [00:11<00:02, 16.65it/s] 77%|███████▋  | 139/180 [00:12<00:02, 17.41it/s] 78%|███████▊  | 141/180 [00:12<00:02, 15.97it/s] 80%|████████  | 144/180 [00:12<00:01, 18.74it/s] 82%|████████▏ | 147/180 [00:12<00:01, 19.53it/s] 83%|████████▎ | 150/180 [00:12<00:01, 20.66it/s] 85%|████████▌ | 153/180 [00:12<00:01, 21.99it/s] 87%|████████▋ | 156/180 [00:12<00:01, 23.04it/s] 88%|████████▊ | 159/180 [00:12<00:00, 22.58it/s] 90%|█████████ | 162/180 [00:13<00:00, 23.56it/s] 92%|█████████▏| 165/180 [00:13<00:00, 23.62it/s] 93%|█████████▎| 168/180 [00:13<00:00, 23.26it/s] 95%|█████████▌| 171/180 [00:13<00:00, 23.27it/s] 97%|█████████▋| 174/180 [00:13<00:00, 24.11it/s] 98%|█████████▊| 177/180 [00:13<00:00, 25.44it/s]100%|██████████| 180/180 [00:13<00:00, 26.29it/s]100%|██████████| 180/180 [00:14<00:00, 12.25it/s]
Decoding time: 0.8760204315185547s
APL_precision: 0.26153846153846155, APL_recall: 0.1, APL_f1: 0.14468085106382977, APL_number: 170
CMT_precision: 0.1282051282051282, CMT_recall: 0.1282051282051282, CMT_f1: 0.1282051282051282, CMT_number: 195
DSC_precision: 0.18947368421052632, DSC_recall: 0.08237986270022883, DSC_f1: 0.11483253588516747, DSC_number: 437
MAT_precision: 0.6070287539936102, MAT_recall: 0.2785923753665689, MAT_f1: 0.3819095477386934, MAT_number: 682
PRO_precision: 0.3684210526315789, PRO_recall: 0.03631647211413749, PRO_f1: 0.06611570247933884, PRO_number: 771
SMT_precision: 0.06976744186046512, SMT_recall: 0.03508771929824561, SMT_f1: 0.046692607003891044, SMT_number: 171
SPL_precision: 0.4166666666666667, SPL_recall: 0.06666666666666667, SPL_f1: 0.1149425287356322, SPL_number: 75
overall_precision: 0.327641408751334, overall_recall: 0.12275089964014395, overall_f1: 0.17859220477021523, overall_accuracy: 0.7148881423772425
Finish training, best metric: 
{'APL_precision': 0.26153846153846155, 'APL_recall': 0.1, 'APL_f1': 0.14468085106382977, 'APL_number': 170, 'CMT_precision': 0.1282051282051282, 'CMT_recall': 0.1282051282051282, 'CMT_f1': 0.1282051282051282, 'CMT_number': 195, 'DSC_precision': 0.18947368421052632, 'DSC_recall': 0.08237986270022883, 'DSC_f1': 0.11483253588516747, 'DSC_number': 437, 'MAT_precision': 0.6070287539936102, 'MAT_recall': 0.2785923753665689, 'MAT_f1': 0.3819095477386934, 'MAT_number': 682, 'PRO_precision': 0.3684210526315789, 'PRO_recall': 0.03631647211413749, 'PRO_f1': 0.06611570247933884, 'PRO_number': 771, 'SMT_precision': 0.06976744186046512, 'SMT_recall': 0.03508771929824561, 'SMT_f1': 0.046692607003891044, 'SMT_number': 171, 'SPL_precision': 0.4166666666666667, 'SPL_recall': 0.06666666666666667, 'SPL_f1': 0.1149425287356322, 'SPL_number': 75, 'overall_precision': 0.327641408751334, 'overall_recall': 0.12275089964014395, 'overall_f1': 0.17859220477021523, 'overall_accuracy': 0.7148881423772425}
