09/13/2023 11:32:21 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13551.87it/s]
Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 58.78it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 4 examples [00:00, 793.62 examples/s]
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 546 examples [00:00, 73081.76 examples/s]
loading configuration file /home/liwentao/learn/DecT_Mat_NER/model/config.json
Model config BertConfig {
  "_name_or_path": "/home/liwentao/learn/DecT_Mat_NER/model",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file /home/liwentao/learn/DecT_Mat_NER/model/config.json
Model config BertConfig {
  "_name_or_path": "/home/liwentao/learn/DecT_Mat_NER/model",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
loading configuration file /home/liwentao/learn/DecT_Mat_NER/model/config.json
Model config BertConfig {
  "_name_or_path": "/home/liwentao/learn/DecT_Mat_NER/model",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file /home/liwentao/learn/DecT_Mat_NER/model/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at /home/liwentao/learn/DecT_Mat_NER/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/final_verbalizer.json...
{'I-CMT': ['electron', 'diffraction', 'microscopy', 'spectroscopy', 'transmission', 'test'], 'I-MAT': ['oxide', 'silicon', 'carbon', 'graphene', 'aluminum', 'oxides'], 'I-DSC': ['films', 'doped', 'thin', 'film', 'alloy'], 'I-PRO': ['properties', 'structure', 'magnetic', 'band', 'conductivity', 'electrical'], 'I-SMT': ['annealing', 'gel', 'plasma', 'hydrothermal', 'annealed', 'vapor'], 'I-APL': ['coatings', 'coating', 'solar', 'cells', 'electrode', 'applications', 'electrodes', 'catalysts', 'cathode'], 'I-SPL': ['cubic', 'hexagonal', 'rock', 'ch']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
I-CMT
[3081, 10314, 5820, 7779, 2856, 856]
I-MAT
[6678, 8605, 3473, 11106, 15028, 20464]
I-DSC
[7423, 21155, 5197, 5796, 15937]
I-PRO
[1784, 1187, 3510, 2102, 9370, 4874]
I-SMT
[12040, 4051, 2780, 29973, 27202, 12766]
I-APL
[20189, 9754, 6911, 576, 6665, 2040, 8438, 15834, 19112]
I-SPL
[13879, 22235, 8863, 249]
{'I-CMT': ['electron', 'diffraction', 'microscopy', 'spectroscopy', 'transmission', 'test'], 'I-MAT': ['oxide', 'silicon', 'carbon', 'graphene', 'aluminum', 'oxides'], 'I-DSC': ['films', 'doped', 'thin', 'film', 'alloy'], 'I-PRO': ['properties', 'structure', 'magnetic', 'band', 'conductivity', 'electrical'], 'I-SMT': ['annealing', 'gel', 'plasma', 'hydrothermal', 'annealed', 'vapor'], 'I-APL': ['coatings', 'coating', 'solar', 'cells', 'electrode', 'applications', 'electrodes', 'catalysts', 'cathode'], 'I-SPL': ['cubic', 'hexagonal', 'rock', 'ch']}
Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 305.95 examples/s]
Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4284.19 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 3984.64 examples/s]
/home/liwentao/learn/DecT_Mat_NER/baseline2_EntLM/train_transformer.py:546: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
09/13/2023 11:32:32 - INFO - __main__ - ***** Running training *****
09/13/2023 11:32:32 - INFO - __main__ -   Num examples = 4
09/13/2023 11:32:32 - INFO - __main__ -   Num Epochs = 60
09/13/2023 11:32:32 - INFO - __main__ -   Instantaneous batch size per device = 4
09/13/2023 11:32:32 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
09/13/2023 11:32:32 - INFO - __main__ -   Gradient Accumulation steps = 1
09/13/2023 11:32:32 - INFO - __main__ -   Total optimization steps = 60
tensor([31090, 31091, 31092, 31093, 31094, 31095, 31096], device='cuda:0')
  0%|          | 0/60 [00:00<?, ?it/s]  2%|â–         | 1/60 [00:00<00:07,  7.59it/s]  3%|â–Ž         | 2/60 [00:00<00:06,  8.74it/s]  7%|â–‹         | 4/60 [00:00<00:06,  9.29it/s]  8%|â–Š         | 5/60 [00:00<00:05,  9.32it/s] 10%|â–ˆ         | 6/60 [00:00<00:05,  9.14it/s] 12%|â–ˆâ–        | 7/60 [00:00<00:05,  9.12it/s] 13%|â–ˆâ–Ž        | 8/60 [00:00<00:05,  9.01it/s] 15%|â–ˆâ–Œ        | 9/60 [00:00<00:05,  9.01it/s] 17%|â–ˆâ–‹        | 10/60 [00:01<00:05,  8.90it/s] 18%|â–ˆâ–Š        | 11/60 [00:01<00:05,  8.76it/s] 20%|â–ˆâ–ˆ        | 12/60 [00:01<00:05,  8.34it/s] 22%|â–ˆâ–ˆâ–       | 13/60 [00:01<00:05,  8.03it/s] 23%|â–ˆâ–ˆâ–Ž       | 14/60 [00:01<00:05,  7.73it/s] 25%|â–ˆâ–ˆâ–Œ       | 15/60 [00:01<00:05,  7.57it/s] 27%|â–ˆâ–ˆâ–‹       | 16/60 [00:01<00:05,  7.40it/s] 28%|â–ˆâ–ˆâ–Š       | 17/60 [00:02<00:05,  7.35it/s] 30%|â–ˆâ–ˆâ–ˆ       | 18/60 [00:02<00:05,  7.32it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 19/60 [00:02<00:05,  7.30it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 20/60 [00:02<00:05,  7.22it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/60 [00:02<00:05,  7.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/60 [00:02<00:05,  7.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 23/60 [00:02<00:05,  7.19it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/60 [00:03<00:05,  7.15it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/60 [00:03<00:04,  7.22it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 26/60 [00:03<00:04,  7.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/60 [00:03<00:04,  7.25it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/60 [00:03<00:04,  7.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 29/60 [00:03<00:04,  7.26it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/60 [00:03<00:04,  7.20it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/60 [00:04<00:03,  7.27it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 32/60 [00:04<00:03,  7.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/60 [00:04<00:03,  7.32it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 34/60 [00:04<00:03,  7.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 35/60 [00:04<00:03,  7.34it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/60 [00:04<00:03,  7.26it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/60 [00:04<00:03,  7.28it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 38/60 [00:04<00:03,  7.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/60 [00:05<00:02,  7.31it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 40/60 [00:05<00:02,  7.33it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 41/60 [00:05<00:02,  7.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/60 [00:05<00:02,  7.36it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 43/60 [00:05<00:02,  7.35it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 44/60 [00:05<00:02,  7.34it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 45/60 [00:05<00:02,  7.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 46/60 [00:06<00:01,  7.34it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 47/60 [00:06<00:01,  7.31it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 48/60 [00:06<00:01,  7.33it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 49/60 [00:06<00:01,  7.28it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 50/60 [00:06<00:01,  7.22it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 51/60 [00:06<00:01,  7.26it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 52/60 [00:06<00:01,  7.29it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 53/60 [00:07<00:00,  7.32it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 54/60 [00:07<00:00,  7.38it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 55/60 [00:07<00:00,  7.38it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 56/60 [00:07<00:00,  7.47it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 57/60 [00:07<00:00,  7.38it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 58/60 [00:07<00:00,  7.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 59/60 [00:07<00:00,  7.46it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:07<00:00,  7.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:11<00:00,  5.35it/s]
Decoding time: 3.2741641998291016s
APL_precision: 0.5, APL_recall: 0.10588235294117647, APL_f1: 0.17475728155339806, APL_number: 170
CMT_precision: 0.3333333333333333, CMT_recall: 0.041025641025641026, CMT_f1: 0.0730593607305936, CMT_number: 195
DSC_precision: 0.19047619047619047, DSC_recall: 0.036613272311212815, DSC_f1: 0.06142034548944338, DSC_number: 437
MAT_precision: 0.6170212765957447, MAT_recall: 0.04252199413489736, MAT_f1: 0.07956104252400549, MAT_number: 682
PRO_precision: 0.12280701754385964, PRO_recall: 0.009079118028534372, PRO_f1: 0.016908212560386472, PRO_number: 771
SMT_precision: 0.7142857142857143, SMT_recall: 0.029239766081871343, SMT_f1: 0.05617977528089887, SMT_number: 171
SPL_precision: 0.6666666666666666, SPL_recall: 0.05333333333333334, SPL_f1: 0.09876543209876544, SPL_number: 75
overall_precision: 0.3333333333333333, overall_recall: 0.03478608556577369, overall_f1: 0.06299782766111514, overall_accuracy: 0.6990207990851262
Finish training, best metric: 
{'APL_precision': 0.5, 'APL_recall': 0.10588235294117647, 'APL_f1': 0.17475728155339806, 'APL_number': 170, 'CMT_precision': 0.3333333333333333, 'CMT_recall': 0.041025641025641026, 'CMT_f1': 0.0730593607305936, 'CMT_number': 195, 'DSC_precision': 0.19047619047619047, 'DSC_recall': 0.036613272311212815, 'DSC_f1': 0.06142034548944338, 'DSC_number': 437, 'MAT_precision': 0.6170212765957447, 'MAT_recall': 0.04252199413489736, 'MAT_f1': 0.07956104252400549, 'MAT_number': 682, 'PRO_precision': 0.12280701754385964, 'PRO_recall': 0.009079118028534372, 'PRO_f1': 0.016908212560386472, 'PRO_number': 771, 'SMT_precision': 0.7142857142857143, 'SMT_recall': 0.029239766081871343, 'SMT_f1': 0.05617977528089887, 'SMT_number': 171, 'SPL_precision': 0.6666666666666666, 'SPL_recall': 0.05333333333333334, 'SPL_f1': 0.09876543209876544, 'SPL_number': 75, 'overall_precision': 0.3333333333333333, 'overall_recall': 0.03478608556577369, 'overall_f1': 0.06299782766111514, 'overall_accuracy': 0.6990207990851262}
