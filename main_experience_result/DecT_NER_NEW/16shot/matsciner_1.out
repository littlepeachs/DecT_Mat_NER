/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:44:27 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:44:28 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-68fc820920ef3b72/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:16, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1074.64it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/41 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4941.53 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:44:35 - INFO - __main__ - ***** Running training *****
05/31/2023 13:44:35 - INFO - __main__ -   Num examples = 41
05/31/2023 13:44:35 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:44:35 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:44:35 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:44:35 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:44:35 - INFO - __main__ -   Total optimization steps = 200
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/200 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.746194839477539
Total epoch: 1. epoch loss: 14.839715957641602
Total epoch: 2. epoch loss: 13.955010414123535
Total epoch: 3. epoch loss: 13.092571258544922
Total epoch: 4. epoch loss: 12.253878593444824
Total epoch: 5. epoch loss: 11.441216468811035
Total epoch: 6. epoch loss: 10.657379150390625
Total epoch: 7. epoch loss: 9.905326843261719
Total epoch: 8. epoch loss: 9.187835693359375
Total epoch: 9. epoch loss: 8.507274627685547
Total epoch: 10. epoch loss: 7.865535736083984
Total epoch: 11. epoch loss: 7.263916492462158
Total epoch: 12. epoch loss: 6.703202724456787
Total epoch: 13. epoch loss: 6.183366298675537
Total epoch: 14. epoch loss: 5.7036848068237305
Total epoch: 15. epoch loss: 5.262595176696777
Total epoch: 16. epoch loss: 4.857972145080566
Total epoch: 17. epoch loss: 4.4872589111328125
Total epoch: 18. epoch loss: 4.147551536560059
Total epoch: 19. epoch loss: 3.8359622955322266
Total epoch: 20. epoch loss: 3.549701690673828
Total epoch: 21. epoch loss: 3.2867836952209473
Total epoch: 22. epoch loss: 3.0461223125457764
Total epoch: 23. epoch loss: 2.826525926589966
Total epoch: 24. epoch loss: 2.6266775131225586
Total epoch: 25. epoch loss: 2.4452226161956787
Total epoch: 26. epoch loss: 2.280841827392578
Total epoch: 27. epoch loss: 2.1321704387664795
Total epoch: 28. epoch loss: 1.9978909492492676
Total epoch: 29. epoch loss: 1.8766971826553345
Total epoch: 30. epoch loss: 1.7673251628875732
Total epoch: 31. epoch loss: 1.6685733795166016
Total epoch: 32. epoch loss: 1.5793287754058838
Total epoch: 33. epoch loss: 1.4985506534576416
Total epoch: 34. epoch loss: 1.4253140687942505
Total epoch: 35. epoch loss: 1.3587851524353027
Total epoch: 36. epoch loss: 1.2982256412506104
Total epoch: 37. epoch loss: 1.2429853677749634
Total epoch: 38. epoch loss: 1.192488431930542
Total epoch: 39. epoch loss: 1.1462124586105347
Total epoch: 40. epoch loss: 1.1037073135375977
Total epoch: 41. epoch loss: 1.0645835399627686
Total epoch: 42. epoch loss: 1.0284807682037354
Total epoch: 43. epoch loss: 0.9950874447822571
Total epoch: 44. epoch loss: 0.9641300439834595
Total epoch: 45. epoch loss: 0.9353713989257812
Total epoch: 46. epoch loss: 0.908592700958252
Total epoch: 47. epoch loss: 0.8836020231246948
Total epoch: 48. epoch loss: 0.8602410554885864
Total epoch: 49. epoch loss: 0.8383417725563049
Total epoch: 50. epoch loss: 0.8177843689918518
Total epoch: 51. epoch loss: 0.7984420657157898
Total epoch: 52. epoch loss: 0.7802072763442993
Total epoch: 53. epoch loss: 0.762984573841095
Total epoch: 54. epoch loss: 0.746687650680542
Total epoch: 55. epoch loss: 0.7312342524528503
Total epoch: 56. epoch loss: 0.7165610194206238
Total epoch: 57. epoch loss: 0.7026060819625854
Total epoch: 58. epoch loss: 0.6893094182014465
Total epoch: 59. epoch loss: 0.676624059677124
Total epoch: 60. epoch loss: 0.664507269859314
Total epoch: 61. epoch loss: 0.652923583984375
Total epoch: 62. epoch loss: 0.6418331265449524
Total epoch: 63. epoch loss: 0.6312075257301331
Total epoch: 64. epoch loss: 0.6210179924964905
Total epoch: 65. epoch loss: 0.6112362146377563
Total epoch: 66. epoch loss: 0.6018375754356384
Total epoch: 67. epoch loss: 0.5928020477294922
Total epoch: 68. epoch loss: 0.5841028094291687
Total epoch: 69. epoch loss: 0.5757275223731995
Total epoch: 70. epoch loss: 0.567646861076355
Total epoch: 71. epoch loss: 0.5598487257957458
Total epoch: 72. epoch loss: 0.5523195266723633
Total epoch: 73. epoch loss: 0.5450395345687866
Total epoch: 74. epoch loss: 0.5379934310913086
Total epoch: 75. epoch loss: 0.5311736464500427
Total epoch: 76. epoch loss: 0.5245633125305176
Total epoch: 77. epoch loss: 0.5181527733802795
Total epoch: 78. epoch loss: 0.5119335651397705
Total epoch: 79. epoch loss: 0.5058963894844055
Total epoch: 80. epoch loss: 0.5000297427177429
Total epoch: 81. epoch loss: 0.4943290948867798
Total epoch: 82. epoch loss: 0.48878300189971924
Total epoch: 83. epoch loss: 0.4833872318267822
Total epoch: 84. epoch loss: 0.4781337380409241
Total epoch: 85. epoch loss: 0.47301870584487915
Total epoch: 86. epoch loss: 0.46803387999534607
Total epoch: 87. epoch loss: 0.463172048330307
Total epoch: 88. epoch loss: 0.45842939615249634
Total epoch: 89. epoch loss: 0.45380231738090515
Total epoch: 90. epoch loss: 0.44928300380706787
Total epoch: 91. epoch loss: 0.4448710083961487
Total epoch: 92. epoch loss: 0.4405594766139984
Total epoch: 93. epoch loss: 0.43634456396102905
Total epoch: 94. epoch loss: 0.43222153186798096
Total epoch: 95. epoch loss: 0.4281908869743347
Total epoch: 96. epoch loss: 0.4242437183856964
Total epoch: 97. epoch loss: 0.42038100957870483
Total epoch: 98. epoch loss: 0.4165986180305481
Total epoch: 99. epoch loss: 0.41289278864860535
Total epoch: 99. DecT loss: 0.41289278864860535
Training time: 0.5139343738555908
APL_precision: 0.3215547703180212, APL_recall: 0.5352941176470588, APL_f1: 0.4017660044150111, APL_number: 170
CMT_precision: 0.5279503105590062, CMT_recall: 0.4358974358974359, CMT_f1: 0.47752808988764045, CMT_number: 195
DSC_precision: 0.5260115606936416, DSC_recall: 0.41647597254004576, DSC_f1: 0.46487867177522346, DSC_number: 437
MAT_precision: 0.5251461988304094, MAT_recall: 0.658357771260997, MAT_f1: 0.5842550422901757, MAT_number: 682
PRO_precision: 0.3487544483985765, PRO_recall: 0.25421530479896237, PRO_f1: 0.2940735183795949, PRO_number: 771
SMT_precision: 0.33579335793357934, SMT_recall: 0.5321637426900585, SMT_f1: 0.411764705882353, SMT_number: 171
SPL_precision: 0.36619718309859156, SPL_recall: 0.3466666666666667, SPL_f1: 0.35616438356164387, SPL_number: 75
overall_precision: 0.43938799529227146, overall_recall: 0.4478208716513395, overall_f1: 0.4435643564356436, overall_accuracy: 0.8130226574226288
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/200 [00:04<?, ?it/s]