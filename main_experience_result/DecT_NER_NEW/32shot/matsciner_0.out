/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:44:47 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:44:48 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-3c2959e63eb4f343/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:32, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 160
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1213.28it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/77 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4477.29 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:44:54 - INFO - __main__ - ***** Running training *****
05/31/2023 13:44:54 - INFO - __main__ -   Num examples = 77
05/31/2023 13:44:54 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:44:54 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:44:54 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:44:54 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:44:54 - INFO - __main__ -   Total optimization steps = 300
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/300 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.9409818649292
Total epoch: 1. epoch loss: 15.053810119628906
Total epoch: 2. epoch loss: 14.200237274169922
Total epoch: 3. epoch loss: 13.375823974609375
Total epoch: 4. epoch loss: 12.577428817749023
Total epoch: 5. epoch loss: 11.80435562133789
Total epoch: 6. epoch loss: 11.057917594909668
Total epoch: 7. epoch loss: 10.340472221374512
Total epoch: 8. epoch loss: 9.654769897460938
Total epoch: 9. epoch loss: 9.003457069396973
Total epoch: 10. epoch loss: 8.388825416564941
Total epoch: 11. epoch loss: 7.812440872192383
Total epoch: 12. epoch loss: 7.275094032287598
Total epoch: 13. epoch loss: 6.776645660400391
Total epoch: 14. epoch loss: 6.316060543060303
Total epoch: 15. epoch loss: 5.8915276527404785
Total epoch: 16. epoch loss: 5.5006022453308105
Total epoch: 17. epoch loss: 5.140481948852539
Total epoch: 18. epoch loss: 4.808163166046143
Total epoch: 19. epoch loss: 4.500694274902344
Total epoch: 20. epoch loss: 4.2162675857543945
Total epoch: 21. epoch loss: 3.953488826751709
Total epoch: 22. epoch loss: 3.711043119430542
Total epoch: 23. epoch loss: 3.487621545791626
Total epoch: 24. epoch loss: 3.2819671630859375
Total epoch: 25. epoch loss: 3.0929293632507324
Total epoch: 26. epoch loss: 2.919341564178467
Total epoch: 27. epoch loss: 2.7601208686828613
Total epoch: 28. epoch loss: 2.614189624786377
Total epoch: 29. epoch loss: 2.480522394180298
Total epoch: 30. epoch loss: 2.358067035675049
Total epoch: 31. epoch loss: 2.2458243370056152
Total epoch: 32. epoch loss: 2.142832040786743
Total epoch: 33. epoch loss: 2.0481724739074707
Total epoch: 34. epoch loss: 1.9610260725021362
Total epoch: 35. epoch loss: 1.8806573152542114
Total epoch: 36. epoch loss: 1.8064148426055908
Total epoch: 37. epoch loss: 1.7377530336380005
Total epoch: 38. epoch loss: 1.674165964126587
Total epoch: 39. epoch loss: 1.6152430772781372
Total epoch: 40. epoch loss: 1.5605803728103638
Total epoch: 41. epoch loss: 1.5098249912261963
Total epoch: 42. epoch loss: 1.462654709815979
Total epoch: 43. epoch loss: 1.418760061264038
Total epoch: 44. epoch loss: 1.3778620958328247
Total epoch: 45. epoch loss: 1.3397091627120972
Total epoch: 46. epoch loss: 1.3040733337402344
Total epoch: 47. epoch loss: 1.2707279920578003
Total epoch: 48. epoch loss: 1.239480972290039
Total epoch: 49. epoch loss: 1.2101447582244873
Total epoch: 50. epoch loss: 1.1825469732284546
Total epoch: 51. epoch loss: 1.1565426588058472
Total epoch: 52. epoch loss: 1.1319935321807861
Total epoch: 53. epoch loss: 1.1087733507156372
Total epoch: 54. epoch loss: 1.086782693862915
Total epoch: 55. epoch loss: 1.0659191608428955
Total epoch: 56. epoch loss: 1.0461033582687378
Total epoch: 57. epoch loss: 1.02725088596344
Total epoch: 58. epoch loss: 1.0093019008636475
Total epoch: 59. epoch loss: 0.9921916723251343
Total epoch: 60. epoch loss: 0.9758549928665161
Total epoch: 61. epoch loss: 0.9602422118186951
Total epoch: 62. epoch loss: 0.9453052282333374
Total epoch: 63. epoch loss: 0.9309924244880676
Total epoch: 64. epoch loss: 0.9172652959823608
Total epoch: 65. epoch loss: 0.904083251953125
Total epoch: 66. epoch loss: 0.8914090991020203
Total epoch: 67. epoch loss: 0.8792087435722351
Total epoch: 68. epoch loss: 0.8674600720405579
Total epoch: 69. epoch loss: 0.8561279773712158
Total epoch: 70. epoch loss: 0.8451909422874451
Total epoch: 71. epoch loss: 0.8346253037452698
Total epoch: 72. epoch loss: 0.824410617351532
Total epoch: 73. epoch loss: 0.8145330548286438
Total epoch: 74. epoch loss: 0.8049657940864563
Total epoch: 75. epoch loss: 0.7957022786140442
Total epoch: 76. epoch loss: 0.786716878414154
Total epoch: 77. epoch loss: 0.7780026793479919
Total epoch: 78. epoch loss: 0.7695411443710327
Total epoch: 79. epoch loss: 0.7613245844841003
Total epoch: 80. epoch loss: 0.7533336281776428
Total epoch: 81. epoch loss: 0.7455661296844482
Total epoch: 82. epoch loss: 0.7380058169364929
Total epoch: 83. epoch loss: 0.7306404113769531
Total epoch: 84. epoch loss: 0.7234669327735901
Total epoch: 85. epoch loss: 0.7164771556854248
Total epoch: 86. epoch loss: 0.7096606492996216
Total epoch: 87. epoch loss: 0.703007161617279
Total epoch: 88. epoch loss: 0.6965155601501465
Total epoch: 89. epoch loss: 0.6901782155036926
Total epoch: 90. epoch loss: 0.6839823722839355
Total epoch: 91. epoch loss: 0.6779288649559021
Total epoch: 92. epoch loss: 0.6720125079154968
Total epoch: 93. epoch loss: 0.6662247776985168
Total epoch: 94. epoch loss: 0.6605636477470398
Total epoch: 95. epoch loss: 0.6550207138061523
Total epoch: 96. epoch loss: 0.6495919227600098
Total epoch: 97. epoch loss: 0.6442787051200867
Total epoch: 98. epoch loss: 0.6390694379806519
Total epoch: 99. epoch loss: 0.6339672803878784
Total epoch: 99. DecT loss: 0.6339672803878784
Training time: 0.7558112144470215
APL_precision: 0.37142857142857144, APL_recall: 0.5352941176470588, APL_f1: 0.43855421686746987, APL_number: 170
CMT_precision: 0.32558139534883723, CMT_recall: 0.5025641025641026, CMT_f1: 0.3951612903225806, CMT_number: 195
DSC_precision: 0.5, DSC_recall: 0.5446224256292906, DSC_f1: 0.5213581599123768, DSC_number: 437
MAT_precision: 0.5651074589127687, MAT_recall: 0.655425219941349, MAT_f1: 0.6069246435845214, MAT_number: 682
PRO_precision: 0.38496071829405165, PRO_recall: 0.44487678339818415, PRO_f1: 0.4127557160048135, PRO_number: 771
SMT_precision: 0.3519163763066202, SMT_recall: 0.5906432748538012, SMT_f1: 0.4410480349344978, SMT_number: 171
SPL_precision: 0.35384615384615387, SPL_recall: 0.30666666666666664, SPL_f1: 0.3285714285714285, SPL_number: 75
overall_precision: 0.4388089005235602, overall_recall: 0.5361855257896841, overall_f1: 0.48263451502609317, overall_accuracy: 0.8330355228361089
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/300 [00:04<?, ?it/s]