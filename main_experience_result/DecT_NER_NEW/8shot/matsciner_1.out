/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:44:07 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:44:08 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-90b165f8431f5e0a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1029.66it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/24 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 5254.68 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:44:14 - INFO - __main__ - ***** Running training *****
05/31/2023 13:44:14 - INFO - __main__ -   Num examples = 24
05/31/2023 13:44:14 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:44:14 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:44:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:44:14 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:44:14 - INFO - __main__ -   Total optimization steps = 100
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.55809497833252
Total epoch: 1. epoch loss: 14.592178344726562
Total epoch: 2. epoch loss: 13.64296817779541
Total epoch: 3. epoch loss: 12.713732719421387
Total epoch: 4. epoch loss: 11.808316230773926
Total epoch: 5. epoch loss: 10.931056022644043
Total epoch: 6. epoch loss: 10.086644172668457
Total epoch: 7. epoch loss: 9.27972412109375
Total epoch: 8. epoch loss: 8.514384269714355
Total epoch: 9. epoch loss: 7.793869972229004
Total epoch: 10. epoch loss: 7.120284557342529
Total epoch: 11. epoch loss: 6.4945454597473145
Total epoch: 12. epoch loss: 5.916538238525391
Total epoch: 13. epoch loss: 5.385197162628174
Total epoch: 14. epoch loss: 4.898787975311279
Total epoch: 15. epoch loss: 4.454987049102783
Total epoch: 16. epoch loss: 4.0510101318359375
Total epoch: 17. epoch loss: 3.683736801147461
Total epoch: 18. epoch loss: 3.349698066711426
Total epoch: 19. epoch loss: 3.0454494953155518
Total epoch: 20. epoch loss: 2.767824411392212
Total epoch: 21. epoch loss: 2.515047550201416
Total epoch: 22. epoch loss: 2.2858896255493164
Total epoch: 23. epoch loss: 2.0791091918945312
Total epoch: 24. epoch loss: 1.8934035301208496
Total epoch: 25. epoch loss: 1.7273772954940796
Total epoch: 26. epoch loss: 1.5795691013336182
Total epoch: 27. epoch loss: 1.448487639427185
Total epoch: 28. epoch loss: 1.3326226472854614
Total epoch: 29. epoch loss: 1.2304553985595703
Total epoch: 30. epoch loss: 1.1405062675476074
Total epoch: 31. epoch loss: 1.0613399744033813
Total epoch: 32. epoch loss: 0.9915820956230164
Total epoch: 33. epoch loss: 0.9299755692481995
Total epoch: 34. epoch loss: 0.8753713965415955
Total epoch: 35. epoch loss: 0.8267629146575928
Total epoch: 36. epoch loss: 0.7832788825035095
Total epoch: 37. epoch loss: 0.7442021369934082
Total epoch: 38. epoch loss: 0.7089248299598694
Total epoch: 39. epoch loss: 0.6769389510154724
Total epoch: 40. epoch loss: 0.6478214263916016
Total epoch: 41. epoch loss: 0.6212231516838074
Total epoch: 42. epoch loss: 0.5968372821807861
Total epoch: 43. epoch loss: 0.5744022727012634
Total epoch: 44. epoch loss: 0.5537034869194031
Total epoch: 45. epoch loss: 0.5345564484596252
Total epoch: 46. epoch loss: 0.5167959332466125
Total epoch: 47. epoch loss: 0.5002908706665039
Total epoch: 48. epoch loss: 0.4849242866039276
Total epoch: 49. epoch loss: 0.47058549523353577
Total epoch: 50. epoch loss: 0.4571862518787384
Total epoch: 51. epoch loss: 0.4446396231651306
Total epoch: 52. epoch loss: 0.4328772723674774
Total epoch: 53. epoch loss: 0.42182087898254395
Total epoch: 54. epoch loss: 0.411410927772522
Total epoch: 55. epoch loss: 0.4015994966030121
Total epoch: 56. epoch loss: 0.39232996106147766
Total epoch: 57. epoch loss: 0.3835606575012207
Total epoch: 58. epoch loss: 0.3752526044845581
Total epoch: 59. epoch loss: 0.3673703372478485
Total epoch: 60. epoch loss: 0.3598807454109192
Total epoch: 61. epoch loss: 0.3527536392211914
Total epoch: 62. epoch loss: 0.34595945477485657
Total epoch: 63. epoch loss: 0.3394761383533478
Total epoch: 64. epoch loss: 0.3332769274711609
Total epoch: 65. epoch loss: 0.32734909653663635
Total epoch: 66. epoch loss: 0.32167014479637146
Total epoch: 67. epoch loss: 0.3162231147289276
Total epoch: 68. epoch loss: 0.31099337339401245
Total epoch: 69. epoch loss: 0.30597081780433655
Total epoch: 70. epoch loss: 0.3011355996131897
Total epoch: 71. epoch loss: 0.2964820861816406
Total epoch: 72. epoch loss: 0.2920004725456238
Total epoch: 73. epoch loss: 0.2876768410205841
Total epoch: 74. epoch loss: 0.2835039794445038
Total epoch: 75. epoch loss: 0.2794707715511322
Total epoch: 76. epoch loss: 0.275574266910553
Total epoch: 77. epoch loss: 0.2718033194541931
Total epoch: 78. epoch loss: 0.26815372705459595
Total epoch: 79. epoch loss: 0.264616996049881
Total epoch: 80. epoch loss: 0.2611883282661438
Total epoch: 81. epoch loss: 0.25786319375038147
Total epoch: 82. epoch loss: 0.2546350061893463
Total epoch: 83. epoch loss: 0.2514987289905548
Total epoch: 84. epoch loss: 0.24845075607299805
Total epoch: 85. epoch loss: 0.2454857975244522
Total epoch: 86. epoch loss: 0.24260279536247253
Total epoch: 87. epoch loss: 0.23979395627975464
Total epoch: 88. epoch loss: 0.23705895245075226
Total epoch: 89. epoch loss: 0.23439496755599976
Total epoch: 90. epoch loss: 0.23179607093334198
Total epoch: 91. epoch loss: 0.2292613834142685
Total epoch: 92. epoch loss: 0.2267894297838211
Total epoch: 93. epoch loss: 0.22437378764152527
Total epoch: 94. epoch loss: 0.222015842795372
Total epoch: 95. epoch loss: 0.21971087157726288
Total epoch: 96. epoch loss: 0.217459574341774
Total epoch: 97. epoch loss: 0.21525681018829346
Total epoch: 98. epoch loss: 0.21310189366340637
Total epoch: 99. epoch loss: 0.21099373698234558
Total epoch: 99. DecT loss: 0.21099373698234558
Training time: 0.4931609630584717
APL_precision: 0.20394736842105263, APL_recall: 0.36470588235294116, APL_f1: 0.2616033755274262, APL_number: 170
CMT_precision: 0.29081632653061223, CMT_recall: 0.2923076923076923, CMT_f1: 0.2915601023017903, CMT_number: 195
DSC_precision: 0.39210526315789473, DSC_recall: 0.34096109839816935, DSC_f1: 0.3647490820073439, DSC_number: 437
MAT_precision: 0.5941278065630398, MAT_recall: 0.5043988269794721, MAT_f1: 0.5455987311657414, MAT_number: 682
PRO_precision: 0.34210526315789475, PRO_recall: 0.20233463035019456, PRO_f1: 0.254278728606357, PRO_number: 771
SMT_precision: 0.22304832713754646, SMT_recall: 0.3508771929824561, SMT_f1: 0.2727272727272727, SMT_number: 171
SPL_precision: 0.3409090909090909, SPL_recall: 0.4, SPL_f1: 0.3680981595092024, SPL_number: 75
overall_precision: 0.37764084507042256, overall_recall: 0.343062774890044, overall_f1: 0.3595223130106851, overall_accuracy: 0.7840754770924165
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:04<?, ?it/s]