/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:44:07 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:44:09 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-18e93fae9dc4599b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 938.11it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4919.30 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:44:16 - INFO - __main__ - ***** Running training *****
05/31/2023 13:44:16 - INFO - __main__ -   Num examples = 21
05/31/2023 13:44:16 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:44:16 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:44:16 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:44:16 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:44:16 - INFO - __main__ -   Total optimization steps = 100
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 16.26845932006836
Total epoch: 1. epoch loss: 15.184499740600586
Total epoch: 2. epoch loss: 14.122404098510742
Total epoch: 3. epoch loss: 13.088067054748535
Total epoch: 4. epoch loss: 12.087896347045898
Total epoch: 5. epoch loss: 11.128826141357422
Total epoch: 6. epoch loss: 10.21731948852539
Total epoch: 7. epoch loss: 9.358202934265137
Total epoch: 8. epoch loss: 8.553994178771973
Total epoch: 9. epoch loss: 7.8053297996521
Total epoch: 10. epoch loss: 7.111431121826172
Total epoch: 11. epoch loss: 6.470905780792236
Total epoch: 12. epoch loss: 5.881934642791748
Total epoch: 13. epoch loss: 5.342268943786621
Total epoch: 14. epoch loss: 4.84925651550293
Total epoch: 15. epoch loss: 4.400030136108398
Total epoch: 16. epoch loss: 3.9916703701019287
Total epoch: 17. epoch loss: 3.621321678161621
Total epoch: 18. epoch loss: 3.2862870693206787
Total epoch: 19. epoch loss: 2.9837260246276855
Total epoch: 20. epoch loss: 2.710662603378296
Total epoch: 21. epoch loss: 2.465261936187744
Total epoch: 22. epoch loss: 2.245588779449463
Total epoch: 23. epoch loss: 2.049480438232422
Total epoch: 24. epoch loss: 1.8747670650482178
Total epoch: 25. epoch loss: 1.7194199562072754
Total epoch: 26. epoch loss: 1.581501841545105
Total epoch: 27. epoch loss: 1.4592199325561523
Total epoch: 28. epoch loss: 1.3509324789047241
Total epoch: 29. epoch loss: 1.2550913095474243
Total epoch: 30. epoch loss: 1.170303463935852
Total epoch: 31. epoch loss: 1.095264196395874
Total epoch: 32. epoch loss: 1.028784155845642
Total epoch: 33. epoch loss: 0.9697650074958801
Total epoch: 34. epoch loss: 0.9172170758247375
Total epoch: 35. epoch loss: 0.8702542185783386
Total epoch: 36. epoch loss: 0.8281040787696838
Total epoch: 37. epoch loss: 0.7900962829589844
Total epoch: 38. epoch loss: 0.7556692361831665
Total epoch: 39. epoch loss: 0.7243478894233704
Total epoch: 40. epoch loss: 0.6957334280014038
Total epoch: 41. epoch loss: 0.6694967746734619
Total epoch: 42. epoch loss: 0.6453548073768616
Total epoch: 43. epoch loss: 0.623060405254364
Total epoch: 44. epoch loss: 0.602410078048706
Total epoch: 45. epoch loss: 0.5832239985466003
Total epoch: 46. epoch loss: 0.5653465986251831
Total epoch: 47. epoch loss: 0.5486450791358948
Total epoch: 48. epoch loss: 0.533003568649292
Total epoch: 49. epoch loss: 0.5183287858963013
Total epoch: 50. epoch loss: 0.5045294761657715
Total epoch: 51. epoch loss: 0.49153465032577515
Total epoch: 52. epoch loss: 0.4792719781398773
Total epoch: 53. epoch loss: 0.46768268942832947
Total epoch: 54. epoch loss: 0.4567122757434845
Total epoch: 55. epoch loss: 0.44630876183509827
Total epoch: 56. epoch loss: 0.436428040266037
Total epoch: 57. epoch loss: 0.4270312786102295
Total epoch: 58. epoch loss: 0.418076753616333
Total epoch: 59. epoch loss: 0.40953314304351807
Total epoch: 60. epoch loss: 0.40136951208114624
Total epoch: 61. epoch loss: 0.39356058835983276
Total epoch: 62. epoch loss: 0.3860764503479004
Total epoch: 63. epoch loss: 0.37890276312828064
Total epoch: 64. epoch loss: 0.3720097243785858
Total epoch: 65. epoch loss: 0.36538758873939514
Total epoch: 66. epoch loss: 0.35901132225990295
Total epoch: 67. epoch loss: 0.35287582874298096
Total epoch: 68. epoch loss: 0.34695810079574585
Total epoch: 69. epoch loss: 0.34125182032585144
Total epoch: 70. epoch loss: 0.3357419967651367
Total epoch: 71. epoch loss: 0.3304203152656555
Total epoch: 72. epoch loss: 0.3252759873867035
Total epoch: 73. epoch loss: 0.3202993869781494
Total epoch: 74. epoch loss: 0.3154827058315277
Total epoch: 75. epoch loss: 0.3108161687850952
Total epoch: 76. epoch loss: 0.3062939941883087
Total epoch: 77. epoch loss: 0.30190789699554443
Total epoch: 78. epoch loss: 0.29765257239341736
Total epoch: 79. epoch loss: 0.29352182149887085
Total epoch: 80. epoch loss: 0.2895067632198334
Total epoch: 81. epoch loss: 0.28560540080070496
Total epoch: 82. epoch loss: 0.28181248903274536
Total epoch: 83. epoch loss: 0.278122216463089
Total epoch: 84. epoch loss: 0.2745305299758911
Total epoch: 85. epoch loss: 0.2710322141647339
Total epoch: 86. epoch loss: 0.2676270008087158
Total epoch: 87. epoch loss: 0.2643059492111206
Total epoch: 88. epoch loss: 0.2610725164413452
Total epoch: 89. epoch loss: 0.2579156458377838
Total epoch: 90. epoch loss: 0.25483784079551697
Total epoch: 91. epoch loss: 0.25183573365211487
Total epoch: 92. epoch loss: 0.2489027976989746
Total epoch: 93. epoch loss: 0.24604128301143646
Total epoch: 94. epoch loss: 0.2432459592819214
Total epoch: 95. epoch loss: 0.24051466584205627
Total epoch: 96. epoch loss: 0.2378474473953247
Total epoch: 97. epoch loss: 0.23524042963981628
Total epoch: 98. epoch loss: 0.23269236087799072
Total epoch: 99. epoch loss: 0.23019884526729584
Total epoch: 99. DecT loss: 0.23019884526729584
Training time: 0.46685266494750977
APL_precision: 0.17204301075268819, APL_recall: 0.3764705882352941, APL_f1: 0.23616236162361623, APL_number: 170
CMT_precision: 0.20303030303030303, CMT_recall: 0.3435897435897436, CMT_f1: 0.25523809523809526, CMT_number: 195
DSC_precision: 0.5098039215686274, DSC_recall: 0.2379862700228833, DSC_f1: 0.3244929797191888, DSC_number: 437
MAT_precision: 0.5791245791245792, MAT_recall: 0.5043988269794721, MAT_f1: 0.5391849529780564, MAT_number: 682
PRO_precision: 0.375, PRO_recall: 0.17509727626459143, PRO_f1: 0.23872679045092832, PRO_number: 771
SMT_precision: 0.0880503144654088, SMT_recall: 0.08187134502923976, SMT_f1: 0.08484848484848485, SMT_number: 171
SPL_precision: 0.2975206611570248, SPL_recall: 0.48, SPL_f1: 0.3673469387755102, SPL_number: 75
overall_precision: 0.35700934579439253, overall_recall: 0.30547780887644943, overall_f1: 0.3292393880629175, overall_accuracy: 0.7734972482310056
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:04<?, ?it/s]