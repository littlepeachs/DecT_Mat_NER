/home/liwentao/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
05/31/2023 13:44:07 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/31/2023 13:44:09 - WARNING - datasets.builder - Found cached dataset json (/home/liwentao/.cache/huggingface/datasets/json/default-0af7def6dc20224d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
################## Hyper-parameters: ##################
epochs:100,lr:0.005, batch_size:32, shot:8, proto_dim:160, logits_weight:10.0, weight_decay:1e-05 
################## Hyper-parameters: ##################
shot and proto_dim
proto_dim: 128
model_logits_weight: 10
num_train_epochs: 100
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1032.57it/s]
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading file vocab.txt from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/vocab.txt
loading file tokenizer.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/tokenizer_config.json
loading configuration file config.json from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/config.json
Model config BertConfig {
  "_name_or_path": "m3rg-iitd/matscibert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 31090
}

loading weights file pytorch_model.bin from cache at /home/liwentao/.cache/huggingface/hub/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.1"
}

All model checkpoint weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the model checkpoint at m3rg-iitd/matscibert.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Generation config file not found, using a generation config created from the model config.
Loading label map from scripts/matsciner/proto_verbalizer.json...
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
{'O': 0, 'I-CMT': 1, 'I-MAT': 2, 'I-DSC': 3, 'I-PRO': 4, 'I-SMT': 5, 'I-APL': 6, 'I-SPL': 7, 'B-CMT': 8, 'B-MAT': 9, 'B-DSC': 10, 'B-PRO': 11, 'B-SMT': 12, 'B-APL': 13, 'B-SPL': 14}
{'I-CMT': ['electron'], 'I-MAT': ['silicon'], 'I-DSC': ['doped'], 'I-PRO': ['properties'], 'I-SMT': ['annealing'], 'I-APL': ['coating'], 'I-SPL': ['cubic']}
Running tokenizer on dataset:   0%|          | 0/21 [00:00<?, ? examples/s]                                                                           Running tokenizer on dataset:   0%|          | 0/546 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [00:00<00:00, 4897.65 examples/s]                                                                                        /home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py:563: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("./seqeval_metric.py")
05/31/2023 13:44:14 - INFO - __main__ - ***** Running training *****
05/31/2023 13:44:14 - INFO - __main__ -   Num examples = 21
05/31/2023 13:44:14 - INFO - __main__ -   Num Epochs = 100
05/31/2023 13:44:14 - INFO - __main__ -   Instantaneous batch size per device = 32
05/31/2023 13:44:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
05/31/2023 13:44:14 - INFO - __main__ -   Gradient Accumulation steps = 1
05/31/2023 13:44:14 - INFO - __main__ -   Total optimization steps = 100
tensor([101, 101, 101, 101, 101, 101, 101], device='cuda:0')
  0%|          | 0/100 [00:00<?, ?it/s]/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py:377: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  logits = F.softmax(self.extract_logits(batch_logits))
Total epoch: 0. epoch loss: 15.577195167541504
Total epoch: 1. epoch loss: 14.492756843566895
Total epoch: 2. epoch loss: 13.428366661071777
Total epoch: 3. epoch loss: 12.393125534057617
Total epoch: 4. epoch loss: 11.397795677185059
Total epoch: 5. epoch loss: 10.4530029296875
Total epoch: 6. epoch loss: 9.56676959991455
Total epoch: 7. epoch loss: 8.742514610290527
Total epoch: 8. epoch loss: 7.979315757751465
Total epoch: 9. epoch loss: 7.273879528045654
Total epoch: 10. epoch loss: 6.622527122497559
Total epoch: 11. epoch loss: 6.022144317626953
Total epoch: 12. epoch loss: 5.47014045715332
Total epoch: 13. epoch loss: 4.964152812957764
Total epoch: 14. epoch loss: 4.501645088195801
Total epoch: 15. epoch loss: 4.080002784729004
Total epoch: 16. epoch loss: 3.696385145187378
Total epoch: 17. epoch loss: 3.3479361534118652
Total epoch: 18. epoch loss: 3.031870126724243
Total epoch: 19. epoch loss: 2.745434045791626
Total epoch: 20. epoch loss: 2.4863672256469727
Total epoch: 21. epoch loss: 2.2534751892089844
Total epoch: 22. epoch loss: 2.0450291633605957
Total epoch: 23. epoch loss: 1.8590399026870728
Total epoch: 24. epoch loss: 1.6934226751327515
Total epoch: 25. epoch loss: 1.5460821390151978
Total epoch: 26. epoch loss: 1.4151273965835571
Total epoch: 27. epoch loss: 1.2988665103912354
Total epoch: 28. epoch loss: 1.1957828998565674
Total epoch: 29. epoch loss: 1.10453462600708
Total epoch: 30. epoch loss: 1.0238755941390991
Total epoch: 31. epoch loss: 0.952655017375946
Total epoch: 32. epoch loss: 0.8897465467453003
Total epoch: 33. epoch loss: 0.8341190218925476
Total epoch: 34. epoch loss: 0.7848107218742371
Total epoch: 35. epoch loss: 0.7409520745277405
Total epoch: 36. epoch loss: 0.7017554640769958
Total epoch: 37. epoch loss: 0.6665902733802795
Total epoch: 38. epoch loss: 0.6348838210105896
Total epoch: 39. epoch loss: 0.6061803698539734
Total epoch: 40. epoch loss: 0.5800867676734924
Total epoch: 41. epoch loss: 0.556277334690094
Total epoch: 42. epoch loss: 0.5344791412353516
Total epoch: 43. epoch loss: 0.5144572257995605
Total epoch: 44. epoch loss: 0.49600720405578613
Total epoch: 45. epoch loss: 0.47895678877830505
Total epoch: 46. epoch loss: 0.463155061006546
Total epoch: 47. epoch loss: 0.4484679698944092
Total epoch: 48. epoch loss: 0.4347839653491974
Total epoch: 49. epoch loss: 0.42201173305511475
Total epoch: 50. epoch loss: 0.41006162762641907
Total epoch: 51. epoch loss: 0.39886677265167236
Total epoch: 52. epoch loss: 0.38835409283638
Total epoch: 53. epoch loss: 0.37847286462783813
Total epoch: 54. epoch loss: 0.3691669702529907
Total epoch: 55. epoch loss: 0.36039066314697266
Total epoch: 56. epoch loss: 0.3520961403846741
Total epoch: 57. epoch loss: 0.34425073862075806
Total epoch: 58. epoch loss: 0.33680811524391174
Total epoch: 59. epoch loss: 0.3297480642795563
Total epoch: 60. epoch loss: 0.3230321407318115
Total epoch: 61. epoch loss: 0.31663841009140015
Total epoch: 62. epoch loss: 0.3105406165122986
Total epoch: 63. epoch loss: 0.30471867322921753
Total epoch: 64. epoch loss: 0.2991511821746826
Total epoch: 65. epoch loss: 0.29382261633872986
Total epoch: 66. epoch loss: 0.28871721029281616
Total epoch: 67. epoch loss: 0.2838188111782074
Total epoch: 68. epoch loss: 0.2791168987751007
Total epoch: 69. epoch loss: 0.27460020780563354
Total epoch: 70. epoch loss: 0.2702520787715912
Total epoch: 71. epoch loss: 0.26607072353363037
Total epoch: 72. epoch loss: 0.262042760848999
Total epoch: 73. epoch loss: 0.2581583261489868
Total epoch: 74. epoch loss: 0.25441116094589233
Total epoch: 75. epoch loss: 0.2507915496826172
Total epoch: 76. epoch loss: 0.24729269742965698
Total epoch: 77. epoch loss: 0.24391071498394012
Total epoch: 78. epoch loss: 0.24063760042190552
Total epoch: 79. epoch loss: 0.23746412992477417
Total epoch: 80. epoch loss: 0.23439152538776398
Total epoch: 81. epoch loss: 0.23140761256217957
Total epoch: 82. epoch loss: 0.22851456701755524
Total epoch: 83. epoch loss: 0.22570380568504333
Total epoch: 84. epoch loss: 0.222969651222229
Total epoch: 85. epoch loss: 0.22031497955322266
Total epoch: 86. epoch loss: 0.21773098409175873
Total epoch: 87. epoch loss: 0.21521706879138947
Total epoch: 88. epoch loss: 0.21276700496673584
Total epoch: 89. epoch loss: 0.210381418466568
Total epoch: 90. epoch loss: 0.2080550193786621
Total epoch: 91. epoch loss: 0.2057873010635376
Total epoch: 92. epoch loss: 0.20357495546340942
Total epoch: 93. epoch loss: 0.20141614973545074
Total epoch: 94. epoch loss: 0.1993086189031601
Total epoch: 95. epoch loss: 0.1972496509552002
Total epoch: 96. epoch loss: 0.1952395886182785
Total epoch: 97. epoch loss: 0.19327497482299805
Total epoch: 98. epoch loss: 0.19135311245918274
Total epoch: 99. epoch loss: 0.18947561085224152
Total epoch: 99. DecT loss: 0.18947561085224152
Training time: 0.5172090530395508
APL_precision: 0.32515337423312884, APL_recall: 0.31176470588235294, APL_f1: 0.3183183183183183, APL_number: 170
CMT_precision: 0.0797872340425532, CMT_recall: 0.23076923076923078, CMT_f1: 0.11857707509881424, CMT_number: 195
DSC_precision: 0.5108695652173914, DSC_recall: 0.2151029748283753, DSC_f1: 0.30273752012882454, DSC_number: 437
MAT_precision: 0.5659955257270693, MAT_recall: 0.3709677419354839, MAT_f1: 0.4481842338352524, MAT_number: 682
PRO_precision: 0.305668016194332, PRO_recall: 0.19584954604409857, PRO_f1: 0.23873517786561263, PRO_number: 771
SMT_precision: 0.1608910891089109, SMT_recall: 0.38011695906432746, SMT_f1: 0.22608695652173916, SMT_number: 171
SPL_precision: 0.3592233009708738, SPL_recall: 0.49333333333333335, SPL_f1: 0.4157303370786517, SPL_number: 75
overall_precision: 0.29588808817295464, overall_recall: 0.27908836465413833, overall_f1: 0.2872427983539095, overall_accuracy: 0.7546994496462012
finish test
Traceback (most recent call last):
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 985, in <module>
    main()
  File "/home/liwentao/Dec-Tuning-in-Mat/run_ner_dect.py", line 808, in main
    runner.run(train_dataloader,None ,eval_dataloader)
  File "/home/liwentao/Dec-Tuning-in-Mat/dect_trainer.py", line 484, in run
    return score
NameError: name 'score' is not defined
  0%|          | 0/100 [00:04<?, ?it/s]